{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT for Humans Baseline.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "MNzAuLZDzG_R",
        "javRwDS6zhOC",
        "lRKAhZK8yvO_"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hogo56/BertQA/blob/master/BERT_for_Humans_Baseline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MNzAuLZDzG_R",
        "colab_type": "text"
      },
      "source": [
        "# ============= Machine Spinup ============="
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zNni7Dc1NzVe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Reset kernel without removing downloaded data files and libs\n",
        "#%reset\n",
        "#! rm -i /content/output/*"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M0k60Je4YEQa",
        "colab_type": "text"
      },
      "source": [
        "## -- Main System Config --\n",
        "<Details><Summary>Global Config</Summary>\n",
        "Put any global system configuration here"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JNKoTizrh9RG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "b4aa85b3-6c35-4a21-975b-93d2d289a1b7"
      },
      "source": [
        "%%bash\n",
        "zdump PST               # Not sure what is up with the time, PST is running about 8 hrs ahead\n",
        "mkdir -p /content/lib\n",
        "mkdir -p /content/data\n",
        "mkdir -p /content/output       # Maybe symlink to Google Drive for permenance\n",
        "rm -rf /content/sample_data"
      ],
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "PST  Mon Dec 16 10:37:49 2019 PST\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UdjitRnQynwk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os, sys\n",
        "sys.path.append('/content/lib')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "88YILsdhCzqR",
        "colab_type": "text"
      },
      "source": [
        "### Runtime Parameters\n",
        "<Details><Summary>Global Variables</Summary>\n",
        "EnableAllCode - There are code blocks here that should not be run with \"Run All\". By default EnableAllCode will set to False and those blocks will be excluded. If you want to run them individually for some reason set EnableAllCode True.<p>\n",
        "DownloadBigFiles - There are GBs of files and downloads to make this run. If you are just wanting to spin up the Colab so you can SSH into it set DownloadBigFiles = False then Runtime -> RunAfter</Details>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uw9zV9bwScZe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "EnableAllCode = False                # Prevent codeblocks that should not execute on Run All\n",
        "DownloadBigFiles = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aheaRzVE6fs1",
        "colab_type": "text"
      },
      "source": [
        "## -- Setup --"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gixU6_gv678n",
        "colab_type": "text"
      },
      "source": [
        "###Google Drive\n",
        "<Details>There are several ways to provide access to your Google Drive from Colab. (What about the Drive FUSE wrapper?)<br>\n",
        "I am not sure if this is the best. This mounts your Drive into the machine.<br>\n",
        "I expect there will be a folder in the Drive that we all share.</Details>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "auXx45x70Qcs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "46eecf93-3211-4aaa-d1bc-ad96653e5799"
      },
      "source": [
        "## File link to Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=False)   # true to reread drive\n",
        "# Create a shorter shared directory name than one with a space\n",
        "! ln -s '/content/gdrive/My Drive/bertqa' /content/bertqa"
      ],
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "ln: failed to create symbolic link '/content/bertqa/bertqa': Operation not supported\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ga-sQOkk1YvA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if EnableAllCode:\n",
        "    ## Flush and unmount Google Drive\n",
        "    # You probablyu won't do this but if you want to at some point click the play button\n",
        "    drive.flush_and_unmount()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q8BLJAW95ih-",
        "colab_type": "text"
      },
      "source": [
        "### Kaggle API\n",
        "<Details>You will need Kaggle API token to link the Colab instance to your Kaggle account to get data, etc.<br>\n",
        "Go to: https://www.kaggle.com/yourID/account and click on the \"Create New API Token: button to get a file named kaggle.json.<p>You can put your kaggle.json file in your google drive at My Drive/colab/kaggle.json.<br>\n",
        "Alternately, you can store it on your local machine and the script will ask you to upload it.</Details>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lc3IYPUg17c8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "e0f4f8b4-e47d-4744-c295-582060cf06dc"
      },
      "source": [
        "## Link to Kaggle\n",
        "from google.colab import files\n",
        "\n",
        "# see if there is a kaggle.json file in gdrive\n",
        "try:\n",
        "    # see if auth file is in gdrive\n",
        "    f = open(\"/content/gdrive/My Drive/colab/kaggle.json\")\n",
        "    os.environ['KAGGLE_CONFIG_DIR'] = \"/content/gdrive/My Drive/colab/\"\n",
        "    ! ls -l \"/content/gdrive/My Drive/colab/kaggle.json\"\n",
        "except IOError:\n",
        "    # Have user upload file\n",
        "    ! rm /content/kaggle.json  2> /dev/null\n",
        "    print('Upload kaggle.json.')\n",
        "    # The files.upload() command is failing sporatically with:\n",
        "    #   TypeError: Cannot read property '_uploadFiles' of undefined (just run again)\n",
        "    files.upload()\n",
        "    os.environ['KAGGLE_CONFIG_DIR'] = \"/content/\"\n",
        "    ! ls -l /content/kaggle.json\n",
        "\n",
        "import kaggle"
      ],
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-rw------- 1 root root 66 Dec 15 08:31 '/content/gdrive/My Drive/colab/kaggle.json'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "javRwDS6zhOC",
        "colab_type": "text"
      },
      "source": [
        "# =========== Project Specific Stuff ==========="
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "JFQgZRN2PaEE"
      },
      "source": [
        "## -- Project Setup --"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gAqCNh4y8DJz",
        "colab_type": "text"
      },
      "source": [
        "### Download Dataset and Support Files"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cGGM0EVmM24H",
        "colab_type": "text"
      },
      "source": [
        "Kaggle Competition Files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UF-28Eb21E2w",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "outputId": "17912c9e-b1ad-4271-b895-b3a37ee765e5"
      },
      "source": [
        "## Competition Dataset  (5GB zipped)\n",
        "if DownloadBigFiles:\n",
        "    if not os.path.exists(\"/content/data/compdata.flag\"):\n",
        "        print(\"Downloading Competition Data\\n\")        # ! kaggle competitions list\n",
        "        # ! kaggle competitions download -c tensorflow2-question-answering -p /content/data\n",
        "        # ! mv /content/data/sample_submission.csv /content/output/\n",
        "        # ! unzip /content/data/simplified-nq-test.jsonl.zip -d /content/data/\n",
        "        # ! rm /content/data/simplified-nq-test.jsonl.zip\n",
        "        # ! unzip /content/data/simplified-nq-train.jsonl.zip -d /content/data/\n",
        "        # ! rm /content/data/simplified-nq-train.jsonl.zip\n",
        "        ! touch /content/data/compdata.flag\n",
        "    else:\n",
        "        print(\"Competition Data already exists. Not downloading.\\n\")\n",
        "        !ls -l /content/data"
      ],
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Competition Data already exists. Not downloading.\n",
            "\n",
            "total 17063648\n",
            "-rw-r--r-- 1 root root           0 Dec 16 10:16 bertdata.flag\n",
            "drwxr-xr-x 2 root root        4096 Dec 16 08:15 bert-joint-baseline\n",
            "-rw-r--r-- 1 root root           0 Dec 16 10:07 compdata.flag\n",
            "-rw-r--r-- 1 root root    18771612 Oct 28 13:02 simplified-nq-test.jsonl\n",
            "-rw-r--r-- 1 root root 17454394481 Oct 28 13:02 simplified-nq-train.jsonl\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XJoB8YB3MrvX",
        "colab_type": "text"
      },
      "source": [
        "Bert-Joint files from: \n",
        "https://github.com/google-research/language/tree/master/language/question_answering/bert_joint\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1uvuFGJ0MqHo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "outputId": "e9da68ab-f17d-4dc8-9203-feeaeec495d8"
      },
      "source": [
        "if DownloadBigFiles:\n",
        "    if not os.path.exists(\"/content/data/bertdata.flag\"):\n",
        "        print(\"Downloading BERT-joint Data\\n\")        # ! kaggle competitions list\n",
        "        ! gsutil cp -R gs://bert-nq/bert-joint-baseline /content/data\n",
        "        ! touch \"/content/data/bertdata.flag\"\n",
        "    else:\n",
        "        print(\"BERT-joint Data already exists. Not downloading.\\n\")\n",
        "        !ls -l /content/data/bert-joint-baseline/"
      ],
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "BERT-joint Data already exists. Not downloading.\n",
            "\n",
            "total 2401128\n",
            "-rw-r--r-- 1 root root        314 Dec 16 08:14 bert_config.json\n",
            "-rw-r--r-- 1 root root 1340596260 Dec 16 08:15 bert_joint.ckpt.data-00000-of-00001\n",
            "-rw-r--r-- 1 root root      16781 Dec 16 08:15 bert_joint.ckpt.index\n",
            "-rw-r--r-- 1 root root 1117884423 Dec 16 08:15 nq-train.tfrecords-00000-of-00001\n",
            "-rw-r--r-- 1 root root     231601 Dec 16 08:15 vocab-nq.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c4NrOOtpOVN0",
        "colab_type": "text"
      },
      "source": [
        "Bert files from: https://github.com/google-research/bert<br>\n",
        "(Not the model we are using at the moment)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pdp6tqXmmLw_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## get BERT (this is unlikely to be the BERT-joint files needed for competition)\n",
        "# this version of BERT seems won't import as is. On line 88 of lib/bert/optimization.py\n",
        "#    change   tr.train.Optimizer to tf.keras.optimizers.Optimizer\n",
        "if DownloadBigFiles and False:\n",
        "    ! git clone https://github.com/google-research/bert.git\n",
        "    ! mv bert lib\n",
        "\n",
        "    # get some pretrained models  (I really  have no idea what these are or if useful)\n",
        "    ! wget https://storage.googleapis.com/bert_models/2018_10_18/cased_L-12_H-768_A-12.zip\n",
        "    ! unzip cased_L-12_H-768_A-12.zip\n",
        "    ! rm cased_L-12_H-768_A-12.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oU6FPUHjvH1r",
        "colab_type": "text"
      },
      "source": [
        "<Details><Summary>BERT tf.compat.v1 Notes</Summary>\n",
        "baseline_w_bert_translated_to_tf2_0 (next code block) comes from /dimitreoliveira with this warning:<br>\n",
        "This baseline uses code that was migrated from TF1.x. Be aware that it contains use of tf.compat.v1, which is not permitted to be eligible for TF2.0 prizes in this competition. It is intended to be used as a starting point, but we're excited to see how much better you can do using TF2.0!<br>\n",
        "https://www.kaggle.com/dimitreoliveira/using-tf-2-0-w-bert-on-nq-translated-to-tf2-0</Details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "bNPxSmhMJC5c"
      },
      "source": [
        "### Library Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yr69yF8fepTA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Copy lib files over from Google Drive\n",
        "! cp -a /content/bertqa/lib/* lib/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qhpBRp11R0jv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "b049ecf3-c037-4cc7-fbc6-6d6b3b021a53"
      },
      "source": [
        "## Load Libraries\n",
        "#magic to make colab path to Tensorflow V2\n",
        "%tensorflow_version 2.x \n",
        "import tensorflow as tf\n",
        "print(\"TensofFlow\", tf.__version__)\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import bert_modeling as modeling                    # from philculliton\n",
        "import bert_optimization as optimization            # from philculliton\n",
        "import bert_tokenization as tokenization            # from philculliton\n",
        "\n",
        "# import tf2_0_baseline_w_bert as tf2baseline # old script from philculliton\n",
        "import tf2_0_baseline_w_bert_translated_to_tf2_0 as tf2baseline # from dimitreoliveira\n",
        "\n",
        "import json\n",
        "import absl\n",
        "from zipfile import ZipFile"
      ],
      "execution_count": 150,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "`%tensorflow_version` only switches the major version: `1.x` or `2.x`.\n",
            "You set: `2.x                         # magic to make colab path to Tensorflow V2`. This will be interpreted as: `2.x`.\n",
            "\n",
            "\n",
            "TensorFlow is already loaded. Please restart the runtime to change versions.\n",
            "TensofFlow 2.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yJC4o-hkuIfO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "8cfe3e2f-391d-47cd-a96b-7409c52b19e2"
      },
      "source": [
        "%%bash\n",
        "zdump PST"
      ],
      "execution_count": 151,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "PST  Mon Dec 16 10:38:00 2019 PST\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s7niWvRW-R5C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# assert False                ### Stop Execution"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NsnKczo07ygW",
        "colab_type": "text"
      },
      "source": [
        "## -- Code Implementation in Tensorflow 2.0 --"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v7JSYsp-72La",
        "colab_type": "text"
      },
      "source": [
        "The code for this notebook is taken from the [translated version](https://www.kaggle.com/dimitreoliveira/using-tf-2-0-w-bert-on-nq-translated-to-tf2-0) posted by [Dimitre Oliviera](https://www.kaggle.com/dimitreoliveira)\n",
        "\n",
        "Dimitre updated the baseline [philculliton script](https://www.kaggle.com/philculliton/using-tensorflow-2-0-w-bert-on-nq) to the Tensorflow 2.0 version, this way we can take part in the TF2 prizes and may use the version to improve the work.\n",
        "\n",
        "The original source for this may have originated from: [Google BERTjoint](https://github.com/google-research/language/tree/master/language/question_answering/bert_joint)\n",
        "\n",
        "**A few notes:**\n",
        "- If you want to keep using **flags** and **logging** you will have to use the **absl** lib (this is recommended by the TF team).\n",
        "- Since we won't use it with the kernels, he removed most of the **TPU** related stuff to reduce complexity.\n",
        "- Tensorflow 2 don't let us use global variables **(tf.compat.v1.trainable_variables())**.\n",
        "\n",
        "In this notebook, we'll be using the Bert baseline for Tensorflow to create predictions for the Natural Questions test set. Note that this uses a model that has already been pre-trained - we're only doing inference here. A GPU is required, and this should take between 1-2 hours to run.\n",
        "\n",
        "The original script can be found [here](https://github.com/google-research/language/blob/master/language/question_answering/bert_joint/run_nq.py).<br>\n",
        "The supporting modules were drawn from the [official Tensorflow model repository](https://github.com/tensorflow/models/tree/master/official).<br>\n",
        "The bert-joint-baseline data is described [here](https://github.com/google-research/language/tree/master/language/question_answering/bert_joint).\n",
        "\n",
        "**Note:** This baseline uses code that was migrated from TF1.x. Be aware that it contains use of tf.compat.v1, which is not permitted to be eligible for [TF2.0 prizes in this competition](https://www.kaggle.com/c/tensorflow2-question-answering/overview/prizes)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ioz1gRjeJiB7",
        "colab_type": "text"
      },
      "source": [
        "### Tensorflow Flags"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mdSi099XJsGa",
        "colab_type": "text"
      },
      "source": [
        "Tensorflow flags are variables that can be passed around within the TF system. Every flag below has some context provided regarding what the flag is and how it's used.<p>\n",
        "Most of these can be changed as desired, with the exception of the Special Flags at the bottom, which must stay as-is to work with the Kaggle back end."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rhcWhldD17Q4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260
        },
        "outputId": "3c6984c8-2386-4751-c47f-78e1ca6cf5a8"
      },
      "source": [
        "%%bash\n",
        "zdump PST\n",
        "ls -l /content/data/\n",
        "ls -l /content/data/bert-joint-baseline/\n",
        "ls -l /content/output/"
      ],
      "execution_count": 153,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "PST  Mon Dec 16 10:38:01 2019 PST\n",
            "total 17063648\n",
            "-rw-r--r-- 1 root root           0 Dec 16 10:16 bertdata.flag\n",
            "drwxr-xr-x 2 root root        4096 Dec 16 08:15 bert-joint-baseline\n",
            "-rw-r--r-- 1 root root           0 Dec 16 10:07 compdata.flag\n",
            "-rw-r--r-- 1 root root    18771612 Oct 28 13:02 simplified-nq-test.jsonl\n",
            "-rw-r--r-- 1 root root 17454394481 Oct 28 13:02 simplified-nq-train.jsonl\n",
            "total 2401128\n",
            "-rw-r--r-- 1 root root        314 Dec 16 08:14 bert_config.json\n",
            "-rw-r--r-- 1 root root 1340596260 Dec 16 08:15 bert_joint.ckpt.data-00000-of-00001\n",
            "-rw-r--r-- 1 root root      16781 Dec 16 08:15 bert_joint.ckpt.index\n",
            "-rw-r--r-- 1 root root 1117884423 Dec 16 08:15 nq-train.tfrecords-00000-of-00001\n",
            "-rw-r--r-- 1 root root     231601 Dec 16 08:15 vocab-nq.txt\n",
            "total 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DrfU_hZQJxWv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "a03d1046-8a8f-4b9e-aa1d-923d1d988ae2"
      },
      "source": [
        "def del_all_flags(FLAGS):\n",
        "    flags_dict = FLAGS._flags()\n",
        "    keys_list = [keys for keys in flags_dict]\n",
        "    for keys in keys_list:\n",
        "        FLAGS.__delattr__(keys)\n",
        "\n",
        "del_all_flags(absl.flags.FLAGS)\n",
        "\n",
        "flags = absl.flags\n",
        "\n",
        "flags.DEFINE_string(\n",
        "    \"bert_config_file\", \"/content/data/bert-joint-baseline/bert_config.json\",\n",
        "    \"The config json file corresponding to the pre-trained BERT model. \"\n",
        "    \"This specifies the model architecture.\")\n",
        "\n",
        "flags.DEFINE_string(\"vocab_file\", \"/content/data/bert-joint-baseline/vocab-nq.txt\",\n",
        "                    \"The vocabulary file that the BERT model was trained on.\")\n",
        "\n",
        "flags.DEFINE_string(\n",
        "    \"output_dir\", \"/content/output\",\n",
        "    \"The output directory where the model checkpoints will be written.\")\n",
        "\n",
        "flags.DEFINE_string(\"train_precomputed_file\", None,\n",
        "                    \"Precomputed tf records for training.\")\n",
        "\n",
        "flags.DEFINE_integer(\"train_num_precomputed\", None,\n",
        "                     \"Number of precomputed tf records for training.\")\n",
        "\n",
        "flags.DEFINE_string(\n",
        "    \"output_prediction_file\", \"predictions.json\",\n",
        "    \"Where to print predictions in NQ prediction format, to be passed to\"\n",
        "    \"natural_questions.nq_eval.\")\n",
        "\n",
        "flags.DEFINE_string(\n",
        "    \"init_checkpoint\", \"/content/data/bert-joint-baseline/bert_joint.ckpt\",\n",
        "    \"Initial checkpoint (usually from a pre-trained BERT model).\")\n",
        "\n",
        "flags.DEFINE_bool(\n",
        "    \"do_lower_case\", True,\n",
        "    \"Whether to lower case the input text. Should be True for uncased \"\n",
        "    \"models and False for cased models.\")\n",
        "\n",
        "flags.DEFINE_integer(\n",
        "    \"max_seq_length\", 384,\n",
        "    \"The maximum total input sequence length after WordPiece tokenization. \"\n",
        "    \"Sequences longer than this will be truncated, and sequences shorter \"\n",
        "    \"than this will be padded.\")\n",
        "\n",
        "flags.DEFINE_integer(\n",
        "    \"doc_stride\", 128,\n",
        "    \"When splitting up a long document into chunks, how much stride to \"\n",
        "    \"take between chunks.\")\n",
        "\n",
        "flags.DEFINE_integer(\n",
        "    \"max_query_length\", 64,\n",
        "    \"The maximum number of tokens for the question. Questions longer than \"\n",
        "    \"this will be truncated to this length.\")\n",
        "\n",
        "flags.DEFINE_bool(\"do_train\", False, \"Whether to run training.\")\n",
        "\n",
        "flags.DEFINE_bool(\"do_predict\", True, \"Whether to run eval on the dev set.\")\n",
        "\n",
        "flags.DEFINE_integer(\"train_batch_size\", 32, \"Total batch size for training.\")\n",
        "\n",
        "flags.DEFINE_integer(\"predict_batch_size\", 8,\n",
        "                     \"Total batch size for predictions.\")\n",
        "\n",
        "flags.DEFINE_float(\"learning_rate\", 5e-5, \"The initial learning rate for Adam.\")\n",
        "\n",
        "flags.DEFINE_float(\"num_train_epochs\", 3.0,\n",
        "                   \"Total number of training epochs to perform.\")\n",
        "\n",
        "flags.DEFINE_float(\n",
        "    \"warmup_proportion\", 0.1,\n",
        "    \"Proportion of training to perform linear learning rate warmup for. \"\n",
        "    \"E.g., 0.1 = 10% of training.\")\n",
        "\n",
        "flags.DEFINE_integer(\"save_checkpoints_steps\", 1000,\n",
        "                     \"How often to save the model checkpoint.\")\n",
        "\n",
        "flags.DEFINE_integer(\"iterations_per_loop\", 1000,\n",
        "                     \"How many steps to make in each estimator call.\")\n",
        "\n",
        "flags.DEFINE_integer(\n",
        "    \"n_best_size\", 20,\n",
        "    \"The total number of n-best predictions to generate in the \"\n",
        "    \"nbest_predictions.json output file.\")\n",
        "\n",
        "flags.DEFINE_integer(\n",
        "    \"verbosity\", 1, \"How verbose our error messages should be\")\n",
        "\n",
        "flags.DEFINE_integer(\n",
        "    \"max_answer_length\", 30,\n",
        "    \"The maximum length of an answer that can be generated. This is needed \"\n",
        "    \"because the start and end predictions are not conditioned on one another.\")\n",
        "\n",
        "flags.DEFINE_float(\n",
        "    \"include_unknowns\", -1.0,\n",
        "    \"If positive, probability of including answers of type `UNKNOWN`.\")\n",
        "\n",
        "flags.DEFINE_bool(\"use_tpu\", False, \"Whether to use TPU or GPU/CPU.\")\n",
        "flags.DEFINE_bool(\"use_one_hot_embeddings\", False, \"Whether to use use_one_hot_embeddings\")\n",
        "\n",
        "absl.flags.DEFINE_string(\n",
        "    \"gcp_project\", None,\n",
        "    \"[Optional] Project name for the Cloud TPU-enabled project. If not \"\n",
        "    \"specified, we will attempt to automatically detect the GCE project from \"\n",
        "    \"metadata.\")\n",
        "\n",
        "flags.DEFINE_bool(\n",
        "    \"verbose_logging\", False,\n",
        "    \"If true, all of the warnings related to data processing will be printed. \"\n",
        "    \"A number of warnings are expected for a normal NQ evaluation.\")\n",
        "\n",
        "flags.DEFINE_boolean(\n",
        "    \"skip_nested_contexts\", True,\n",
        "    \"Completely ignore context that are not top level nodes in the page.\")\n",
        "\n",
        "flags.DEFINE_integer(\"task_id\", 0,\n",
        "                     \"Train and dev shard to read from and write to.\")\n",
        "\n",
        "flags.DEFINE_integer(\"max_contexts\", 48,\n",
        "                     \"Maximum number of contexts to output for an example.\")\n",
        "\n",
        "flags.DEFINE_integer(\n",
        "    \"max_position\", 50,\n",
        "    \"Maximum context position for which to generate special tokens.\")\n",
        "\n",
        "\n",
        "## Special flags - do not change\n",
        "\n",
        "flags.DEFINE_string(\n",
        "    \"predict_file\", \"/content/data/simplified-nq-test.jsonl\",\n",
        "    \"NQ json for predictions. E.g., dev-v1.1.jsonl.gz or test-v1.1.jsonl.gz\")\n",
        "flags.DEFINE_boolean(\"logtostderr\", True, \"Logs to stderr\")\n",
        "flags.DEFINE_boolean(\"undefok\", True, \"it's okay to be undefined\")\n",
        "flags.DEFINE_string('f', '', 'kernel')\n",
        "flags.DEFINE_string('HistoryManager.hist_file', '', 'kernel')\n",
        "\n",
        "FLAGS = flags.FLAGS\n",
        "FLAGS(sys.argv) # Parse the flags"
      ],
      "execution_count": 154,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 154
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jr2VvpY58cA_",
        "colab_type": "text"
      },
      "source": [
        "**Here, we:**\n",
        "1. Set up Bert\n",
        "2. Read in the test set\n",
        "3. Run it past the pre-built Bert model to create embeddings\n",
        "4. Use those embeddings to make predictions\n",
        "5. Write those predictions to `predictions.json`\n",
        "\n",
        "Feel free to change the code below. Code for the `tf2baseline.*` functions is included in the `tf2_0_baseline_w_bert` utility script, and can be customized, whether by forking the utility script and updating it, or by creating your own non-`tf2baseline` versions in this kernel.\n",
        "\n",
        "Note: the `tf2_0_baseline_w_bert` utility script contains code for training your own embeddings. Here that code is removed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uhXiuA8l8inG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4afab406-862d-45c8-908a-99702b814096"
      },
      "source": [
        "bert_config = modeling.BertConfig.from_json_file(FLAGS.bert_config_file)\n",
        "\n",
        "tf2baseline.validate_flags_or_throw(bert_config)\n",
        "tf.io.gfile.makedirs(FLAGS.output_dir)\n",
        "\n",
        "tokenizer = tokenization.FullTokenizer(\n",
        "    vocab_file=FLAGS.vocab_file, do_lower_case=FLAGS.do_lower_case)\n",
        "\n",
        "run_config = tf.estimator.RunConfig(\n",
        "    model_dir=FLAGS.output_dir,\n",
        "    save_checkpoints_steps=FLAGS.save_checkpoints_steps)\n",
        "\n",
        "num_train_steps = None\n",
        "num_warmup_steps = None\n",
        "\n",
        "model_fn = tf2baseline.model_fn_builder(\n",
        "    bert_config=bert_config,\n",
        "    init_checkpoint=FLAGS.init_checkpoint,\n",
        "    learning_rate=FLAGS.learning_rate,\n",
        "    num_train_steps=num_train_steps,\n",
        "    num_warmup_steps=num_warmup_steps,\n",
        "    use_tpu=FLAGS.use_tpu,\n",
        "    use_one_hot_embeddings=FLAGS.use_one_hot_embeddings)\n",
        "\n",
        "estimator = tf.estimator.Estimator(\n",
        "    model_fn=model_fn,\n",
        "    config=run_config,\n",
        "    params={'batch_size':FLAGS.train_batch_size})\n",
        "\n",
        "\n",
        "if FLAGS.do_predict:\n",
        "  if not FLAGS.output_prediction_file:\n",
        "    raise ValueError(\n",
        "        \"--output_prediction_file must be defined in predict mode.\")\n",
        "    \n",
        "  eval_examples = tf2baseline.read_nq_examples(\n",
        "      input_file=FLAGS.predict_file, is_training=False)\n",
        "\n",
        "  print(\"FLAGS.predict_file\", FLAGS.predict_file)\n",
        "\n",
        "  eval_writer = tf2baseline.FeatureWriter(\n",
        "      filename=os.path.join(FLAGS.output_dir, \"eval.tf_record\"),\n",
        "      is_training=False)\n",
        "  eval_features = []\n",
        "\n",
        "  def append_feature(feature):\n",
        "    eval_features.append(feature)\n",
        "    eval_writer.process_feature(feature)\n",
        "\n",
        "  num_spans_to_ids = tf2baseline.convert_examples_to_features(\n",
        "      examples=eval_examples,\n",
        "      tokenizer=tokenizer,\n",
        "      is_training=False,\n",
        "      output_fn=append_feature)\n",
        "  eval_writer.close()\n",
        "  eval_filename = eval_writer.filename\n",
        "\n",
        "  print(\"***** Running predictions *****\")\n",
        "  print(f\"  Num orig examples = %d\" % len(eval_examples))\n",
        "  print(f\"  Num split examples = %d\" % len(eval_features))\n",
        "  print(f\"  Batch size = %d\" % FLAGS.predict_batch_size)\n",
        "  for spans, ids in num_spans_to_ids.items():\n",
        "    print(f\"  Num split into %d = %d\" % (spans, len(ids)))\n",
        "\n",
        "  predict_input_fn = tf2baseline.input_fn_builder(\n",
        "      input_file=eval_filename,\n",
        "      seq_length=FLAGS.max_seq_length,\n",
        "      is_training=False,\n",
        "      drop_remainder=False)\n",
        "\n",
        "  all_results = []\n",
        "\n",
        "  for result in estimator.predict(\n",
        "      predict_input_fn, yield_single_examples=True):\n",
        "    if len(all_results) % 1000 == 0:\n",
        "      print(\"Processing example: %d\" % (len(all_results)))\n",
        "\n",
        "    unique_id = int(result[\"unique_ids\"])\n",
        "    start_logits = [float(x) for x in result[\"start_logits\"].flat]\n",
        "    end_logits = [float(x) for x in result[\"end_logits\"].flat]\n",
        "    answer_type_logits = [float(x) for x in result[\"answer_type_logits\"].flat]\n",
        "\n",
        "    all_results.append(\n",
        "        tf2baseline.RawResult(\n",
        "            unique_id=unique_id,\n",
        "            start_logits=start_logits,\n",
        "            end_logits=end_logits,\n",
        "            answer_type_logits=answer_type_logits))\n",
        "\n",
        "  print (\"Going to candidates file\")\n",
        "\n",
        "  candidates_dict = tf2baseline.read_candidates(FLAGS.predict_file)\n",
        "\n",
        "  print (\"setting up eval features\")\n",
        "\n",
        "  raw_dataset = tf.data.TFRecordDataset(eval_filename)\n",
        "  eval_features = []\n",
        "  for raw_record in raw_dataset:\n",
        "    eval_features.append(tf.train.Example.FromString(raw_record.numpy()))\n",
        "    \n",
        "  print (\"compute_pred_dict\")\n",
        "\n",
        "  nq_pred_dict = tf2baseline.compute_pred_dict(candidates_dict, eval_features,\n",
        "                                   [r._asdict() for r in all_results])\n",
        "  predictions_json = {\"predictions\": list(nq_pred_dict.values())}\n",
        "\n",
        "  print (\"writing json\")\n",
        "\n",
        "  with tf.io.gfile.GFile(FLAGS.output_prediction_file, \"w\") as f:\n",
        "    json.dump(predictions_json, f, indent=4)"
      ],
      "execution_count": 155,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Using config: {'_model_dir': '/content/output', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 1000, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n",
            "graph_options {\n",
            "  rewrite_options {\n",
            "    meta_optimizer_iterations: ONE\n",
            "  }\n",
            "}\n",
            ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fc240304588>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Using config: {'_model_dir': '/content/output', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 1000, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n",
            "graph_options {\n",
            "  rewrite_options {\n",
            "    meta_optimizer_iterations: ONE\n",
            "  }\n",
            "}\n",
            ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fc240304588>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "FLAGS.predict_file /content/data/simplified-nq-test.jsonl\n",
            "***** Running predictions *****\n",
            "  Num orig examples = 346\n",
            "  Num split examples = 9409\n",
            "  Batch size = 8\n",
            "  Num split into 3 = 8\n",
            "  Num split into 19 = 9\n",
            "  Num split into 50 = 5\n",
            "  Num split into 2 = 6\n",
            "  Num split into 34 = 6\n",
            "  Num split into 54 = 1\n",
            "  Num split into 40 = 7\n",
            "  Num split into 42 = 3\n",
            "  Num split into 22 = 7\n",
            "  Num split into 11 = 12\n",
            "  Num split into 29 = 8\n",
            "  Num split into 102 = 1\n",
            "  Num split into 60 = 3\n",
            "  Num split into 10 = 12\n",
            "  Num split into 21 = 6\n",
            "  Num split into 41 = 4\n",
            "  Num split into 6 = 7\n",
            "  Num split into 35 = 8\n",
            "  Num split into 23 = 4\n",
            "  Num split into 32 = 7\n",
            "  Num split into 17 = 10\n",
            "  Num split into 85 = 1\n",
            "  Num split into 30 = 6\n",
            "  Num split into 9 = 8\n",
            "  Num split into 1 = 7\n",
            "  Num split into 57 = 3\n",
            "  Num split into 5 = 9\n",
            "  Num split into 28 = 5\n",
            "  Num split into 31 = 7\n",
            "  Num split into 18 = 6\n",
            "  Num split into 47 = 5\n",
            "  Num split into 4 = 12\n",
            "  Num split into 67 = 1\n",
            "  Num split into 45 = 4\n",
            "  Num split into 27 = 7\n",
            "  Num split into 8 = 9\n",
            "  Num split into 63 = 1\n",
            "  Num split into 43 = 5\n",
            "  Num split into 13 = 9\n",
            "  Num split into 12 = 6\n",
            "  Num split into 16 = 6\n",
            "  Num split into 24 = 2\n",
            "  Num split into 14 = 4\n",
            "  Num split into 53 = 4\n",
            "  Num split into 20 = 5\n",
            "  Num split into 15 = 6\n",
            "  Num split into 7 = 6\n",
            "  Num split into 44 = 2\n",
            "  Num split into 112 = 1\n",
            "  Num split into 37 = 5\n",
            "  Num split into 46 = 3\n",
            "  Num split into 39 = 5\n",
            "  Num split into 87 = 1\n",
            "  Num split into 48 = 1\n",
            "  Num split into 33 = 2\n",
            "  Num split into 66 = 1\n",
            "  Num split into 49 = 3\n",
            "  Num split into 62 = 2\n",
            "  Num split into 125 = 1\n",
            "  Num split into 36 = 6\n",
            "  Num split into 26 = 8\n",
            "  Num split into 76 = 2\n",
            "  Num split into 121 = 1\n",
            "  Num split into 38 = 6\n",
            "  Num split into 55 = 1\n",
            "  Num split into 25 = 7\n",
            "  Num split into 56 = 3\n",
            "  Num split into 82 = 1\n",
            "  Num split into 58 = 1\n",
            "  Num split into 98 = 1\n",
            "  Num split into 52 = 1\n",
            "  Num split into 89 = 1\n",
            "  Num split into 73 = 1\n",
            "  Num split into 187 = 1\n",
            "INFO:tensorflow:Could not find trained model in model_dir: /content/output, running initialization to predict.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Could not find trained model in model_dir: /content/output, running initialization to predict.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Calling model_fn.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Calling model_fn.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Done calling model_fn.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Done calling model_fn.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Graph was finalized.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Graph was finalized.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Running local_init_op.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Running local_init_op.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Done running local_init_op.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Done running local_init_op.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Processing example: 0\n",
            "Processing example: 1000\n",
            "Processing example: 2000\n",
            "Processing example: 3000\n",
            "Processing example: 4000\n",
            "Processing example: 5000\n",
            "Processing example: 6000\n",
            "Processing example: 7000\n",
            "Processing example: 8000\n",
            "Processing example: 9000\n",
            "Going to candidates file\n",
            "setting up eval features\n",
            "compute_pred_dict\n",
            "Examples processed: 100\n",
            "Examples processed: 200\n",
            "Examples processed: 300\n",
            "writing json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8sZ10NKAJhhU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "75b4bfbb-fbc0-42d5-e633-d5af3abe0444"
      },
      "source": [
        "%%bash\n",
        "zdump PST\n",
        "ls -l /content/output/"
      ],
      "execution_count": 156,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "PST  Mon Dec 16 10:47:01 2019 PST\n",
            "total 24236\n",
            "-rw-r--r-- 1 root root 24815064 Dec 16 10:39 eval.tf_record\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JewhO67Y8v8h",
        "colab_type": "text"
      },
      "source": [
        "**Now, we turn `predictions.json` into a `submission.csv` file.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2hG3FVACGlEj",
        "colab_type": "text"
      },
      "source": [
        "Note: In most recent run predictions.json was not created from above code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EzQ38Vqh818y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "outputId": "5cf62574-d530-48b1-8a4a-93e3e3398eb7"
      },
      "source": [
        "test_answers_df = pd.read_json(\"/content/output/predictions.json\")"
      ],
      "execution_count": 157,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-157-7363f007f1a8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_answers_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/output/predictions.json\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/json/_json.py\u001b[0m in \u001b[0;36mread_json\u001b[0;34m(path_or_buf, orient, typ, dtype, convert_axes, convert_dates, keep_default_dates, numpy, precise_float, date_unit, encoding, lines, chunksize, compression)\u001b[0m\n\u001b[1;32m    590\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mjson_reader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 592\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    593\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mshould_close\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    594\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/json/_json.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    715\u001b[0m             \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_object_parser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_combine_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 717\u001b[0;31m             \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_object_parser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    718\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    719\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/json/_json.py\u001b[0m in \u001b[0;36m_get_object_parser\u001b[0;34m(self, json)\u001b[0m\n\u001b[1;32m    737\u001b[0m         \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    738\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtyp\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"frame\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 739\u001b[0;31m             \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFrameParser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    741\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtyp\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"series\"\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/json/_json.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    847\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    848\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 849\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parse_no_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    850\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    851\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/json/_json.py\u001b[0m in \u001b[0;36m_parse_no_numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1091\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0morient\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"columns\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1092\u001b[0m             self.obj = DataFrame(\n\u001b[0;32m-> 1093\u001b[0;31m                 \u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprecise_float\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprecise_float\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1094\u001b[0m             )\n\u001b[1;32m   1095\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0morient\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"split\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Expected object or value"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AOsY9ciQ86T9",
        "colab_type": "text"
      },
      "source": [
        "The Bert model produces a `confidence` score, which the Kaggle metric does not use. You, however, can use that score to determine which answers get submitted. See the limits commented out in `create_short_answer` and `create_long_answer` below for an example.\n",
        "\n",
        "Values for `confidence` will range between `1.0` and `2.0`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_pLLGrp68_dx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_short_answer(entry):\n",
        "    # if entry[\"short_answers_score\"] < 1.5:\n",
        "    #     return \"\"\n",
        "    \n",
        "    answer = []    \n",
        "    for short_answer in entry[\"short_answers\"]:\n",
        "        if short_answer[\"start_token\"] > -1:\n",
        "            answer.append(str(short_answer[\"start_token\"]) + \":\" + str(short_answer[\"end_token\"]))\n",
        "    if entry[\"yes_no_answer\"] != \"NONE\":\n",
        "        answer.append(entry[\"yes_no_answer\"])\n",
        "    return \" \".join(answer)\n",
        "\n",
        "def create_long_answer(entry):\n",
        "   # if entry[\"long_answer_score\"] < 1.5:\n",
        "   # return \"\"\n",
        "\n",
        "    answer = []\n",
        "    if entry[\"long_answer\"][\"start_token\"] > -1:\n",
        "        answer.append(str(entry[\"long_answer\"][\"start_token\"]) + \":\" + str(entry[\"long_answer\"][\"end_token\"]))\n",
        "    return \" \".join(answer)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qGPtPWt-9Ezv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_answers_df[\"long_answer_score\"] = test_answers_df[\"predictions\"].apply(lambda q: q[\"long_answer_score\"])\n",
        "test_answers_df[\"short_answer_score\"] = test_answers_df[\"predictions\"].apply(lambda q: q[\"short_answers_score\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BAYeB3b49FD7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_answers_df[\"long_answer_score\"].describe()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P8CKpPl_9FlK",
        "colab_type": "text"
      },
      "source": [
        "An example of what each sample's answers look like in `prediction.json`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dZq25BPS9F4K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_answers_df.predictions.values[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wrV1V3bY9GLX",
        "colab_type": "text"
      },
      "source": [
        "We re-format the JSON answers to match the requirements for submission."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jTkDHMbH9GdD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_answers_df[\"long_answer\"] = test_answers_df[\"predictions\"].apply(create_long_answer)\n",
        "test_answers_df[\"short_answer\"] = test_answers_df[\"predictions\"].apply(create_short_answer)\n",
        "test_answers_df[\"example_id\"] = test_answers_df[\"predictions\"].apply(lambda q: str(q[\"example_id\"]))\n",
        "\n",
        "long_answers = dict(zip(test_answers_df[\"example_id\"], test_answers_df[\"long_answer\"]))\n",
        "short_answers = dict(zip(test_answers_df[\"example_id\"], test_answers_df[\"short_answer\"]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GfNRzj9m9IJI",
        "colab_type": "text"
      },
      "source": [
        "Then we add them to our sample submission. Recall that each sample has both a `_long` and `_short` entry in the sample submission, one for each type of answer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YzzaCu2N9IcO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sample_submission = pd.read_csv(\"/content/data/tensorflow2-question-answering/sample_submission.csv\")\n",
        "\n",
        "long_prediction_strings = sample_submission[sample_submission[\"example_id\"].str.contains(\"_long\")].apply(lambda q: long_answers[q[\"example_id\"].replace(\"_long\", \"\")], axis=1)\n",
        "short_prediction_strings = sample_submission[sample_submission[\"example_id\"].str.contains(\"_short\")].apply(lambda q: short_answers[q[\"example_id\"].replace(\"_short\", \"\")], axis=1)\n",
        "\n",
        "sample_submission.loc[sample_submission[\"example_id\"].str.contains(\"_long\"), \"PredictionString\"] = long_prediction_strings\n",
        "sample_submission.loc[sample_submission[\"example_id\"].str.contains(\"_short\"), \"PredictionString\"] = short_prediction_strings"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YpuRebPx9I98",
        "colab_type": "text"
      },
      "source": [
        "And finally, we write out our submission!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lT0rej2E9JTr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sample_submission.to_csv(\"submission.csv\", index=False)\n",
        "sample_submission.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "akKwcXNnJvi2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%bash\n",
        "zdump PST\n",
        "ls -l /content/output/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CmkZDNQK98df",
        "colab_type": "text"
      },
      "source": [
        "## -- Submitting Results --"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rKE9TaFxJx1P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "assert True                     ## Protect from being executed"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rYwGNnso97Yr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%bash\n",
        "## View Previous Results\n",
        "#kaggle competitions list\n",
        "kaggle competitions submissions -c tensorflow2-question-answering"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rr1JY__B-PnG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Make Submission\n",
        "# I am not sure if we can submit this competition from this as it has to be a kernel submission\n",
        "#! kaggle competitions submit -c tensorflow2-question-answering -f $RESULT_CSV  -m 'test kaggle cli 3'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sBJjB1J3-fUL",
        "colab_type": "text"
      },
      "source": [
        "Verify submission by viewing previous results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "lRKAhZK8yvO_"
      },
      "source": [
        "End of Project Notebook\n",
        "# ====== Please fold this stuff up and ignore ====="
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "r_TVSYoeyvPa"
      },
      "source": [
        "### SSH Setup\n",
        "This is only neeeded if you want to log into the Colab machine. Otherwise fold it up and ignore.<br>\n",
        "To use it you have to create a login at https://ngrok.com\n",
        "<Details>Thanks to Imad El Hanafi (https://imadelhanafi.com) for showing me how to do this.<p>\n",
        "You will need to create a free account at https://ngrok.com/ for the SSH tunnel to work.</Details>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Udo19R6y9LD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "assert False        # Make sure user does not accedentially drop into this code"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Y0NMPj1fyvPb",
        "colab": {}
      },
      "source": [
        "%%bash\n",
        "## Install sshd; Set to allow login and config\n",
        "apt-get install -o=Dpkg::Use-Pty=0 openssh-server pwgen > /dev/null\n",
        "mkdir -p /var/run/sshd\n",
        "echo \"PermitRootLogin yes\" >> /etc/ssh/sshd_config\n",
        "echo \"PasswordAuthentication yes\" >> /etc/ssh/sshd_config\n",
        "# set host key to known value (need to test if exist)\n",
        "if [ -f \"/content/bertqa/colab/ssh_host_rsa_key.pub\" ]; then\n",
        "    cp \"/content/bertqa/colab/ssh_host_rsa_key.pub\" /etc/ssh/\n",
        "    echo \"Using ssh_host_rsa_key from gdrive\"\n",
        "fi\n",
        "# this script will give fix the login shell so Python will work\n",
        "if [ -f \"/content/bertqa/colab/init_shell.sh\" ]; then\n",
        "    echo \"source /content/bertqa/colab/init_shell.sh\" >> /root/.bashrc\n",
        "fi"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "S1mmJ__lyvPh",
        "colab": {}
      },
      "source": [
        "## setup ssh user / pass and start sshd\n",
        "\n",
        "#Generate a random root password\n",
        "import random, string\n",
        "sshpass = ''.join(random.choice(string.ascii_letters + string.digits) for i in range(30))\n",
        "\n",
        "#Set root password\n",
        "! echo root:$sshpass | chpasswd\n",
        "\n",
        "#Run sshd\n",
        "get_ipython().system_raw('/usr/sbin/sshd -D &')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "edJ3pW6YyvPl",
        "colab": {}
      },
      "source": [
        "%%bash\n",
        "## Get Ngrok from gdrive or try to download (see: https://ngrok.com/download)\n",
        "if [ -f \"/content/bertqa/colab/ngrok-stable-linux-amd64.zip\" ]; then\n",
        "    cp \"/content/bertqa/colab/ngrok-stable-linux-amd64.zip\" .\n",
        "    echo \"Using ngrok-stable-linux-amd64.zip from gdrive\"\n",
        "else\n",
        "    wget -q -c -nc https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
        "fi\n",
        "unzip -qq -n ngrok-stable-linux-amd64.zip\n",
        "rm ngrok-stable-linux-amd64.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YR0N4Iw8yvPq",
        "colab": {}
      },
      "source": [
        "## Get user to enter auth token from ngrok and start tunnel\n",
        "\n",
        "# Get token from ngrok for the tunnel\n",
        "print(\"Get your authtoken from https://dashboard.ngrok.com/auth\")\n",
        "import getpass\n",
        "authtoken = getpass.getpass()\n",
        "\n",
        "#Create tunnel\n",
        "get_ipython().system_raw('./ngrok authtoken $authtoken && ./ngrok tcp 22 &')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "1VjsRvCTyvPt"
      },
      "source": [
        "#### ==============================<br>|====&nbsp;&nbsp;  SSH Login Credentials &nbsp;&nbsp;====||<br>=============================="
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "cellView": "both",
        "id": "WKjt0Wh0yvPv",
        "colab": {}
      },
      "source": [
        "#@title\n",
        "print(\"username: root\")\n",
        "print(\"password: \", sshpass)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7OlwbxpWyvPz"
      },
      "source": [
        "Get the host name and port number at: https://dashboard.ngrok.com/status\n",
        "\n",
        "```bash\n",
        "ssh root@0.tcp.ngrok.io -p [ngrok_port]\n",
        "Login as: root\n",
        "Servrer refused our key\n",
        "root@0.tcp.ngrok.io's password: [see above]\n",
        "\n",
        "(Colab):/content$\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "bjJcssgxyvP0"
      },
      "source": [
        "Install vim"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "euzlUBHLyvP1",
        "colab": {}
      },
      "source": [
        "! apt-get install vim > /dev/null"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Ugtt5PxzyvP3"
      },
      "source": [
        "If you need to kill Ngrok run this cell"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CiP9qYfgyvP4",
        "colab": {}
      },
      "source": [
        "if EnableAllCode and False:\n",
        "    !kill $(ps aux | grep './ngrok' | awk '{print $2}')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "n3MP_pJ5yvP5"
      },
      "source": [
        "## -- Misc Notes --"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "cm6ErVGkyvP6"
      },
      "source": [
        "### Prevent Disconnects\n",
        "Colab periodically disconnects the browser.<br>\n",
        "You have to save model checkpoints to Google Drive so you don't lose work<br>\n",
        "See: https://mc.ai/google-colab-drive-as-persistent-storage-for-long-training-runs/<br>\n",
        "Something to try...<br>\n",
        "Ctrl+Shift+i in browser and in console run this code...\n",
        "```\n",
        "function KeepAlive(){\n",
        "    console.log(\"Maintaining Connection\");\n",
        "    document.querySelector(\"colab-toolbar-button#connect\").click()\n",
        "}\n",
        "setInterval(KeepAlive,60000);\n",
        "```\n",
        "There have been reports of people having their GPU privileges suspended for letting processes run for over 12 hours. It seems that they may penalize you rather than just cutting you off."
      ]
    }
  ]
}