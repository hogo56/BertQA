{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT for Humans Baseline.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hogo56/BertQA/blob/master/BERT_for_Humans_Baseline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M0k60Je4YEQa",
        "colab_type": "text"
      },
      "source": [
        "## --Main System Config--\n",
        "<Details><Summary>Global Config</Summary>\n",
        "Put any global system configuration here"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JNKoTizrh9RG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os, sys\n",
        "## Setup directory structure\n",
        "! mkdir /content/lib\n",
        "! mkdir -p /content/data\n",
        "! mkdir -p /content/bert_output       # Maybe output to Google Drive for durability\n",
        "sys.path.append('/content/lib')\n",
        "\n",
        "! apt-get install vim > /dev/null"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v-5P0vP3PlaX",
        "colab_type": "code",
        "outputId": "e3e10503-1f98-4fd0-a500-556f661a472f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 116
        }
      },
      "source": [
        "! pwd\n",
        "! ls -l"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n",
            "total 16\n",
            "drwxr-xr-x 2 root root 4096 Dec 15 05:57 bert_output\n",
            "drwxr-xr-x 2 root root 4096 Dec 15 05:57 data\n",
            "drwxr-xr-x 2 root root 4096 Dec 15 05:57 lib\n",
            "drwxr-xr-x 1 root root 4096 Dec 12 16:48 sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "88YILsdhCzqR",
        "colab_type": "text"
      },
      "source": [
        "### Runtime Parameters\n",
        "EnableAllCode - There are code blocks here that should not be run with \"Run All\". By default EnableAllCode will set to False and those blocks will be excluded. If you want to run them individually for some reason set EnableAllCode True.<p>\n",
        "DownloadBigFiles - There are GBs of files and downloads to make this run. If you are just wanting to spin up the Colab so you can SSH into it set DownloadBigFiles = False then Runtime -> RunAfter"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uw9zV9bwScZe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "EnableAllCode = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N1OMFCqATMSX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "EnableAllCode = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BYv6ZKgbEE4w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DownloadBigFiles = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aw7EKdk1D_UZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DownloadBigFiles = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aheaRzVE6fs1",
        "colab_type": "text"
      },
      "source": [
        "## -- Setup --"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OQGgsjkY0Rwh",
        "colab_type": "text"
      },
      "source": [
        "### Machine Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gixU6_gv678n",
        "colab_type": "text"
      },
      "source": [
        "--- Google Drive\n",
        "<Details>There are several ways to provide access to your Google Drive from Colab. (What about the Drive FUSE wrapper?)<br>\n",
        "I am not sure if this is the best. This mounts your Drive into the machine.<br>\n",
        "I expect there will be a folder in the Drive that we all share.</Details>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "auXx45x70Qcs",
        "colab_type": "code",
        "outputId": "efd286dd-179d-4d74-be27-6142c1eecfed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120
        }
      },
      "source": [
        "## File link to Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=False)   # true to reread drive\n",
        "# Create a shorter shared directory name than one with a space\n",
        "! ln -s '/content/gdrive/My Drive/bertqa' /content/bertqa"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ga-sQOkk1YvA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if EnableAllCode:\n",
        "    ## Flush and unmount Google Drive\n",
        "    # You probablyu won't do this but if you want to at some point click the play button\n",
        "    drive.flush_and_unmount()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "YfDRXH9NYmi3"
      },
      "source": [
        "### SSH Setup\n",
        "<Details> Shell access is not technically required but if you are like me and being able to look under the hood brings you comfort you can step throuth this.<br>\n",
        "Thanks to Imad El Hanafi (https://imadelhanafi.com) for showing me how to do this.<p>\n",
        "I think you will need to create a free account at https://ngrok.com/ for the SSH tunnel to work.</Details>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9eXiaG8E9jqP",
        "colab_type": "code",
        "outputId": "6acfcefe-2a47-4e44-afb4-5c5da4c430dc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        }
      },
      "source": [
        "## Install sshd\n",
        "! apt-get install -o=Dpkg::Use-Pty=0 openssh-server pwgen > /dev/null\n",
        "\n",
        "! mkdir -p /var/run/sshd\n",
        "! echo \"PermitRootLogin yes\" >> /etc/ssh/sshd_config\n",
        "! echo \"PasswordAuthentication yes\" >> /etc/ssh/sshd_config"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Creating config file /etc/ssh/sshd_config with new version\n",
            "Creating SSH2 RSA key; this may take some time ...\n",
            "2048 SHA256:L2IuDmRtem6ndIbxXVQc9wI7t8Nnrrv57Ec3xnNeoPY root@e0f2060c6019 (RSA)\n",
            "Creating SSH2 ECDSA key; this may take some time ...\n",
            "256 SHA256:YA0fsId2x+HpV71juKoqj20elKIOG4NpJFf7fDG2gU0 root@e0f2060c6019 (ECDSA)\n",
            "Creating SSH2 ED25519 key; this may take some time ...\n",
            "256 SHA256:kkij3OU4iH2J/fYocli3gaqaPEGGCXQto98bTsplxQ4 root@e0f2060c6019 (ED25519)\n",
            "Created symlink /etc/systemd/system/sshd.service → /lib/systemd/system/ssh.service.\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/ssh.service → /lib/systemd/system/ssh.service.\n",
            "invoke-rc.d: could not determine current runlevel\n",
            "invoke-rc.d: policy-rc.d denied execution of start.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zypzu9GdYmjH",
        "outputId": "6c52e7d6-b804-4a73-cfb6-3d0c6029f370",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66
        }
      },
      "source": [
        "# 1 - setup ssh/user \n",
        "\n",
        "#Generate a random root password\n",
        "import random, string\n",
        "sshpass = ''.join(random.choice(string.ascii_letters + string.digits) for i in range(30))\n",
        "\n",
        "#Set root password\n",
        "! echo root:$sshpass | chpasswd\n",
        "\n",
        "print()\n",
        "print(\"###### SSH Login Credentials ######\")\n",
        "print(\"username: root\")\n",
        "print(\"password: \", sshpass)\n",
        "\n",
        "#Run sshd\n",
        "get_ipython().system_raw('/usr/sbin/sshd -D &')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "###### SSH Login Credentials ######\n",
            "username: root\n",
            "password:  46U4SlVWMjv8RiV2Xn6l1jbakC4GYr\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "9I6mIU8nYmjb"
      },
      "source": [
        "On Ngrok dashboard https://dashboard.ngrok.com/status you'll find the tcp address and the port you can connect to with your favorite ssh client. eg...\n",
        "\n",
        "```\n",
        "ssh root@0.tcp.ngrok.io -p [ngrok_port]\n",
        "> then enter the username and password shown from the codeblock above\n",
        "\n",
        "root@hostname:~ PS1=':\\w\\$ '     (or whatever you want)\n",
        ":~# vim .bashrc        (if you want to change prompt PS1 there; also add cd/content to end)\n",
        ":~# cd /content\n",
        ":/content# ls -lh \n",
        "```\n",
        "If you want to keep your ssh sessions from being disconnected you need to have your client send a KeepAlive about every 200 seconds."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wMqEbK_gYmjP",
        "colab": {}
      },
      "source": [
        "# 2 - Download Ngrok\n",
        "! wget -q -c -nc https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
        "! unzip -qq -n ngrok-stable-linux-amd64.zip\n",
        "! rm ngrok-stable-linux-amd64.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "T3qqUMr8YmjX",
        "outputId": "93d8f652-76ec-4c14-b351-1bbed06a35f3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "# 3 - setup Ngrok - authtoken\n",
        "\n",
        "#Ask token\n",
        "print(\"Get your authtoken from https://dashboard.ngrok.com/auth\")\n",
        "import getpass\n",
        "authtoken = getpass.getpass()\n",
        "\n",
        "#Create tunnel\n",
        "get_ipython().system_raw('./ngrok authtoken $authtoken && ./ngrok tcp 22 &')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Get your authtoken from https://dashboard.ngrok.com/auth\n",
            "··········\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_gkQz2BDYmjb",
        "colab": {}
      },
      "source": [
        "if EnableAllCode:\n",
        "    # When done, kill Ngrok\n",
        "    !kill $(ps aux | grep './ngrok' | awk '{print $2}')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q8BLJAW95ih-",
        "colab_type": "text"
      },
      "source": [
        "### Kaggle API\n",
        "<Details>You will need a token to link the Colab instance to the API of your Kaggle account to get data, etc.<br>\n",
        "Go to: https://www.kaggle.com/yourID/account and click on the \"Create New API Token: button to get a file named kaggle.json and save it somewhere on your local drive.</Details>\n",
        "<Details><Summary>More...</Summary>I am choosing to upload the kaggle.json file from local hard drive each time. Alternately, we could put it in a private location on your gdrive (not the shared folder.)<br>\n",
        "To use Drive the location in the environ would be like /content/gdrive/kaggle/</Details>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lc3IYPUg17c8",
        "colab_type": "code",
        "outputId": "d0090d64-8474-443f-aada-f00b05b94e17",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "## Link to Kaggle\n",
        "from google.colab import files\n",
        "\n",
        "# see if there is a kaggle.json file in gdrive\n",
        "try:\n",
        "    # see if auth file is in gdrive\n",
        "    f = open(\"/content/gdrive/My Drive/kaggle/kaggle.json\")\n",
        "    ! chmod 600 \"/content/gdrive/My Drive/kaggle/kaggle.json\"      # file is likely already 600\n",
        "    os.environ['KAGGLE_CONFIG_DIR'] = \"/content/gdrive/My Drive/kaggle/\"\n",
        "    ! ls -l \"/content/gdrive/My Drive/kaggle/kaggle.json\"\n",
        "except IOError:\n",
        "    # Have user upload file\n",
        "    ! rm /content/kaggle.json  2> /dev/null\n",
        "    print('Upload kaggle.json.')\n",
        "    # The files.upload() command is failing sporatically with:\n",
        "    #   TypeError: Cannot read property '_uploadFiles' of undefined (just run again)\n",
        "    files.upload()\n",
        "    ! chmod 600 /content/kaggle.json\n",
        "    os.environ['KAGGLE_CONFIG_DIR'] = \"/content/\"\n",
        "    ! ls -l /content/kaggle.json\n",
        "\n",
        "import kaggle"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-rw------- 1 root root 66 Dec 15 02:55 '/content/gdrive/My Drive/kaggle/kaggle.json'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gAqCNh4y8DJz",
        "colab_type": "text"
      },
      "source": [
        "### Download Dataset and Support Files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UF-28Eb21E2w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Competition Dataset  (5GB zipped)\n",
        "if DownloadBigFiles:\n",
        "    ! kaggle competitions list\n",
        "    ! kaggle competitions download -c tensorflow2-question-answering -p /content/data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "bNPxSmhMJC5c"
      },
      "source": [
        "### Library Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eoljj40aIiAc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 261
        },
        "outputId": "c0f326c6-642a-422a-d98d-8afe6370fb89"
      },
      "source": [
        "## Stop Run All Execution here\n",
        "# This is a really ugly way to get the Notebook to stop, any better ideas???\n",
        "\n",
        "print()\n",
        "print(\"### Stopping Notebook execution ###\")\n",
        "print(\"To continue Notebook put cursor on next cell and Runtime -> Run After\")\n",
        "print()\n",
        "assert False"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "### Stopping Notebook execution ###\n",
            "To continue Notebook put cursor on next cell and Runtime -> Run After\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-39f1c9cdf36f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"To continue Notebook put cursor on next cell and Runtime -> Run After\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F18Sq3F35xtR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Fix Nvidia            (make sure your Runtime type is python3 + GPU)\n",
        "\n",
        "\"\"\" ToDo - Get Latest Nvidia Drivers \"\"\"\n",
        "if EnableAllCode:   # (this is just disabled because the code is not tested)\n",
        "    # https://developer.nvidia.com/cudnn       (need to join developer program to get latest)\n",
        "\n",
        "    # Extracts the cuDNN files from Drive folder directly to the VM CUDA folders\n",
        "    !tar -xzvf bertqa/nvidia/cuDNN/cudnn-10.0-linux-x64-v7.5.0.56.tgz -C /usr/local/\n",
        "    !chmod a+r /usr/local/cuda/include/cudnn.h\n",
        "\n",
        "    # Now we check the version we already installed. Can comment this line on future runs\n",
        "    !cat /usr/local/cuda/include/cudnn.h | grep CUDNN_MAJOR -A 2\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yr69yF8fepTA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Copy lib files over from Google Drive\n",
        "! cp -a /content/bertqa/lib/* lib/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pdp6tqXmmLw_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if DownloadBigFiles:\n",
        "    ## get BERT (this is unlikely to be the BERT-joint files needed for competition)\n",
        "    # this version of BERT seems won't import as is. On line 88 of lib/bert/optimization.py\n",
        "    #    change   tr.train.Optimizer to tf.keras.optimizers.Optimizer\n",
        "    ! git clone https://github.com/google-research/bert.git\n",
        "    ! mv bert lib\n",
        "\n",
        "    # get some pretrained models  (I really  have no idea what these are or if useful)\n",
        "    ! wget https://storage.googleapis.com/bert_models/2018_10_18/cased_L-12_H-768_A-12.zip\n",
        "    ! unzip cased_L-12_H-768_A-12.zip\n",
        "    ! rm cased_L-12_H-768_A-12.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oU6FPUHjvH1r",
        "colab_type": "text"
      },
      "source": [
        "Are these the correct bert models we are supposed to be using?<br>\n",
        "Maybe also look at https://github.com/tensorflow/models/tree/master/official/nlp/bert or,<br>\n",
        "https://github.com/google-research/language/tree/master/language/question_answering/bert_joint\n",
        "\n",
        "<Details><Summary>BERT Notes</Summary>\n",
        "baseline_w_bert_translated_to_tf2_0 (next code block) comes from /dimitreoliveira with this warning:<br>\n",
        "This baseline uses code that was migrated from TF1.x. Be aware that it contains use of tf.compat.v1, which is not permitted to be eligible for TF2.0 prizes in this competition. It is intended to be used as a starting point, but we're excited to see how much better you can do using TF2.0!<br>\n",
        "https://www.kaggle.com/dimitreoliveira/using-tf-2-0-w-bert-on-nq-translated-to-tf2-0</Details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RKFSBd4nIaKx",
        "colab_type": "text"
      },
      "source": [
        "<Details><Summary>Bert Updates</Summary>\n",
        "On line 88 of lib/bert/optimization.py<br>\n",
        "Change the parent class from tr.train.Optimizer to tf.keras.optimizers.Optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qhpBRp11R0jv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Load Libraries\n",
        "\n",
        "%tensorflow_version 2.x \n",
        "import tensorflow\n",
        "print(tensorflow.__version__)\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "\n",
        "# import tf2_0_baseline_w_bert as tf2baseline # old script\n",
        "import tf2_0_baseline_w_bert_translated_to_tf2_0 as tf2baseline # Oliviera's script\n",
        "\n",
        "import bert.modeling as modeling\n",
        "import bert.optimization as optimization\n",
        "import bert.tokenization as tokenization\n",
        "\n",
        "import json\n",
        "import absl\n",
        "from zipfile import ZipFile"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q6zf1MNKGefD",
        "colab_type": "text"
      },
      "source": [
        "## --Explore Environment--\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nzX2trD0GY5-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if EnableAllCode:\n",
        "    print('===== Python =====')\n",
        "    print('Version: ', sys.version)\n",
        "    ! pip -V\n",
        "    print('===== CPU Info =====')\n",
        "    !cat /proc/cpuinfo\n",
        "    print('\\n===== MEM Info =====')\n",
        "    !cat /proc/meminfo\n",
        "    print('\\n===== printenv =====')\n",
        "    ! printenv\n",
        "    from tensorflow.python.client import device_lib\n",
        "    print('\\n===== List Local Devices =====')\n",
        "    print(device_lib.list_local_devices())\n",
        "    print()\n",
        "    print(\"===== Dir of /:\", *(os.listdir('/')), sep='\\n')\n",
        "    print(\"\\n===== cwd: \", os.getcwd())                       # will be /content\n",
        "    print('\\n===== Files in', os.getcwd())\n",
        "    for dirname, _, filenames in os.walk(os.getcwd()):\n",
        "        for filename in filenames:\n",
        "            print(os.path.join(dirname, filename))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rEo292sNysZ2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if EnableAllCode:\n",
        "    print('\\n===== NVIDIA Info =====')\n",
        "    !nvidia-smi\n",
        "    print()\n",
        "    !/usr/local/cuda/bin/nvcc --version"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VTGd9wM7VxHz",
        "colab_type": "text"
      },
      "source": [
        "## --Misc Notes--"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nZzPMe0vWHdP",
        "colab_type": "text"
      },
      "source": [
        "### File Transfer to Local Machine"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G4_3T1hiV_BI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ## To upload files to Colab\n",
        "# from google.colab import files\n",
        "# uploaded = files.upload()\n",
        "# print(uploaded)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e_ucQLj9V3kr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ## To download files from Colab\n",
        "# from google.colab import files\n",
        "# model.save('trained_model.h5')          # but probably better to save directly to Drive\n",
        "# files.download('trained_model.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XBAZEpofWU1J",
        "colab_type": "text"
      },
      "source": [
        "### File Transfer from Web"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kOzigjWWWYYn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from urllib.request import urlretrieve\n",
        "\n",
        "# def download(url, file):\n",
        "#     if not os.path.isfile(file):\n",
        "#         print(\"Downloading file... \" + file + \" ...\")\n",
        "#         urlretrieve(url,file)\n",
        "#         print(\"File downloaded\")\n",
        "\n",
        "# download('Url of the file','Name of the file to be saved')\n",
        "# print(\"All the files are downloaded\")\n",
        "# #If the downloaded file is a zip file than you can use below function to unzip it.\n",
        "# def uncompress_features_labels(file):\n",
        "#     if(os.path.isdir('data')):\n",
        "#         print('Data extracted')\n",
        "#     else:\n",
        "#         with ZipFile(dir) as zipf:\n",
        "#             zipf.extractall('data')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LOGMBL7znY_5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! printenv"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rzADEFEdzHor",
        "colab_type": "text"
      },
      "source": [
        "### Prevent Disconnects\n",
        "Colab periodically disconnects the browser.<br>\n",
        "You have to save model checkpoints to Google Drive so you don't lose wor<br>\n",
        "See: https://mc.ai/google-colab-drive-as-persistent-storage-for-long-training-runs/<br>\n",
        "Something to try...<br>\n",
        "Ctrl+Shift+i in browser and in console run this code...\n",
        "```\n",
        "function KeepAlive(){\n",
        "    console.log(\"Maintaining Connection\");\n",
        "    document.querySelector(\"colab-toolbar-button#connect\").click()\n",
        "}\n",
        "setInterval(KeepAlive,60000);\n",
        "```\n",
        "There have been reports of people having their GPU privileges suspended for letting processes run for over 12 hours. It seems that they may penalize you rather than just cutting you off."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ioz1gRjeJiB7",
        "colab_type": "text"
      },
      "source": [
        "## --Model Code--"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mdSi099XJsGa",
        "colab_type": "text"
      },
      "source": [
        "Tensorflow flags are variables that can be passed around within the TF system. Every flag below has some context provided regarding what the flag is and how it's used.<p>\n",
        "Most of these can be changed as desired, with the exception of the Special Flags at the bottom, which must stay as-is to work with the Kaggle back end."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DrfU_hZQJxWv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DoNotGoPastHere()\n",
        "def del_all_flags(FLAGS):\n",
        "    flags_dict = FLAGS._flags()\n",
        "    keys_list = [keys for keys in flags_dict]\n",
        "    for keys in keys_list:\n",
        "        FLAGS.__delattr__(keys)\n",
        "\n",
        "del_all_flags(absl.flags.FLAGS)\n",
        "\n",
        "flags = absl.flags\n",
        "\n",
        "flags.DEFINE_string(\n",
        "    \"bert_config_file\", \"/kaggle/input/bertjointbaseline/bert_config.json\",\n",
        "    \"The config json file corresponding to the pre-trained BERT model. \"\n",
        "    \"This specifies the model architecture.\")\n",
        "\n",
        "flags.DEFINE_string(\"vocab_file\", \"/kaggle/input/bertjointbaseline/vocab-nq.txt\",\n",
        "                    \"The vocabulary file that the BERT model was trained on.\")\n",
        "\n",
        "flags.DEFINE_string(\n",
        "    \"output_dir\", \"outdir\",\n",
        "    \"The output directory where the model checkpoints will be written.\")\n",
        "\n",
        "flags.DEFINE_string(\"train_precomputed_file\", None,\n",
        "                    \"Precomputed tf records for training.\")\n",
        "\n",
        "flags.DEFINE_integer(\"train_num_precomputed\", None,\n",
        "                     \"Number of precomputed tf records for training.\")\n",
        "\n",
        "flags.DEFINE_string(\n",
        "    \"output_prediction_file\", \"predictions.json\",\n",
        "    \"Where to print predictions in NQ prediction format, to be passed to\"\n",
        "    \"natural_questions.nq_eval.\")\n",
        "\n",
        "flags.DEFINE_string(\n",
        "    \"init_checkpoint\", \"/kaggle/input/bertjointbaseline/bert_joint.ckpt\",\n",
        "    \"Initial checkpoint (usually from a pre-trained BERT model).\")\n",
        "\n",
        "flags.DEFINE_bool(\n",
        "    \"do_lower_case\", True,\n",
        "    \"Whether to lower case the input text. Should be True for uncased \"\n",
        "    \"models and False for cased models.\")\n",
        "\n",
        "flags.DEFINE_integer(\n",
        "    \"max_seq_length\", 384,\n",
        "    \"The maximum total input sequence length after WordPiece tokenization. \"\n",
        "    \"Sequences longer than this will be truncated, and sequences shorter \"\n",
        "    \"than this will be padded.\")\n",
        "\n",
        "flags.DEFINE_integer(\n",
        "    \"doc_stride\", 128,\n",
        "    \"When splitting up a long document into chunks, how much stride to \"\n",
        "    \"take between chunks.\")\n",
        "\n",
        "flags.DEFINE_integer(\n",
        "    \"max_query_length\", 64,\n",
        "    \"The maximum number of tokens for the question. Questions longer than \"\n",
        "    \"this will be truncated to this length.\")\n",
        "\n",
        "flags.DEFINE_bool(\"do_train\", False, \"Whether to run training.\")\n",
        "\n",
        "flags.DEFINE_bool(\"do_predict\", True, \"Whether to run eval on the dev set.\")\n",
        "\n",
        "flags.DEFINE_integer(\"train_batch_size\", 32, \"Total batch size for training.\")\n",
        "\n",
        "flags.DEFINE_integer(\"predict_batch_size\", 8,\n",
        "                     \"Total batch size for predictions.\")\n",
        "\n",
        "flags.DEFINE_float(\"learning_rate\", 5e-5, \"The initial learning rate for Adam.\")\n",
        "\n",
        "flags.DEFINE_float(\"num_train_epochs\", 3.0,\n",
        "                   \"Total number of training epochs to perform.\")\n",
        "\n",
        "flags.DEFINE_float(\n",
        "    \"warmup_proportion\", 0.1,\n",
        "    \"Proportion of training to perform linear learning rate warmup for. \"\n",
        "    \"E.g., 0.1 = 10% of training.\")\n",
        "\n",
        "flags.DEFINE_integer(\"save_checkpoints_steps\", 1000,\n",
        "                     \"How often to save the model checkpoint.\")\n",
        "\n",
        "flags.DEFINE_integer(\"iterations_per_loop\", 1000,\n",
        "                     \"How many steps to make in each estimator call.\")\n",
        "\n",
        "flags.DEFINE_integer(\n",
        "    \"n_best_size\", 20,\n",
        "    \"The total number of n-best predictions to generate in the \"\n",
        "    \"nbest_predictions.json output file.\")\n",
        "\n",
        "flags.DEFINE_integer(\n",
        "    \"verbosity\", 1, \"How verbose our error messages should be\")\n",
        "\n",
        "flags.DEFINE_integer(\n",
        "    \"max_answer_length\", 30,\n",
        "    \"The maximum length of an answer that can be generated. This is needed \"\n",
        "    \"because the start and end predictions are not conditioned on one another.\")\n",
        "\n",
        "flags.DEFINE_float(\n",
        "    \"include_unknowns\", -1.0,\n",
        "    \"If positive, probability of including answers of type `UNKNOWN`.\")\n",
        "\n",
        "flags.DEFINE_bool(\"use_tpu\", False, \"Whether to use TPU or GPU/CPU.\")\n",
        "flags.DEFINE_bool(\"use_one_hot_embeddings\", False, \"Whether to use use_one_hot_embeddings\")\n",
        "\n",
        "absl.flags.DEFINE_string(\n",
        "    \"gcp_project\", None,\n",
        "    \"[Optional] Project name for the Cloud TPU-enabled project. If not \"\n",
        "    \"specified, we will attempt to automatically detect the GCE project from \"\n",
        "    \"metadata.\")\n",
        "\n",
        "flags.DEFINE_bool(\n",
        "    \"verbose_logging\", False,\n",
        "    \"If true, all of the warnings related to data processing will be printed. \"\n",
        "    \"A number of warnings are expected for a normal NQ evaluation.\")\n",
        "\n",
        "flags.DEFINE_boolean(\n",
        "    \"skip_nested_contexts\", True,\n",
        "    \"Completely ignore context that are not top level nodes in the page.\")\n",
        "\n",
        "flags.DEFINE_integer(\"task_id\", 0,\n",
        "                     \"Train and dev shard to read from and write to.\")\n",
        "\n",
        "flags.DEFINE_integer(\"max_contexts\", 48,\n",
        "                     \"Maximum number of contexts to output for an example.\")\n",
        "\n",
        "flags.DEFINE_integer(\n",
        "    \"max_position\", 50,\n",
        "    \"Maximum context position for which to generate special tokens.\")\n",
        "\n",
        "\n",
        "## Special flags - do not change\n",
        "\n",
        "flags.DEFINE_string(\n",
        "    \"predict_file\", \"/kaggle/input/tensorflow2-question-answering/simplified-nq-test.jsonl\",\n",
        "    \"NQ json for predictions. E.g., dev-v1.1.jsonl.gz or test-v1.1.jsonl.gz\")\n",
        "flags.DEFINE_boolean(\"logtostderr\", True, \"Logs to stderr\")\n",
        "flags.DEFINE_boolean(\"undefok\", True, \"it's okay to be undefined\")\n",
        "flags.DEFINE_string('f', '', 'kernel')\n",
        "flags.DEFINE_string('HistoryManager.hist_file', '', 'kernel')\n",
        "\n",
        "FLAGS = flags.FLAGS\n",
        "FLAGS(sys.argv) # Parse the flags"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}