{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERTjoint_yes_no_Howard.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "CmkZDNQK98df",
        "lRKAhZK8yvO_"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "7599dcbaa912496bbeda880686b8f25a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_61e7ed87ebd94acba7696eed19e1cc13",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_0d4b78ce0101416a9e707dc8b5eab0f8",
              "IPY_MODEL_1fb94434d0624959a48177015e3bc053"
            ]
          }
        },
        "61e7ed87ebd94acba7696eed19e1cc13": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0d4b78ce0101416a9e707dc8b5eab0f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_15e56d4afc81435784a1e9d579fd45c1",
            "_dom_classes": [],
            "description": "",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 346,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 346,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_7a4189c5a1fd4e69807c3567205224ac"
          }
        },
        "1fb94434d0624959a48177015e3bc053": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_bb45125c044448e9bfd30144af68a3f1",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100% 346/346 [00:16&lt;00:00, 20.86it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_5a684f810ebf4149a4c1fa6b9ec9f18e"
          }
        },
        "15e56d4afc81435784a1e9d579fd45c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "7a4189c5a1fd4e69807c3567205224ac": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "bb45125c044448e9bfd30144af68a3f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "5a684f810ebf4149a4c1fa6b9ec9f18e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hogo56/BertQA/blob/master/BERTjoint_yes_no_Howard.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Js7aj8RDhSjz",
        "colab_type": "text"
      },
      "source": [
        "# BERTjoint Question Answering Contest"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SA1MAb09BlQB",
        "colab_type": "text"
      },
      "source": [
        "I prefer to do my primary development in a Colab virtual machine but, competitions require your kernel to run on Kaggle for scoring. You can start with this notebook and add your own project code which should run in either location if you use the directory variables for file locations and correctly configure data and user libraries in Kaggle.<p>\n",
        "The main differences are the file locations. On Colab you will symlink to a google Drive and for Kaggle you will need to upload your files as a .zip data file If you want SSH access there is code at bottom to allow this for Colab (not possible on Kaggle).<p>\n",
        "If you run into problems or have suggestions I'd love to hear from you."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "93AfEfEABMXB",
        "colab_type": "text"
      },
      "source": [
        "## Explanation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eKO_K9DlBuyk",
        "colab_type": "text"
      },
      "source": [
        "### Data & Directories\n",
        "There are a couple of differences between Kaggle and Colab.<p>\n",
        "**Colab** - Is a live linux system with nothing being persistant. You can attach a google drive to your kernel and/or download files from gs://, GitHub, Kaggle, etc. Using drive can have a performance penalty but is the easiest way to access persistent files.<br>\n",
        "I am symlinking Library files and output files.<br>\n",
        "Because my google drive space is limited I choose to download large data files each time the kernel is used.<p>\n",
        "**Kaggle** - The kernel has a persistent ./input directory for data that is read only. You can attach data and library files there. (zip your files into a single file and upload the .zip).<br>\n",
        "Your personal ./lib directory can ba zipped and uploaded there as ./input/lib (but I am uncertain if the competition scoring allows use of private data sets.)<br>\n",
        "You can also create notebooks of kernel type \"script\" and then file->Add Utility Script in your competition notebook to attach the script files under ./usr/lib but you have to do this one file at a time. (see: https://www.kaggle.com/product-feedback/91185 for more information)<br>\n",
        "I don't think competition scoring allows internet access so all files have to be attached to your notebook at submit time.<p>\n",
        "Because I am using symlinks to lib files on google drive if you ssh into the kernel and vim the library files you are changing the persistent copy of the files."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IOQz9QFOB5EB",
        "colab_type": "text"
      },
      "source": [
        "### Switching between Colab & Kaggle\n",
        "One way of moving your script from Colab to Kaggle to run is:<br>\n",
        "   * delete all cells from your Kaggle competition notebook<br>\n",
        "   * download the .ipynb from Colab<br>\n",
        "   * upload it into the blank Kaggle notebook.<br>\n",
        "   * delete the cells near the bottom of the notebook that need to be deleted when running on Kaggle<br>\n",
        "   * update any script parameters (eg. verbose)\n",
        "   * zip your current library files into a lib.zip and upload to your Kaggle dataset file\n",
        "   * if you have changed ./data files make sure your Kaggle kernel has the current versions of those files"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7dMWt8F8B9k2",
        "colab_type": "text"
      },
      "source": [
        "### Drectory Structure (Notebook)\n",
        "\n",
        "* Kaggle &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; <em>(cwd = /kaggle/working/)</em><br>\n",
        "  {datadir} = /kaggle/input<br>\n",
        "  {libdir} = /kaggle/input/lib &nbsp; &nbsp; &nbsp; (or /kaggle/usr/lib)<br>\n",
        "  {outdir} = /kaggle/working<br>\n",
        "* Colab &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; <em>(cwd = /content/)</em><br>\n",
        "  {datadir} = /content/data<br>\n",
        "  {libdir} = /content/lib<br>\n",
        "  {outdir} = /content/output<br>\n",
        "\n",
        "#### - Required Libraries\n",
        "   * {libdir}/bert_utils.py\n",
        "   * {libdir}/modeling.py\n",
        "   * {libdir}/tokenization.py\n",
        "\n",
        "#### - Inputs (competition data)\n",
        "   * {datadir}/tensorflow2-question-answering/ (from https://www.kaggle.com/c/tensorflow2-question-answering/data)\n",
        "\n",
        "#### - Required Data (additional packages)\n",
        "   * {datadir}/bert-joint-baseline/ (from prvi\n",
        "https://www.kaggle.com/prokaj/bert-joint-baseline; contains model and scripts)\n",
        "\n",
        "#### - Outputs\n",
        "   * {outdir}/predictions.json\n",
        "   * {outdir}/submission.csv<br>\n",
        "   * {outdir}/eval.tf_record<br>\n",
        "   * {outdir}/.ipynb_checkpoints/<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ftZ8SR0WC5Nj",
        "colab_type": "text"
      },
      "source": [
        "### Drectory Structure (Google Drive)\n",
        "The following is the file structure for your google drive:<p>\n",
        "\n",
        "/My Drive/Colab/ &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; <em>(this directory should be private)</em><br>\n",
        "/My Drive/Colab/kaggle.json &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; <em>(your personal Kaggle auth file)</em><br>\n",
        "/My Drive/Colab/{projdir}/ &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; (folder for a specific project/competition)<br>\n",
        "/My Drive/Colab/{projdir}/{notebook}/&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;(individual notebook within project)<br>\n",
        "/My Drive/Colab/{projdir}/{notebook}/notebook.ipynb<br>\n",
        "/My Drive/Colab/{projdir}/{notebook}/lib/<br>\n",
        "/My Drive/Colab/{projdir}/{notebook}/output/<br>\n",
        "/My Drive/{projdir}/ &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; <em>(optional directory to be shared between teams)</em><br><p>\n",
        "Using this directory structure and the variables in the config variables section below you can have any number of projects on your google drive and any number of notebook versions in each project. Each notebook can have a unique set of libraries.<p>\n",
        "There is also a {nbver} which, if not '' is appended to the end of {outdir} and {libdir} offering additional flexibility if you want (if not just set it to ''}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NxmuhjhzpZqL",
        "colab_type": "text"
      },
      "source": [
        "### GitHub\n",
        "As I learn more about GitHub I will add information here. But, I think you can choose to make repositaries out of either /My Drive/Colab/{projdir}/ or /My Drive/Colab/{projdir}/{notebook}/   <p>\n",
        "You can then either pull/push to the shared repo directly from the Colab (using ssh, using %%bash commands in notebook or by linking the google Drive to your local machine and doing it from there.<p>\n",
        "From within the Colab all files are in your google drive from the path /content/gdrive/My\\ Drive\\Colab\\..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tiQT_p1OCN9J",
        "colab_type": "text"
      },
      "source": [
        "### - Notes\n",
        "Put notes about your Notebook here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "jCycAU1aL99h"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### - Credits / Ancestry\n",
        "If your notebook is a fork or combination of other notebooks here you should provide links so other people can look at where you built your current work from.<p>\n",
        "This notebook is a fork of [mmmarchetti's notebook](https://www.kaggle.com/mmmarchetti/tensorflow-2-0-bert-yes-no-answers) which was a fork of [prokaj's - bert joint baseline notebook](https://www.kaggle.com/prokaj/bert-joint-baseline-notebook/notebook).<br>\n",
        "mmmarchetti made some modifications to slightly improve the code and get the YES / NO answers and leave the unknowns blank."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jNoEkLc0CXXu",
        "colab_type": "text"
      },
      "source": [
        "## Notebook Variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pUu4nppvoEEb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "CompSubmission = False                       # Set to True if submitting to Competition"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XNmKNoxqqARP",
        "colab_type": "code",
        "outputId": "3892058c-8617-4f33-8a3b-d259060683d3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "## Turn this on for development to make sure library updates get reimported\n",
        "#  will reduce performance so don't use for production.\n",
        "%load_ext autoreload\n",
        "%autoreload 2"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The autoreload extension is already loaded. To reload it, use:\n",
            "  %reload_ext autoreload\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HaUs7DxSKDVG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Helper Functions   (probably will move to a library eventually)\n",
        "import os\n",
        "\n",
        "# Custom Error Handler\n",
        "class ExecutionStop(Exception):\n",
        "    def __init__(self, value): self.value=value\n",
        "    def __str__(self): return(str(self.value))\n",
        "\n",
        "#  Show list of file sand directories                 (it seems that this is skipping the symlinks)\n",
        "def list_files(startpath, exclude=[\"/.config\", \"/gdrive\"]):\n",
        "\n",
        "    for root, _, files in os.walk(startpath):\n",
        "        if any([e in root for e in exclude]):\n",
        "            continue\n",
        "        level = root.replace(startpath, '').count(os.sep)\n",
        "        indent = ' ' * 4 * (level)\n",
        "        print(f\"{indent}{os.path.basename(root)}/\")\n",
        "        subindent = ' ' * 4 * (level + 1)\n",
        "        for f in files:\n",
        "            print(f\"{subindent}{f}\")\n",
        "\n",
        "def print_flags():\n",
        "    if verbose:\n",
        "        print(\"\\nParameters:\")\n",
        "        FLAGS = tf.flags.FLAGS\n",
        "        for attr, obj in sorted(FLAGS.__flags.items()):\n",
        "            print(f\"{attr.upper()}={obj.value}\")\n",
        "        print(\"\")\n",
        "\n",
        "def del_all_flags(FLAGS):\n",
        "    flags_dict = FLAGS._flags()\n",
        "    keys_list = [keys for keys in flags_dict]\n",
        "    for keys in keys_list:\n",
        "        FLAGS.__delattr__(keys)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F30YID4j_khp",
        "colab_type": "code",
        "outputId": "06cc35b2-d5a8-4856-c7e0-f9c41c6660ce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "## Set file locations    (these variables are not implemented in the FLAGS code yet)\n",
        "import os, sys\n",
        "from pathlib import Path\n",
        "\n",
        "## Config Variables\n",
        "verbose = True if not CompSubmission else False  # Turn this off to supress some of the \"fyi\" output\n",
        "competition = 'tensorflow2-question-answering'\n",
        "train_file = ''                       # Set this below when you download the training file\n",
        "test_file = ''                        # Set this below when you download the test file\n",
        "projdir = 'bertqa'                    # The project directory on Drive for this competition\n",
        "notebook = 'BERTjoint_yes_no'         # Subdir on Drive for files specific to this notebook\n",
        "nbver = ''                            # library/output subfolder for this notebook version, or ''\n",
        "DownloadBigFiles = True               # Files will not download if already on drive\n",
        "\n",
        "if Path('/content').exists():\n",
        "    print(\"Detected running on Colab\")\n",
        "    kernel = 'Colab'\n",
        "    basedir = \"/content\"\n",
        "    libdir = f\"{basedir}/lib\"\n",
        "    datadir = f\"{basedir}/data\"\n",
        "    outdir = f\"{basedir}/output\"      # will be symlinked to a user's private gdrive for persistence\n",
        "elif Path('/kaggle').exists():\n",
        "    print(\"Detected running on Kaggle\")\n",
        "    kernel = 'Kaggle'\n",
        "    basedir = \"/kaggle\"\n",
        "    libdir = f\"{basedir}/input/lib\"   # this has to be in a writable location\n",
        "    datadir = f\"{basedir}/input\"      # this may need to be '../input' for scoring\n",
        "    outdir = f\"{basedir}/working\"     # this may need to be '.' for scoring\n",
        "else:\n",
        "    raise ExecutionStop(\"Cannot continue without determining file locations\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Detected running on Colab\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MNzAuLZDzG_R",
        "colab_type": "text"
      },
      "source": [
        "# ============= Machine Spinup ============="
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "808f1526-4290-48f6-eb25-2f18a9d36dc9",
        "id": "fYw0W98nXsD0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "! zdump PST\n",
        "if verbose:\n",
        "    list_files(basedir)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "PST  Sat Jan  4 00:07:11 2020 PST\n",
            "content/\n",
            "    sample_data/\n",
            "        README.md\n",
            "        anscombe.json\n",
            "        california_housing_test.csv\n",
            "        mnist_train_small.csv\n",
            "        california_housing_train.csv\n",
            "        mnist_test.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aheaRzVE6fs1",
        "colab_type": "text"
      },
      "source": [
        "## -- Setup --"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gixU6_gv678n",
        "colab_type": "text"
      },
      "source": [
        "### Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "auXx45x70Qcs",
        "colab_type": "code",
        "outputId": "6100c8c9-5255-4a29-9efa-efc15647b477",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        }
      },
      "source": [
        "## File link to Google Drive\n",
        "\n",
        "######\n",
        "##  WARNING: This is convienent but a rather bad idea from a security point of view. Mounting your\n",
        "##           Google Drive in this way makes your entire drive accessible read/write and then you\n",
        "##           are likely to run code from libraries that could be untrustable.\n",
        "##           Possible alternatives would be to use read only file sharing links or if you want\n",
        "##           read/write access (to allow file output for example) I would recommend creating a\n",
        "##           special Google Drive account that only has files related to your Colab work.\n",
        "######\n",
        "\n",
        "if kernel == 'Colab':\n",
        "    from google.colab import drive\n",
        "    drive.mount(f\"{basedir}/gdrive\", force_remount=False)   # true to reread drive\n",
        "\n",
        "    # Create a shorter shared directory name and avoid having to deal with the space in \"My Drive\"\n",
        "    if Path(f\"{basedir}/{projdir}\").is_symlink():\n",
        "        ! rm \"{basedir}/{projdir}\"\n",
        "    ! ln -s \"{basedir}/gdrive/My Drive/{projdir}/\" \"{basedir}/{projdir}\"\n",
        "    if not Path(f\"{basedir}/{projdir}\").exists():\n",
        "        raise ExecutionStop(\"Symlink to shared project directory not found!\")\n",
        "\n",
        "    ## If you do not want to use the lib directoy from your Google Drive set this block False\n",
        "    if True:\n",
        "        if Path(libdir).is_symlink():\n",
        "            ! rm \"{libdir}\"\n",
        "        ! ln -s \"{basedir}/gdrive/My Drive/Colab/{projdir}/{notebook}/lib{nbver}/\" \"{libdir}\"\n",
        "        if not Path(libdir).exists():\n",
        "            raise ExecutionStop(\"Project libdir directory not found!\")\n",
        "\n",
        "    ## If you do not want output to be written to your Google Drive set this block False\n",
        "    if True:\n",
        "        if Path(outdir).is_symlink():\n",
        "            ! rm \"{outdir}\"\n",
        "        ! ln -s \"{basedir}/gdrive/My Drive/Colab/{projdir}/{notebook}/output{nbver}/\" \"{outdir}\"\n",
        "        if not Path(outdir).exists():\n",
        "            raise ExecutionStop(\"Project outdir directory not found!\")\n",
        "\n",
        "    if verbose:\n",
        "        print('\\n', basedir)\n",
        "        ! ls -l \"{basedir}\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·\n",
            "Mounted at /content/gdrive\n",
            "\n",
            " /content\n",
            "total 12\n",
            "lrwxrwxrwx 1 root root   32 Jan  4 00:07 bertqa -> '/content/gdrive/My Drive/bertqa/'\n",
            "drwx------ 4 root root 4096 Jan  4 00:07 gdrive\n",
            "lrwxrwxrwx 1 root root   59 Jan  4 00:07 lib -> '/content/gdrive/My Drive/Colab/bertqa/BERTjoint_yes_no/lib/'\n",
            "lrwxrwxrwx 1 root root   62 Jan  4 00:07 output -> '/content/gdrive/My Drive/Colab/bertqa/BERTjoint_yes_no/output/'\n",
            "drwxr-xr-x 1 root root 4096 Dec 18 16:52 sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q8BLJAW95ih-",
        "colab_type": "text"
      },
      "source": [
        "### Kaggle API\n",
        "<Details>You will need Kaggle API token to link the Colab instance to your Kaggle account to get data, etc.<br>\n",
        "Go to: https://www.kaggle.com/yourID/account and click on the \"Create New API Token: button to get a file named kaggle.json.<p>You can put your kaggle.json file in your google drive at My Drive/colab/kaggle.json.<br>\n",
        "Alternately, you can store it on your local machine and the script will ask you to upload it.</Details>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lc3IYPUg17c8",
        "colab_type": "code",
        "outputId": "b5353bf9-84e5-4644-f9ea-c12db72fc609",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "## Link to Kaggle\n",
        "if kernel == 'Colab':\n",
        "    from google.colab import files\n",
        "    if Path(f\"{basedir}/gdrive/My Drive/Colab/kaggle.json\").exists():\n",
        "        # if there is a kaggle.json file in gdrive use it\n",
        "        os.environ['KAGGLE_CONFIG_DIR'] = f\"{basedir}/gdrive/My Drive/Colab/\"\n",
        "        ! ls -l \"{basedir}/gdrive/My Drive/Colab/kaggle.json\"\n",
        "        import kaggle\n",
        "    else:\n",
        "        # Have user upload file\n",
        "        print('Upload kaggle.json.')\n",
        "        # The files.upload() command is failing sporatically with:\n",
        "        #   TypeError: Cannot read property '_uploadFiles' of undefined (just run this cell again)\n",
        "        ! rm \"{basedir}/kaggle.json\"  2> /dev/null\n",
        "        files.upload()\n",
        "        ! chmod 600 kaggle.json\n",
        "        os.environ['KAGGLE_CONFIG_DIR'] = f\"{basedir}/\"\n",
        "        ! ls -l \"{basedir}/kaggle.json\"\n",
        "        import kaggle\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-rw------- 1 root root 66 Dec 15 08:31 '/content/gdrive/My Drive/Colab/kaggle.json'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M0k60Je4YEQa",
        "colab_type": "text"
      },
      "source": [
        "## -- Main System Config --\n",
        "<Details><Summary>Global Config</Summary>\n",
        "Put any global system configuration here"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "72Ermuc261_x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if kernel == \"Colab\":\n",
        "    if Path(f\"{basedir}/sample_data\").exists():\n",
        "        ! rm -rf \"{basedir}/sample_data\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JNKoTizrh9RG",
        "colab_type": "code",
        "outputId": "ee830acd-86ba-4b93-bdbb-e908bd0b314a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%%bash -s \"{libdir}\" \"{datadir}\" \"{outdir}\"\n",
        "# make directories if not already exist\n",
        "[ -d \"$1\" ] || mkdir -p \"$1\"        # {libdir}\n",
        "[ -d \"$2\" ] || mkdir -p \"$2\"        # {datadir}\n",
        "[ -d \"$3\" ] || mkdir -p \"$3\"        # {outdir}\n",
        "zdump PST"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "PST  Sat Jan  4 00:07:45 2020 PST\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UdjitRnQynwk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys, os\n",
        "if not libdir in sys.path:                       # don't add multiple times\n",
        "    sys.path.append(libdir)\n",
        "if not (libdir in os.environ['PYTHONPATH']):     # needed to run python scripts from shell\n",
        "    os.environ['PYTHONPATH'] += f\":{libdir}\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Pq9DBlECVVvY",
        "outputId": "0c8a8514-3d27-4f60-ccd8-e73599dca3e6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        }
      },
      "source": [
        "if verbose:\n",
        "    !pwd\n",
        "    !ls -l\n",
        "    print('', \"sys.path:\", *sys.path, '', sep='\\n')\n",
        "    !printenv |grep -E 'KAGGLE|PYTHON'\n",
        "    print()\n",
        "! zdump PST"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n",
            "total 12\n",
            "lrwxrwxrwx 1 root root   32 Jan  4 00:07 bertqa -> '/content/gdrive/My Drive/bertqa/'\n",
            "drwxr-xr-x 2 root root 4096 Jan  4 00:07 data\n",
            "drwx------ 4 root root 4096 Jan  4 00:07 gdrive\n",
            "lrwxrwxrwx 1 root root   59 Jan  4 00:07 lib -> '/content/gdrive/My Drive/Colab/bertqa/BERTjoint_yes_no/lib/'\n",
            "lrwxrwxrwx 1 root root   62 Jan  4 00:07 output -> '/content/gdrive/My Drive/Colab/bertqa/BERTjoint_yes_no/output/'\n",
            "\n",
            "sys.path:\n",
            "\n",
            "/env/python\n",
            "/usr/lib/python36.zip\n",
            "/usr/lib/python3.6\n",
            "/usr/lib/python3.6/lib-dynload\n",
            "/usr/local/lib/python3.6/dist-packages\n",
            "/usr/lib/python3/dist-packages\n",
            "/usr/local/lib/python3.6/dist-packages/IPython/extensions\n",
            "/root/.ipython\n",
            "/content/lib\n",
            "\n",
            "KAGGLE_CONFIG_DIR=/content/gdrive/My Drive/Colab/\n",
            "PYTHONWARNINGS=ignore:::pip._internal.cli.base_command\n",
            "PYTHONPATH=/env/python:/content/lib\n",
            "\n",
            "PST  Sat Jan  4 00:07:52 2020 PST\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "javRwDS6zhOC",
        "colab_type": "text"
      },
      "source": [
        "# =========== Project Specific Stuff ==========="
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "JFQgZRN2PaEE"
      },
      "source": [
        "## -- Project Setup --"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gAqCNh4y8DJz",
        "colab_type": "text"
      },
      "source": [
        "### Download Dataset and Support Files"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cGGM0EVmM24H",
        "colab_type": "text"
      },
      "source": [
        "Kaggle Competition Files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5XcpPkyLBzaA",
        "outputId": "bd3ca81c-101e-4295-cbe9-9ec6a5689d20",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 697
        }
      },
      "source": [
        "## Competition Dataset  (5GB zipped)\n",
        "if DownloadBigFiles and kernel == 'Colab':\n",
        "    if not Path(f\"{datadir}/compdata.flag\").exists():      ## Don't download again if exists\n",
        "        if verbose:\n",
        "            ! kaggle competitions list\n",
        "            print()\n",
        "        print(\"Downloading Competition Data\\n\")\n",
        "        ! kaggle competitions download -c \"{competition}\" -p \"{datadir}\"\n",
        "        ! mkdir -p \"{datadir}/{competition}/\"\n",
        "        ! mv \"{datadir}/sample_submission.csv\"  \"{datadir}/{competition}\"\n",
        "        ! unzip \"{datadir}/simplified-nq-train.jsonl.zip\" -d \"{datadir}/{competition}\"\n",
        "        ! rm \"{datadir}/simplified-nq-train.jsonl.zip\"\n",
        "        ! unzip \"{datadir}/simplified-nq-test.jsonl.zip\" -d \"{datadir}/{competition}\"\n",
        "        ! rm \"{datadir}/simplified-nq-test.jsonl.zip\"\n",
        "        ! touch \"{datadir}/compdata.flag\"\n",
        "    else:\n",
        "        print(\"Competition Data already exists. Not downloading.\\n\")\n",
        "        !ls -l \"{datadir}/{competition}\"/*\n",
        "    train_file = f\"{datadir}/{competition}/simplified-nq-train.jsonl\"\n",
        "    test_file = f\"{datadir}/{competition}/simplified-nq-test.jsonl\"\n",
        "else:\n",
        "    print(\"For Kaggle, make sure you download a copy of the competition data into your kernel\")\n",
        "    ! ls -l \"{datadir}/{competition}\"/*\n",
        "    train_file = f\"{datadir}/{competition}/simplified-nq-train.jsonl\"\n",
        "    test_file = f\"{datadir}/{competition}/simplified-nq-test.jsonl\"\n",
        "\n",
        "    # public_dataset = os.path.getsize(f\"{test_file}\") < 20_000_000\n",
        "    # private_dataset = os.path.getsize(f\"{test_file}\") >= 20_000_000"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.6 / client 1.5.4)\n",
            "ref                                            deadline             category             reward  teamCount  userHasEntered  \n",
            "---------------------------------------------  -------------------  ---------------  ----------  ---------  --------------  \n",
            "digit-recognizer                               2030-01-01 00:00:00  Getting Started   Knowledge       2240           False  \n",
            "titanic                                        2030-01-01 00:00:00  Getting Started   Knowledge      16405           False  \n",
            "house-prices-advanced-regression-techniques    2030-01-01 00:00:00  Getting Started   Knowledge       5384           False  \n",
            "connectx                                       2030-01-01 00:00:00  Getting Started   Knowledge         14           False  \n",
            "imagenet-object-localization-challenge         2029-12-31 07:00:00  Research          Knowledge         61           False  \n",
            "competitive-data-science-predict-future-sales  2020-12-31 23:59:00  Playground            Kudos       5348           False  \n",
            "deepfake-detection-challenge                   2020-03-31 23:59:00  Featured         $1,000,000        851           False  \n",
            "cat-in-the-dat-ii                              2020-03-31 23:59:00  Playground             Swag         79           False  \n",
            "nlp-getting-started                            2020-03-23 23:59:00  Getting Started     $10,000        969           False  \n",
            "bengaliai-cv19                                 2020-03-16 23:59:00  Research            $10,000        390           False  \n",
            "google-quest-challenge                         2020-02-10 23:59:00  Featured            $25,000        808           False  \n",
            "tensorflow2-question-answering                 2020-01-22 23:59:00  Featured            $50,000       1066            True  \n",
            "data-science-bowl-2019                         2020-01-22 23:59:00  Featured           $160,000       2846           False  \n",
            "pku-autonomous-driving                         2020-01-21 23:59:00  Featured            $25,000        748           False  \n",
            "santa-2019-revenge-of-the-accountants          2020-01-16 23:59:00  Playground             Swag         82           False  \n",
            "santa-workshop-tour-2019                       2020-01-15 23:59:00  Featured            $25,000       1466           False  \n",
            "nfl-big-data-bowl-2020                         2020-01-06 23:59:00  Featured            $75,000       2038           False  \n",
            "nfl-playing-surface-analytics                  2020-01-02 23:59:00  Analytics           $75,000          0           False  \n",
            "ashrae-energy-prediction                       2019-12-19 23:59:00  Featured            $25,000       1877           False  \n",
            "Kannada-MNIST                                  2019-12-17 23:59:00  Playground        Knowledge       1214           False  \n",
            "\n",
            "Downloading Competition Data\n",
            "\n",
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.6 / client 1.5.4)\n",
            "Downloading simplified-nq-train.jsonl.zip to /content/data\n",
            "100% 4.46G/4.46G [01:30<00:00, 13.5MB/s]\n",
            "100% 4.46G/4.46G [01:30<00:00, 52.8MB/s]\n",
            "Downloading simplified-nq-test.jsonl.zip to /content/data\n",
            "100% 4.78M/4.78M [00:00<00:00, 18.2MB/s]\n",
            "\n",
            "Downloading sample_submission.csv to /content/data\n",
            "  0% 0.00/18.2k [00:00<?, ?B/s]\n",
            "100% 18.2k/18.2k [00:00<00:00, 43.2MB/s]\n",
            "Archive:  /content/data/simplified-nq-train.jsonl.zip\n",
            "  inflating: /content/data/tensorflow2-question-answering/simplified-nq-train.jsonl  \n",
            "Archive:  /content/data/simplified-nq-test.jsonl.zip\n",
            "  inflating: /content/data/tensorflow2-question-answering/simplified-nq-test.jsonl  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oZhNV3PJHOEL",
        "colab_type": "text"
      },
      "source": [
        "BERTjoint files from: https://www.kaggle.com/prokaj/bert-joint-baseline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s207mPqAHDVw",
        "colab_type": "code",
        "outputId": "6dc8b6a6-f875-4405-a773-1e55a2058232",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        }
      },
      "source": [
        "# Get BERTjoint model files (this a copy of the prokaj file from my Google Drive)\n",
        "if DownloadBigFiles and kernel == 'Colab':\n",
        "    if not Path(f\"{datadir}/bertfiles.flag\").exists():      ## Don't download again if exists\n",
        "        print(\"Downloading BERT-joint Model\\n\")\n",
        "        ! mkdir -p \"{datadir}/bert-joint-baseline/\"\n",
        "        filestoget = \"bert_config* model_cpkt* nq-test* vocab*\"\n",
        "        ! kaggle datasets download -d prokaj/bert-joint-baseline -p \"{datadir}\"\n",
        "        ! unzip \"{datadir}/bert-joint-baseline.zip\" {filestoget} -d \"{datadir}/bert-joint-baseline/\"\n",
        "        ! rm \"{datadir}/bert-joint-baseline.zip\"\n",
        "        if Path(f\"{datadir}/bert-joint-baseline-output.npz\").exists():\n",
        "            ! rm \"{datadir}/bert-joint-baseline-output.npz\" # if kaggle downloaded this delete it\n",
        "        if verbose:\n",
        "            ! ls -l \"{datadir}/bert-joint-baseline/\"\n",
        "        ! touch \"{datadir}/bertfiles.flag\"\n",
        "    else:\n",
        "        print(\"BERT-joint Files already exists. Not downloading.\\n\")\n",
        "        ! ls -l \"{datadir}/bert-joint-baseline/\"\n",
        "else:\n",
        "    print(\"For Kaggle, make sure you download a copy of prokaj's bert-joint-baseline to your kernel\")\n",
        "    ! ls -l \"{datadir}/bert-joint-baseline/\"\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading BERT-joint Model\n",
            "\n",
            "Downloading bert-joint-baseline.zip to /content/data\n",
            " 99% 1.52G/1.53G [00:42<00:01, 10.3MB/s]\n",
            "100% 1.53G/1.53G [00:42<00:00, 38.4MB/s]\n",
            "Archive:  /content/data/bert-joint-baseline.zip\n",
            "  inflating: /content/data/bert-joint-baseline/bert_config.json  \n",
            "  inflating: /content/data/bert-joint-baseline/model_cpkt-1.data-00000-of-00002  \n",
            "  inflating: /content/data/bert-joint-baseline/model_cpkt-1.data-00001-of-00002  \n",
            "  inflating: /content/data/bert-joint-baseline/model_cpkt-1.index  \n",
            "  inflating: /content/data/bert-joint-baseline/nq-test.tfrecords  \n",
            "  inflating: /content/data/bert-joint-baseline/vocab-nq.txt  \n",
            "total 1339896\n",
            "-rw-r--r-- 1 root root        314 Nov 17 15:02 bert_config.json\n",
            "-rw-r--r-- 1 root root      79677 Nov 17 15:02 model_cpkt-1.data-00000-of-00002\n",
            "-rw-r--r-- 1 root root 1340596252 Nov 17 15:02 model_cpkt-1.data-00001-of-00002\n",
            "-rw-r--r-- 1 root root      29452 Nov 17 15:05 model_cpkt-1.index\n",
            "-rw-r--r-- 1 root root   31094348 Nov 17 15:05 nq-test.tfrecords\n",
            "-rw-r--r-- 1 root root     231601 Nov 17 15:06 vocab-nq.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XJoB8YB3MrvX",
        "colab_type": "text"
      },
      "source": [
        "BERTjoint files from: \n",
        "https://github.com/google-research/language/tree/master/language/question_answering/bert_joint\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1uvuFGJ0MqHo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get BERTjoint model files    (we are not using this model / files)\n",
        "if False and DownloadBigFiles and kernel == 'Colab':\n",
        "    if not Path(f\"{datadir}/bertfiles.flag\").exists():\n",
        "        print(\"Downloading BERT-joint Model\\n\")\n",
        "        ! gsutil cp -R gs://bert-nq/bert-joint-baseline \"{datadir}\"\"\n",
        "        ! touch \"{datadir}/bertfiles.flag\"\n",
        "    else:\n",
        "        print(\"BERT-joint Files already exists. Not downloading.\\n\")\n",
        "        ! ls -l \"{datadir}/bert-joint-baseline/\"\n",
        "else:\n",
        "    pass        # if Kaggle, data will already be there"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c4NrOOtpOVN0",
        "colab_type": "text"
      },
      "source": [
        "BERT files from: https://github.com/google-research/bert<br>\n",
        "(Not the model we are using at the moment)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pdp6tqXmmLw_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## get BERT (this is unlikely to be the BERT-joint files needed for competition)\n",
        "# this version of BERT seems won't import as is. On line 88 of lib/bert/optimization.py\n",
        "#    change   tr.train.Optimizer to tf.keras.optimizers.Optimizer\n",
        "if False and DownloadBigFiles and kernel == 'Colab':\n",
        "    ! git clone https://github.com/google-research/bert.git\n",
        "    ! mv bert lib\n",
        "\n",
        "    # get some pretrained models  (I really  have no idea what these are or if useful)\n",
        "    ! wget https://storage.googleapis.com/bert_models/2018_10_18/cased_L-12_H-768_A-12.zip\n",
        "    ! unzip cased_L-12_H-768_A-12.zip\n",
        "    ! rm cased_L-12_H-768_A-12.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oU6FPUHjvH1r",
        "colab_type": "text"
      },
      "source": [
        "<Details><Summary>BERT tf.compat.v1 Notes</Summary>\n",
        "baseline_w_bert_translated_to_tf2_0 and other model libraries being passed around often seem to have tf.compat.v1 in them. Be aware that use of tf.compat.v1, which is not permitted to be eligible for TF2.0 prizes in this competition.<br>\n",
        "https://www.kaggle.com/dimitreoliveira/using-tf-2-0-w-bert-on-nq-translated-to-tf2-0</Details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "bNPxSmhMJC5c"
      },
      "source": [
        "### Library Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yr69yF8fepTA",
        "colab_type": "code",
        "cellView": "both",
        "outputId": "882ae31d-d4a4-44a1-edd3-4ee8cdcb42a4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "## Copy lib files from Google Drive   (includes bert-tensorflow & natural-questions)\n",
        "if kernel == 'Colab':\n",
        "    ## It is likely that the {libdir} was symlinked to a gDrive directory above\n",
        "    #  otherwise install code to copy any needed lib files into {libdir}\n",
        "    pass\n",
        "if kernel == 'Kaggle':\n",
        "    ! cp \"{datadir}/bert-joint-baseline\"/*.py \"{libdir}\"\n",
        "if verbose:\n",
        "    print(f\"{libdir}/\")\n",
        "    ! ls -l \"{libdir}\"/*"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/lib/\n",
            "-rw------- 1 root root 44021 Dec 24 20:05 /content/lib/bert_utils.py\n",
            "-rw------- 1 root root 41596 Dec 24 20:04 /content/lib/modeling.py\n",
            "-rw------- 1 root root 12411 Dec 24 20:04 /content/lib/tokenization.py\n",
            "\n",
            "/content/lib/__pycache__:\n",
            "total 66\n",
            "-rw------- 1 root root 25440 Dec 25 21:52 bert_utils.cpython-36.pyc\n",
            "-rw------- 1 root root 31247 Dec 25 21:52 modeling.cpython-36.pyc\n",
            "-rw------- 1 root root  9861 Dec 25 21:52 tokenization.cpython-36.pyc\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qhpBRp11R0jv",
        "colab_type": "code",
        "outputId": "943b32a6-6b00-4120-be7e-404572c3048d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "## Load Libraries\n",
        "import os, sys, importlib\n",
        "\n",
        "if kernel == \"Colab\":               # Kaggle is V2 by default\n",
        "    #magic to make Colab path to Tensorflow V2 on Colab\n",
        "    %tensorflow_version 2.x \n",
        "\n",
        "import tensorflow as tf\n",
        "print(\"TensofFlow\", tf.__version__)\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import collections\n",
        "\n",
        "import bert_utils\n",
        "import modeling\n",
        "import tokenization\n",
        "\n",
        "import json"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n",
            "TensofFlow 2.1.0-rc1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XsBNWHa3p6CL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## uncomment and use this cell to reimport libs you have updated\n",
        "# importlibe.reload(bert_utils)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yJC4o-hkuIfO",
        "colab_type": "code",
        "outputId": "4540b299-fbda-410c-d6ca-8540eac5cb8e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "! zdump PST\n",
        "if verbose:\n",
        "    list_files(basedir)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "PST  Sat Jan  4 00:18:18 2020 PST\n",
            "content/\n",
            "    data/\n",
            "        compdata.flag\n",
            "        bertfiles.flag\n",
            "        bert-joint-baseline/\n",
            "            vocab-nq.txt\n",
            "            nq-test.tfrecords\n",
            "            model_cpkt-1.data-00000-of-00002\n",
            "            bert_config.json\n",
            "            model_cpkt-1.index\n",
            "            model_cpkt-1.data-00001-of-00002\n",
            "        tensorflow2-question-answering/\n",
            "            simplified-nq-train.jsonl\n",
            "            simplified-nq-test.jsonl\n",
            "            sample_submission.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s7niWvRW-R5C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# raise ExecutionStop(\"Execution stopped\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NsnKczo07ygW",
        "colab_type": "text"
      },
      "source": [
        "## -- Code Implementation in Tensorflow 2.0 --"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v7JSYsp-72La",
        "colab_type": "text"
      },
      "source": [
        "**A few notes:**\n",
        "- If you want to keep using **flags** and **logging** you will have to use the **absl** lib (this is recommended by the TF team).\n",
        "- Since we won't use it with the kernels, he removed most of the **TPU** related stuff to reduce complexity.\n",
        "- Tensorflow 2 don't let us use global variables **(tf.compat.v1.trainable_variables())**.\n",
        "\n",
        "In this notebook, we'll be using the Bert baseline for Tensorflow to create predictions for the Natural Questions test set. Note that this uses a model that has already been pre-trained - we're only doing inference here. A GPU is required, and this should take between 1-2 hours to run.\n",
        "\n",
        "The original script can be found [here](https://github.com/google-research/language/blob/master/language/question_answering/bert_joint/run_nq.py).<br>\n",
        "The supporting modules were drawn from the [official Tensorflow model repository](https://github.com/tensorflow/models/tree/master/official).<br>\n",
        "The bert-joint-baseline data is described [here](https://github.com/google-research/language/tree/master/language/question_answering/bert_joint).\n",
        "\n",
        "**Note:** This baseline uses code that was migrated from TF1.x. Be aware that it contains use of tf.compat.v1, which is not permitted to be eligible for [TF2.0 prizes in this competition](https://www.kaggle.com/c/tensorflow2-question-answering/overview/prizes)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rhcWhldD17Q4",
        "colab_type": "code",
        "outputId": "4b6fcba9-ab63-457b-b45f-8dbb28de01fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "! zdump PST"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "PST  Sat Jan  4 00:18:32 2020 PST\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "beaF_fYhtUxn",
        "colab_type": "text"
      },
      "source": [
        "### Support Classes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X4hOx_86tTFI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TDense(tf.keras.layers.Layer):\n",
        "    def __init__(self,\n",
        "                 output_size,\n",
        "                 kernel_initializer=None,\n",
        "                 bias_initializer=\"zeros\",\n",
        "                **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.output_size = output_size\n",
        "        self.kernel_initializer = kernel_initializer\n",
        "        self.bias_initializer = bias_initializer\n",
        "\n",
        "    def build(self,input_shape):\n",
        "        dtype = tf.as_dtype(self.dtype or tf.keras.backend.floatx())\n",
        "        if not (dtype.is_floating or dtype.is_complex):\n",
        "          raise TypeError(\"Unable to build `TDense` layer with \"\n",
        "                          \"non-floating point (and non-complex) \"\n",
        "                          \"dtype %s\" % (dtype,))\n",
        "        input_shape = tf.TensorShape(input_shape)\n",
        "        if tf.compat.dimension_value(input_shape[-1]) is None:\n",
        "          raise ValueError(\"The last dimension of the inputs to \"\n",
        "                           \"`TDense` should be defined. \"\n",
        "                           \"Found `None`.\")\n",
        "        last_dim = tf.compat.dimension_value(input_shape[-1])\n",
        "#       self.input_spec = tf.keras.layers.InputSpec(min_ndim=3, axes={-1: last_dim})    # original value\n",
        "        self.input_spec = tf.keras.layers.InputSpec(min_ndim=2, axes={-1: last_dim})    # ChrisM changed\n",
        "        self.kernel = self.add_weight(\n",
        "            \"kernel\",\n",
        "            shape=[self.output_size,last_dim],\n",
        "            initializer=self.kernel_initializer,\n",
        "            dtype=self.dtype,\n",
        "            trainable=True)\n",
        "        self.bias = self.add_weight(\n",
        "            \"bias\",\n",
        "            shape=[self.output_size],\n",
        "            initializer=self.bias_initializer,\n",
        "            dtype=self.dtype,\n",
        "            trainable=True)\n",
        "        super(TDense, self).build(input_shape)\n",
        "\n",
        "    def call(self,x):\n",
        "        return tf.matmul(x,self.kernel,transpose_b=True)+self.bias\n",
        "\n",
        "\n",
        "def mk_model(config):\n",
        "    seq_len = config['max_position_embeddings']\n",
        "    unique_id  = tf.keras.Input(shape=(1,),dtype=tf.int64,name='unique_id')\n",
        "    input_ids   = tf.keras.Input(shape=(seq_len,),dtype=tf.int32,name='input_ids')\n",
        "    input_mask  = tf.keras.Input(shape=(seq_len,),dtype=tf.int32,name='input_mask')\n",
        "    segment_ids = tf.keras.Input(shape=(seq_len,),dtype=tf.int32,name='segment_ids')\n",
        "    BERT = modeling.BertModel(config=config,name='bert')\n",
        "    pooled_output, sequence_output = BERT(input_word_ids=input_ids,\n",
        "                                          input_mask=input_mask,\n",
        "                                          input_type_ids=segment_ids)\n",
        "    \n",
        "    logits = TDense(2,name='logits')(sequence_output)\n",
        "    start_logits,end_logits = tf.split(logits,axis=-1,num_or_size_splits= 2,name='split')\n",
        "    start_logits = tf.squeeze(start_logits,axis=-1,name='start_squeeze')\n",
        "    end_logits   = tf.squeeze(end_logits,  axis=-1,name='end_squeeze')\n",
        "    \n",
        "    ans_type      = TDense(5,name='ans_type')(pooled_output)\n",
        "    return tf.keras.Model([input_ for input_ in [unique_id,input_ids,input_mask,segment_ids] \n",
        "                           if input_ is not None],\n",
        "                          [unique_id,start_logits,end_logits,ans_type],\n",
        "                          name='bert-baseline')    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qEnip0KLtx--",
        "colab_type": "text"
      },
      "source": [
        "### Create Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HNUN2aCD0sG5",
        "colab_type": "code",
        "outputId": "e5d18d8c-8816-49f7-9ce4-ae6a6e3073a7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "print(\"\\nGPU Memory\\n\")\n",
        "!nvidia-smi --query-gpu=utilization.memory,memory.total,memory.free,memory.used --format=csv"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "GPU Memory\n",
            "\n",
            "utilization.memory [%], memory.total [MiB], memory.free [MiB], memory.used [MiB]\n",
            "0 %, 16280 MiB, 16280 MiB, 0 MiB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zTkY-he7qRbj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## grab bert config\n",
        "with open(f\"{datadir}/bert-joint-baseline/bert_config.json\", 'r') as f:\n",
        "    bert_config = json.load(f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xzVE2wQGt2Au",
        "colab_type": "code",
        "outputId": "82ea36b5-b055-4777-86d6-545caf8f68e4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 833
        }
      },
      "source": [
        "model= mk_model(bert_config)\n",
        "print(json.dumps(bert_config, indent=4))\n",
        "\n",
        "model.summary()\n",
        "print(\"\\nGPU Memory\\n\")\n",
        "!nvidia-smi --query-gpu=utilization.memory,memory.total,memory.free,memory.used --format=csv"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{\n",
            "    \"attention_probs_dropout_prob\": 0.1,\n",
            "    \"hidden_act\": \"gelu\",\n",
            "    \"hidden_dropout_prob\": 0.1,\n",
            "    \"hidden_size\": 1024,\n",
            "    \"initializer_range\": 0.02,\n",
            "    \"intermediate_size\": 4096,\n",
            "    \"max_position_embeddings\": 512,\n",
            "    \"num_attention_heads\": 16,\n",
            "    \"num_hidden_layers\": 24,\n",
            "    \"type_vocab_size\": 2,\n",
            "    \"vocab_size\": 30522\n",
            "}\n",
            "Model: \"bert-baseline\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_ids (InputLayer)          [(None, 512)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_mask (InputLayer)         [(None, 512)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "segment_ids (InputLayer)        [(None, 512)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "bert (BertModel)                ((None, 1024), (None 335141888   input_ids[0][0]                  \n",
            "                                                                 input_mask[0][0]                 \n",
            "                                                                 segment_ids[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "logits (TDense)                 (None, 512, 2)       2050        bert[0][1]                       \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_split (TensorFlowOp [(None, 512, 1), (No 0           logits[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "unique_id (InputLayer)          [(None, 1)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_start_squeeze (Tens [(None, 512)]        0           tf_op_layer_split[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_end_squeeze (Tensor [(None, 512)]        0           tf_op_layer_split[0][1]          \n",
            "__________________________________________________________________________________________________\n",
            "ans_type (TDense)               (None, 5)            5125        bert[0][0]                       \n",
            "==================================================================================================\n",
            "Total params: 335,149,063\n",
            "Trainable params: 335,149,063\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "\n",
            "GPU Memory\n",
            "\n",
            "utilization.memory [%], memory.total [MiB], memory.free [MiB], memory.used [MiB]\n",
            "0 %, 16280 MiB, 13871 MiB, 2409 MiB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JJOVtX_NuEA3",
        "colab_type": "text"
      },
      "source": [
        "### Checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6eI_LdIAuGFV",
        "colab_type": "code",
        "outputId": "fc2dadcf-7b73-4f9f-afe3-0c7dde3209ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "cpkt = tf.train.Checkpoint(model=model)\n",
        "cpkt.restore(f\"{datadir}/bert-joint-baseline/model_cpkt-1\").assert_consumed()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f21b677d160>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IDnBbvUCuHI-",
        "colab_type": "text"
      },
      "source": [
        "### Setting the Flags"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0D_JoNumuOPH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#                                   ### This cell was in my copy but not in former\n",
        "class DummyObject:\n",
        "    def __init__(self,**kwargs):\n",
        "        self.__dict__.update(kwargs)\n",
        "\n",
        "FLAGS=DummyObject(skip_nested_contexts=True,\n",
        "                 max_position=50,\n",
        "                 max_contexts=48,\n",
        "                 max_query_length=64,\n",
        "                 max_seq_length=512,\n",
        "                 doc_stride=128,\n",
        "                 include_unknowns=-1.0,\n",
        "                 n_best_size=20,\n",
        "                 max_answer_length=30)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CSm1q0MouPwq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tqdm\n",
        "eval_records = f\"{datadir}/bert-joint-baseline/nq-test.tfrecords\"\n",
        "if kernel == 'Kaggle' and private_dataset:\n",
        "    eval_records='nq-test.tfrecords'\n",
        "if not Path(eval_records).exists():\n",
        "    # tf2baseline.FLAGS.max_seq_length = 512\n",
        "    eval_writer = bert_utils.FeatureWriter(\n",
        "        filename=os.path.join(eval_records),\n",
        "        is_training=False)\n",
        "\n",
        "    tokenizer = tokenization.FullTokenizer(vocab_file=f\"{datadir}/bert-joint-baseline/vocab-nq.txt\", \n",
        "                                           do_lower_case=True)\n",
        "\n",
        "    features = []\n",
        "    convert = bert_utils.ConvertExamples2Features(tokenizer=tokenizer,\n",
        "                                                   is_training=False,\n",
        "                                                   output_fn=eval_writer.process_feature,\n",
        "                                                   collect_stat=False)\n",
        "\n",
        "    n_examples = 0\n",
        "    tqdm_notebook= tqdm.tqdm_notebook if not kernel == 'Kaggle' else None\n",
        "    for examples in bert_utils.nq_examples_iter(input_file=f\"{test_file}\", \n",
        "                                           is_training=False,\n",
        "                                           tqdm=tqdm_notebook):\n",
        "        for example in examples:\n",
        "            n_examples += convert(example)\n",
        "\n",
        "    eval_writer.close()\n",
        "    print('number of test examples: %d, written to file: %d' % (n_examples,eval_writer.num_features))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dTJRYQjQuTsB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "seq_length = FLAGS.max_seq_length       # bertconfig['max_position_embeddings']\n",
        "name_to_features = {\n",
        "      \"unique_id\": tf.io.FixedLenFeature([], tf.int64),\n",
        "      \"input_ids\": tf.io.FixedLenFeature([seq_length], tf.int64),\n",
        "      \"input_mask\": tf.io.FixedLenFeature([seq_length], tf.int64),\n",
        "      \"segment_ids\": tf.io.FixedLenFeature([seq_length], tf.int64),\n",
        "  }\n",
        "\n",
        "def _decode_record(record, name_to_features=name_to_features):\n",
        "    \"\"\"Decodes a record to a TensorFlow example.\"\"\"\n",
        "    example = tf.io.parse_single_example(serialized=record, features=name_to_features)\n",
        "\n",
        "    # tf.Example only supports tf.int64, but the TPU only supports tf.int32.\n",
        "    # So cast all int64 to int32.\n",
        "    for name in list(example.keys()):\n",
        "        t = example[name]\n",
        "        if name != 'unique_id': #t.dtype == tf.int64:\n",
        "            t = tf.cast(t, dtype=tf.int32)\n",
        "        example[name] = t\n",
        "\n",
        "    return example\n",
        "\n",
        "def _decode_tokens(record):\n",
        "    return tf.io.parse_single_example(serialized=record, \n",
        "                                      features={\n",
        "                                          \"unique_id\": tf.io.FixedLenFeature([], tf.int64),\n",
        "                                          \"token_map\" :  tf.io.FixedLenFeature([seq_length], tf.int64)\n",
        "                                      })\n",
        "      \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VKzXED-KuXjv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "raw_ds = tf.data.TFRecordDataset(eval_records)\n",
        "token_map_ds = raw_ds.map(_decode_tokens)\n",
        "decoded_ds = raw_ds.map(_decode_record)\n",
        "ds = decoded_ds.batch(batch_size=16,drop_remainder=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "33Npl_YL-1or",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#                                       ### This cell was not in my copy but former\n",
        "# next(iter(decoded_ds))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NT2UMXOZuY4p",
        "colab_type": "code",
        "outputId": "a673dbb7-88bf-44e3-e5ba-f41db86d938f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "result=model.predict_generator(ds, verbose = 1 if verbose else 0)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-38-993a311d8668>:1: Model.predict_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use Model.predict, which supports generators.\n",
            "    567/Unknown - 632s 1s/step"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VkIgBtoIua2p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.savez_compressed('bert-joint-baseline-output.npz',\n",
        "                    **dict(zip(['uniqe_id','start_logits','end_logits','answer_type_logits'],\n",
        "                               result)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S9WQFNVPuctX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Span = collections.namedtuple(\"Span\", [\"start_token_idx\", \"end_token_idx\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5vL3t4j5ufId",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ScoreSummary(object):\n",
        "  def __init__(self):\n",
        "    self.predicted_label = None\n",
        "    self.short_span_score = None\n",
        "    self.cls_token_score = None\n",
        "    self.answer_type_logits = None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5kR_ixQEuhmc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class EvalExample(object):\n",
        "  \"\"\"Eval data available for a single example.\"\"\"\n",
        "  def __init__(self, example_id, candidates):\n",
        "    self.example_id = example_id\n",
        "    self.candidates = candidates\n",
        "    self.results = {}\n",
        "    self.features = {}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3nLoKGECulk3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_best_indexes(logits, n_best_size):\n",
        "  \"\"\"Get the n-best logits from a list.\"\"\"\n",
        "  index_and_score = sorted(\n",
        "      enumerate(logits[1:], 1), key=lambda x: x[1], reverse=True)\n",
        "  best_indexes = []\n",
        "  for i in range(len(index_and_score)):\n",
        "    if i >= n_best_size:\n",
        "      break\n",
        "    best_indexes.append(index_and_score[i][0])\n",
        "  return best_indexes\n",
        "\n",
        "def top_k_indices(logits,n_best_size,token_map):\n",
        "    indices = np.argsort(logits[1:])+1\n",
        "    indices = indices[token_map[indices]!=-1]\n",
        "    return indices[-n_best_size:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e11n9VMvupHr",
        "colab_type": "text"
      },
      "source": [
        "### Understanding the code\n",
        "In the item \"answer_type\", in the last lines of this block, it is responsible for storing the identified response type, which, according to [github project repository](https://github.com/google-research/language/blob/master/language/question_answering/bert_joint/run_nq.py) can be:\n",
        "1. UNKNOWN = 0\n",
        "2. YES = 1\n",
        "3. NO = 2\n",
        "4. SHORT = 3\n",
        "5. LONG = 4"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4GNEaA1QusjQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compute_predictions(example):\n",
        "  \"\"\"Converts an example into an NQEval object for evaluation.\"\"\"\n",
        "  predictions = []\n",
        "  n_best_size = FLAGS.n_best_size\n",
        "  max_answer_length = FLAGS.max_answer_length\n",
        "  i = 0\n",
        "  for unique_id, result in example.results.items():\n",
        "    if unique_id not in example.features:\n",
        "      raise ValueError(\"No feature found with unique_id:\", unique_id)\n",
        "    token_map = np.array(example.features[unique_id][\"token_map\"]) #.int64_list.value\n",
        "    start_indexes = top_k_indices(result.start_logits,n_best_size,token_map)\n",
        "    if len(start_indexes)==0:\n",
        "        continue\n",
        "    end_indexes   = top_k_indices(result.end_logits,n_best_size,token_map)\n",
        "    if len(end_indexes)==0:\n",
        "        continue\n",
        "    indexes = np.array(list(np.broadcast(start_indexes[None],end_indexes[:,None])))  \n",
        "    indexes = indexes[(indexes[:,0]<indexes[:,1])*(indexes[:,1]-indexes[:,0]<max_answer_length)]\n",
        "    for start_index,end_index in indexes:\n",
        "        summary = ScoreSummary()\n",
        "        summary.short_span_score = (\n",
        "            result.start_logits[start_index] +\n",
        "            result.end_logits[end_index])\n",
        "        summary.cls_token_score = (\n",
        "            result.start_logits[0] + result.end_logits[0])\n",
        "        summary.answer_type_logits = result.answer_type_logits-result.answer_type_logits.mean()\n",
        "        start_span = token_map[start_index]\n",
        "        end_span = token_map[end_index] + 1\n",
        "\n",
        "        # Span logits minus the cls logits seems to be close to the best.\n",
        "        score = summary.short_span_score - summary.cls_token_score\n",
        "        predictions.append((score, i, summary, start_span, end_span))\n",
        "        i += 1 # to break ties\n",
        "\n",
        "  # Default empty prediction.\n",
        "  score = -10000.0\n",
        "  short_span = Span(-1, -1)\n",
        "  long_span  = Span(-1, -1)\n",
        "  summary    = ScoreSummary()\n",
        "\n",
        "  if predictions:\n",
        "    score, _, summary, start_span, end_span = sorted(predictions, reverse=True)[0]\n",
        "    short_span = Span(start_span, end_span)\n",
        "    for c in example.candidates:\n",
        "      start = short_span.start_token_idx\n",
        "      end = short_span.end_token_idx\n",
        "      ## print(c['top_level'],c['start_token'],start,c['end_token'],end)\n",
        "      if c[\"top_level\"] and c[\"start_token\"] <= start and c[\"end_token\"] >= end:\n",
        "        long_span = Span(c[\"start_token\"], c[\"end_token\"])\n",
        "        break\n",
        "\n",
        "  summary.predicted_label = {\n",
        "      \"example_id\": int(example.example_id),\n",
        "      \"long_answer\": {\n",
        "          \"start_token\": int(long_span.start_token_idx),\n",
        "          \"end_token\": int(long_span.end_token_idx),\n",
        "          \"start_byte\": -1,\n",
        "          \"end_byte\": -1\n",
        "      },\n",
        "      \"long_answer_score\": float(score),\n",
        "      \"short_answers\": [{\n",
        "          \"start_token\": int(short_span.start_token_idx),\n",
        "          \"end_token\": int(short_span.end_token_idx),\n",
        "          \"start_byte\": -1,\n",
        "          \"end_byte\": -1\n",
        "      }],\n",
        "      \"short_answer_score\": float(score),\n",
        "      \"yes_no_answer\": \"NONE\",\n",
        "      \"answer_type_logits\": summary.answer_type_logits.tolist(),\n",
        "      # here:\n",
        "      \"answer_type\": int(np.argmax(summary.answer_type_logits))\n",
        "  }\n",
        "\n",
        "  return summary"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ww0cVDwMuv62",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compute_pred_dict(candidates_dict, dev_features, raw_results,tqdm=None):\n",
        "    \"\"\"Computes official answer key from raw logits.\"\"\"\n",
        "    raw_results_by_id = [(int(res.unique_id),1, res) for res in raw_results]\n",
        "\n",
        "    examples_by_id = [(int(k),0,v) for k, v in candidates_dict.items()]\n",
        "  \n",
        "    features_by_id = [(int(d['unique_id']),2,d) for d in dev_features] \n",
        "  \n",
        "    # Join examples with features and raw results.\n",
        "    examples = []\n",
        "    print('merging examples...')\n",
        "    merged = sorted(examples_by_id + raw_results_by_id + features_by_id)\n",
        "    print('done.')\n",
        "    for idx, type_, datum in merged:\n",
        "        if type_==0: #isinstance(datum, list):\n",
        "            examples.append(EvalExample(idx, datum))\n",
        "        elif type_==2: #\"token_map\" in datum:\n",
        "            examples[-1].features[idx] = datum\n",
        "        else:\n",
        "            examples[-1].results[idx] = datum\n",
        "\n",
        "    # Construct prediction objects.\n",
        "    print('Computing predictions...')\n",
        "   \n",
        "    nq_pred_dict = {}\n",
        "    #summary_dict = {}\n",
        "    if tqdm is not None:\n",
        "        examples = tqdm(examples)\n",
        "    for e in examples:\n",
        "        summary = compute_predictions(e)\n",
        "        #summary_dict[e.example_id] = summary\n",
        "        nq_pred_dict[e.example_id] = summary.predicted_label\n",
        "\n",
        "    return nq_pred_dict\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wE-BkBWhuzMt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def read_candidates_from_one_split(input_path):\n",
        "  \"\"\"Read candidates from a single jsonl file.\"\"\"\n",
        "  candidates_dict = {}\n",
        "  print(\"Reading examples from: %s\" % input_path)\n",
        "  if input_path.endswith(\".gz\"):\n",
        "    with gzip.GzipFile(fileobj=tf.io.gfile.GFile(input_path, \"rb\")) as input_file:\n",
        "      for index, line in enumerate(input_file):\n",
        "        e = json.loads(line)\n",
        "        candidates_dict[e[\"example_id\"]] = e[\"long_answer_candidates\"]\n",
        "        \n",
        "  else:\n",
        "    with tf.io.gfile.GFile(input_path, \"r\") as input_file:\n",
        "      for index, line in enumerate(input_file):\n",
        "        e = json.loads(line)\n",
        "        candidates_dict[e[\"example_id\"]] = e[\"long_answer_candidates\"]\n",
        "        # candidates_dict['question'] = e['question_text']\n",
        "  return candidates_dict"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DmBJNth5u04G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def read_candidates(input_pattern):\n",
        "  \"\"\"Read candidates with real multiple processes.\"\"\"\n",
        "  input_paths = tf.io.gfile.glob(input_pattern)\n",
        "  final_dict = {}\n",
        "  for input_path in input_paths:\n",
        "    final_dict.update(read_candidates_from_one_split(input_path))\n",
        "  return final_dict"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pYqeZlaLu2uR",
        "colab_type": "code",
        "outputId": "835e7c6a-fa6c-4e54-d472-255c69052f32",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202,
          "referenced_widgets": [
            "7599dcbaa912496bbeda880686b8f25a",
            "61e7ed87ebd94acba7696eed19e1cc13",
            "0d4b78ce0101416a9e707dc8b5eab0f8",
            "1fb94434d0624959a48177015e3bc053",
            "15e56d4afc81435784a1e9d579fd45c1",
            "7a4189c5a1fd4e69807c3567205224ac",
            "bb45125c044448e9bfd30144af68a3f1",
            "5a684f810ebf4149a4c1fa6b9ec9f18e"
          ]
        }
      },
      "source": [
        "all_results = [bert_utils.RawResult(*x) for x in zip(*result)]\n",
        "    \n",
        "print (\"Going to candidates file\")\n",
        "\n",
        "candidates_dict = read_candidates(f\"{test_file}\")\n",
        "\n",
        "print (\"setting up eval features\")\n",
        "\n",
        "eval_features = list(token_map_ds)\n",
        "\n",
        "print (\"compute_pred_dict\")\n",
        "\n",
        "tqdm_notebook= tqdm.tqdm_notebook\n",
        "nq_pred_dict = compute_pred_dict(candidates_dict, \n",
        "                                       eval_features,\n",
        "                                       all_results,\n",
        "                                      tqdm=tqdm_notebook)\n",
        "\n",
        "predictions_json = {\"predictions\": list(nq_pred_dict.values())}\n",
        "\n",
        "print (\"writing json\")\n",
        "\n",
        "with tf.io.gfile.GFile(f\"{outdir}/predictions.json\", \"w\") as f:\n",
        "    json.dump(predictions_json, f, indent=4)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Going to candidates file\n",
            "Reading examples from: /content/data/tensorflow2-question-answering/simplified-nq-test.jsonl\n",
            "setting up eval features\n",
            "compute_pred_dict\n",
            "merging examples...\n",
            "done.\n",
            "Computing predictions...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7599dcbaa912496bbeda880686b8f25a",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, max=346), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "writing json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ore6D3Fku6oY",
        "colab_type": "text"
      },
      "source": [
        "### Main Change\n",
        "mmmarchetti had the following note about this notebook.<br>\n",
        "Here is the small, but main change [to prokaj's notebook]: we created an if to check the predicted response type and thus filter / identify the responses that are passed to the submission file."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sTdPRVxZvIRK",
        "colab_type": "text"
      },
      "source": [
        "#### Filtering the Answers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tefIpu_ou9gm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_short_answer(entry):\n",
        "    answer = []    \n",
        "    if entry['answer_type'] == 0:\n",
        "        return \"\"\n",
        "    \n",
        "    elif entry['answer_type'] == 1:\n",
        "        return 'YES'\n",
        "    \n",
        "    elif entry['answer_type'] == 2:\n",
        "        return 'NO'\n",
        "        \n",
        "    elif entry[\"short_answer_score\"] < 1.5:\n",
        "        return \"\"\n",
        "    \n",
        "    else:\n",
        "        for short_answer in entry[\"short_answers\"]:\n",
        "            if short_answer[\"start_token\"] > -1:\n",
        "                answer.append(str(short_answer[\"start_token\"]) + \":\" + str(short_answer[\"end_token\"]))\n",
        "    \n",
        "        return \" \".join(answer)\n",
        "\n",
        "def create_long_answer(entry):\n",
        "    \n",
        "    answer = []\n",
        "    \n",
        "    if entry['answer_type'] == 0:\n",
        "        return ''\n",
        "    \n",
        "    elif entry[\"long_answer_score\"] < 1.5:\n",
        "        return \"\"\n",
        "\n",
        "    elif entry[\"long_answer\"][\"start_token\"] > -1:\n",
        "        answer.append(str(entry[\"long_answer\"][\"start_token\"]) + \":\" + str(entry[\"long_answer\"][\"end_token\"]))\n",
        "        return \" \".join(answer)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B3s7hVDYvL-I",
        "colab_type": "text"
      },
      "source": [
        "#### Creating a DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qux93i9pvFUQ",
        "colab_type": "code",
        "outputId": "8b681326-755f-4eb7-eaf5-ed9ebda31427",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "source": [
        "test_answers_df = pd.read_json(f\"{outdir}/predictions.json\")\n",
        "for var_name in ['long_answer_score','short_answer_score','answer_type']:\n",
        "    test_answers_df[var_name] = test_answers_df['predictions'].apply(lambda q: q[var_name])\n",
        "test_answers_df[\"long_answer\"] = test_answers_df[\"predictions\"].apply(create_long_answer)\n",
        "test_answers_df[\"short_answer\"] = test_answers_df[\"predictions\"].apply(create_short_answer)\n",
        "test_answers_df[\"example_id\"] = test_answers_df[\"predictions\"].apply(lambda q: str(q[\"example_id\"]))\n",
        "\n",
        "long_answers = dict(zip(test_answers_df[\"example_id\"], test_answers_df[\"long_answer\"]))\n",
        "short_answers = dict(zip(test_answers_df[\"example_id\"], test_answers_df[\"short_answer\"]))\n",
        "\n",
        "test_answers_df.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>predictions</th>\n",
              "      <th>long_answer_score</th>\n",
              "      <th>short_answer_score</th>\n",
              "      <th>answer_type</th>\n",
              "      <th>long_answer</th>\n",
              "      <th>short_answer</th>\n",
              "      <th>example_id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>{'example_id': -9198586108074363474, 'long_ans...</td>\n",
              "      <td>7.318449</td>\n",
              "      <td>7.318449</td>\n",
              "      <td>3</td>\n",
              "      <td>2777:4157</td>\n",
              "      <td>3054:3058</td>\n",
              "      <td>-9198586108074363474</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>{'example_id': -9106782542406690435, 'long_ans...</td>\n",
              "      <td>10.478188</td>\n",
              "      <td>10.478188</td>\n",
              "      <td>3</td>\n",
              "      <td>452:667</td>\n",
              "      <td>561:563</td>\n",
              "      <td>-9106782542406690435</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>{'example_id': -9076824655968712429, 'long_ans...</td>\n",
              "      <td>5.556304</td>\n",
              "      <td>5.556304</td>\n",
              "      <td>4</td>\n",
              "      <td>347:391</td>\n",
              "      <td>387:390</td>\n",
              "      <td>-9076824655968712429</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>{'example_id': -9030298026304897796, 'long_ans...</td>\n",
              "      <td>8.951933</td>\n",
              "      <td>8.951933</td>\n",
              "      <td>3</td>\n",
              "      <td>620:727</td>\n",
              "      <td>629:636</td>\n",
              "      <td>-9030298026304897796</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>{'example_id': -9017879887869944707, 'long_ans...</td>\n",
              "      <td>6.273268</td>\n",
              "      <td>6.273268</td>\n",
              "      <td>3</td>\n",
              "      <td>16:271</td>\n",
              "      <td>87:93</td>\n",
              "      <td>-9017879887869944707</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                         predictions  ...            example_id\n",
              "0  {'example_id': -9198586108074363474, 'long_ans...  ...  -9198586108074363474\n",
              "1  {'example_id': -9106782542406690435, 'long_ans...  ...  -9106782542406690435\n",
              "2  {'example_id': -9076824655968712429, 'long_ans...  ...  -9076824655968712429\n",
              "3  {'example_id': -9030298026304897796, 'long_ans...  ...  -9030298026304897796\n",
              "4  {'example_id': -9017879887869944707, 'long_ans...  ...  -9017879887869944707\n",
              "\n",
              "[5 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KLhi7tgDvRVH",
        "colab_type": "text"
      },
      "source": [
        "### Generating the Submission File"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T62_ONnivUfY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sample_submission = pd.read_csv(f\"{datadir}/{competition}/sample_submission.csv\")\n",
        "\n",
        "long_prediction_strings = sample_submission[sample_submission[\"example_id\"].str.contains(\"_long\")].apply(lambda q: long_answers[q[\"example_id\"].replace(\"_long\", \"\")], axis=1)\n",
        "short_prediction_strings = sample_submission[sample_submission[\"example_id\"].str.contains(\"_short\")].apply(lambda q: short_answers[q[\"example_id\"].replace(\"_short\", \"\")], axis=1)\n",
        "\n",
        "sample_submission.loc[sample_submission[\"example_id\"].str.contains(\"_long\"), \"PredictionString\"] = long_prediction_strings\n",
        "sample_submission.loc[sample_submission[\"example_id\"].str.contains(\"_short\"), \"PredictionString\"] = short_prediction_strings\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DEDab0LOvYXg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sample_submission.to_csv(f\"{outdir}/submission.csv\", index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aPLARC3bvbyi",
        "colab_type": "code",
        "outputId": "793c33e5-fcf5-44b0-c5c5-b0317ef5e43c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "sample_submission.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>example_id</th>\n",
              "      <th>PredictionString</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-1011141123527297803_long</td>\n",
              "      <td>223:277</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-1011141123527297803_short</td>\n",
              "      <td>224:226</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-1028916936938579349_long</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-1028916936938579349_short</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-1055197305756217938_long</td>\n",
              "      <td>221:335</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                   example_id PredictionString\n",
              "0   -1011141123527297803_long          223:277\n",
              "1  -1011141123527297803_short          224:226\n",
              "2   -1028916936938579349_long                 \n",
              "3  -1028916936938579349_short                 \n",
              "4   -1055197305756217938_long          221:335"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "we9yOLCCvvDo",
        "colab_type": "text"
      },
      "source": [
        "* Yes Answers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fD2yAxelvw-h",
        "colab_type": "code",
        "outputId": "c15ec201-67bd-4026-92b8-f363153dcfdd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "yes_answers = sample_submission[sample_submission['PredictionString'] == 'YES']\n",
        "yes_answers"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>example_id</th>\n",
              "      <th>PredictionString</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>-1651666484583736653_short</td>\n",
              "      <td>YES</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>113</th>\n",
              "      <td>-3461207570097431362_short</td>\n",
              "      <td>YES</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>303</th>\n",
              "      <td>-8260765274544672220_short</td>\n",
              "      <td>YES</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>309</th>\n",
              "      <td>-871487000194429353_short</td>\n",
              "      <td>YES</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>553</th>\n",
              "      <td>5962215690907729115_short</td>\n",
              "      <td>YES</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                     example_id PredictionString\n",
              "27   -1651666484583736653_short              YES\n",
              "113  -3461207570097431362_short              YES\n",
              "303  -8260765274544672220_short              YES\n",
              "309   -871487000194429353_short              YES\n",
              "553   5962215690907729115_short              YES"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eT05JPvQvl09",
        "colab_type": "text"
      },
      "source": [
        "* No Answers\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bE7-3uFKv676",
        "colab_type": "code",
        "outputId": "19d97a08-cafc-4bd3-c72a-d4d9155f1a4f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        }
      },
      "source": [
        "no_answers = sample_submission[sample_submission['PredictionString'] == 'NO']\n",
        "no_answers"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>example_id</th>\n",
              "      <th>PredictionString</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>469</th>\n",
              "      <td>418890410382116795_short</td>\n",
              "      <td>NO</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                   example_id PredictionString\n",
              "469  418890410382116795_short               NO"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mMYJ3FN-v-fU",
        "colab_type": "text"
      },
      "source": [
        "* Balnk Answers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VFv3tE0YwB1B",
        "colab_type": "code",
        "outputId": "b284896c-3cb8-4cce-fbb0-14824b5ca670",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "blank_answers = sample_submission[sample_submission['PredictionString'] == '']\n",
        "blank_answers.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>example_id</th>\n",
              "      <th>PredictionString</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-1028916936938579349_long</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-1028916936938579349_short</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>-1114334749483663139_long</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>-1114334749483663139_short</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>-1152268629614456016_long</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                    example_id PredictionString\n",
              "2    -1028916936938579349_long                 \n",
              "3   -1028916936938579349_short                 \n",
              "8    -1114334749483663139_long                 \n",
              "9   -1114334749483663139_short                 \n",
              "10   -1152268629614456016_long                 "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9D_W5tinwEET",
        "colab_type": "code",
        "outputId": "1f795dfe-dd72-4835-c80a-7c089f2d9040",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "blank_answers.count()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "example_id          205\n",
              "PredictionString    205\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q3v0nsddh8V8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## The following code is from the original prokaj notebook\n",
        "if False and public_dataset:\n",
        "    # Stuff for debugging\n",
        "    print(test_answers_df[\"long_answer_score\"].describe())\n",
        "    print(np.bincount(test_answers_df['answer_type'].values))\n",
        "    print(test_answers_df[test_answers_df['answer_type']==0])\n",
        "    print(test_answers_df.predictions.values[-4])\n",
        "    print(sample_submission.head())\n",
        "\n",
        "    class ShowPrediction:\n",
        "        def __init__(self,jsonl_file):\n",
        "            self._data = {}\n",
        "            with open(jsonl_file,'r') as f:\n",
        "                for line in f.readlines():\n",
        "                    d = json.loads(line)\n",
        "                    #print(d.keys())\n",
        "                    self._data[int(d['example_id'])]={\n",
        "                        'text': d['document_text'],\n",
        "                        'question': d['question_text']\n",
        "                    }\n",
        "        def __call__(self,prediction,include_full_text=True):\n",
        "            data = self._data[prediction['example_id']]\n",
        "            res = {'question': data['question']}\n",
        "            if include_full_text:\n",
        "                res['text'] = data['text']\n",
        "            for type_ in ['long_answer','short_answers']:\n",
        "                ans = prediction[type_]\n",
        "                if isinstance(ans,list):\n",
        "                    ans = ans[0]\n",
        "                start,end = ans['start_token'],ans['end_token']\n",
        "                res[type_] = ' '.join(data['text'].split()[start:end])\n",
        "            return res\n",
        "\n",
        "    show_pred = ShowPrediction(f\"{test_file}\")\n",
        "    # the following produces extensive output\n",
        "    for pred in test_answers_df.predictions[test_answers_df.answer_type==0]:\n",
        "        print(json.dumps(show_pred(pred,include_full_text=True),indent=4))\n",
        "\n",
        "    for pred in np.random.choice(predictions_json['predictions'],10):\n",
        "        print(json.dumps(show_pred(pred,include_full_text=False),indent=4))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "akKwcXNnJvi2",
        "colab_type": "code",
        "outputId": "ee613edc-6521-4a0d-de79-d8e2b7ab7b46",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        }
      },
      "source": [
        "! zdump PST\n",
        "if verbose:\n",
        "    ! pwd\n",
        "    list_files(basedir)\n",
        "! ls -l {outdir}"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "PST  Sat Jan  4 00:29:52 2020 PST\n",
            "/content\n",
            "content/\n",
            "    bert-joint-baseline-output.npz\n",
            "    data/\n",
            "        compdata.flag\n",
            "        bertfiles.flag\n",
            "        bert-joint-baseline/\n",
            "            vocab-nq.txt\n",
            "            nq-test.tfrecords\n",
            "            model_cpkt-1.data-00000-of-00002\n",
            "            bert_config.json\n",
            "            model_cpkt-1.index\n",
            "            model_cpkt-1.data-00001-of-00002\n",
            "        tensorflow2-question-answering/\n",
            "            simplified-nq-train.jsonl\n",
            "            simplified-nq-test.jsonl\n",
            "            sample_submission.csv\n",
            "lrwxrwxrwx 1 root root 62 Jan  4 00:07 /content/output -> '/content/gdrive/My Drive/Colab/bertqa/BERTjoint_yes_no/output/'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CmkZDNQK98df",
        "colab_type": "text"
      },
      "source": [
        "## -- Submitting Results --"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rKE9TaFxJx1P",
        "colab_type": "code",
        "outputId": "dd477189-09f9-42ff-838a-7025eb6aee61",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        }
      },
      "source": [
        "raise ExecutionStop(\"Don't let run all go beyond this\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ExecutionStop",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mExecutionStop\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-60-d55cca21a26e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mraise\u001b[0m \u001b[0mExecutionStop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Don't let run all go beyond this\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mExecutionStop\u001b[0m: Don't let run all go beyond this"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rYwGNnso97Yr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%bash\n",
        "## View Previous Results you have submitted\n",
        "#kaggle competitions list\n",
        "kaggle competitions submissions -c {competition}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rr1JY__B-PnG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Make Submission\n",
        "# You may be able to submit to some competitions through the API\n",
        "! kaggle competitions submit -c {competition} -f $RESULT_CSV  -m 'test kaggle cli 3'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Kr2RubeNbUu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!du -lh /"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sBJjB1J3-fUL",
        "colab_type": "text"
      },
      "source": [
        "Verify submission by viewing previous results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7OK-KqArvjKu"
      },
      "source": [
        "# Cells below this need to be deleted before submitting Notebook to competition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pYcisyyw1TRw",
        "colab_type": "text"
      },
      "source": [
        "End of Project Notebook"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Udo19R6y9LD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Make sure user does not accedentially execute beyond end\n",
        "raise ExecutionStop(\"Stopping execution\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "lRKAhZK8yvO_"
      },
      "source": [
        "# ====== Please fold this stuff up and ignore ====="
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "r_TVSYoeyvPa"
      },
      "source": [
        "### SSH Setup\n",
        "This is only neeeded if you want to log into the Colab machine. Otherwise fold it up and ignore.<br>\n",
        "To use it you have to create a login at https://ngrok.com\n",
        "<Details>Thanks to Imad El Hanafi (https://imadelhanafi.com) for showing me how to do this.<p>\n",
        "You will need to create a free account at https://ngrok.com/ for the SSH tunnel to work.</Details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eK8m0r50aPHO",
        "colab_type": "text"
      },
      "source": [
        "File paths are hard coded here because this may be run before program variables are established."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "36OI5-u2u24e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## if you want to use the Kaggle api from command line you will need a kaggle.json file\n",
        "from pathlib import Path\n",
        "if Path('/content/gdrive/My Drive/Colab/kaggle.json').exists() or \\\n",
        "                                    Path('/content/kaggle.json').exists():\n",
        "    pass    # we found a kaggle.json file\n",
        "else:\n",
        "    # Give user opportunity to upload a kaggle.json file\n",
        "    from google.colab import files\n",
        "    print('Upload kaggle.json if you want the Kaggle API to be availabel in bash.')\n",
        "    # The files.upload() command is failing sporatically with:\n",
        "    #   TypeError: Cannot read property '_uploadFiles' of undefined (just run this cell again)\n",
        "    ! rm \"/content/kaggle.json\"  2> /dev/null\n",
        "    files.upload()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Y0NMPj1fyvPb",
        "colab": {}
      },
      "source": [
        "%%bash\n",
        "## Install sshd; Set to allow login and config\n",
        "apt-get install -o=Dpkg::Use-Pty=0 openssh-server pwgen > /dev/null\n",
        "mkdir -p /var/run/sshd\n",
        "echo \"PermitRootLogin yes\" >> /etc/ssh/sshd_config\n",
        "echo \"PasswordAuthentication yes\" >> /etc/ssh/sshd_config\n",
        "\n",
        "# set host key to known value (need to test if exist)\n",
        "gdown -O \"/etc/ssh/ssh_host_rsa_key\" --id 17Vp-rLM0kLVsIqxo7GkV3YXibGCJ7WCR\n",
        "chown 600 \"/etc/ssh/ssh_host_rsa_key\"    # private key will be ignored if not secure\n",
        "gdown -O \"/etc/ssh/ssh_host_rsa_key.pub\" --id 1-5yW1EwMdBN0YlRe7McmwDxzmGyvq-gW\n",
        "# get script to modify login shell to match env of Notebook\n",
        "gdown -O \"/root/init_shell.sh\" --id 1-9s5wuq5TkebgKbFvBYy4EeM8c2Ee0xc\n",
        "\n",
        "# this script will give fix the login shell so Python will work\n",
        "if [ -f \"/root/init_shell.sh\" ]; then\n",
        "    echo \"source /root/init_shell.sh\" >> /root/.bashrc\n",
        "fi"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "S1mmJ__lyvPh",
        "colab": {}
      },
      "source": [
        "## setup ssh user / pass and start sshd\n",
        "\n",
        "#Generate a random root password\n",
        "import random, string\n",
        "sshpass = ''.join(random.choice(string.ascii_letters + string.digits) for i in range(30))\n",
        "\n",
        "#Set root password\n",
        "! echo root:$sshpass | chpasswd\n",
        "\n",
        "#Run sshd\n",
        "get_ipython().system_raw('/usr/sbin/sshd -D &')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "edJ3pW6YyvPl",
        "colab": {}
      },
      "source": [
        "%%bash\n",
        "## Get Ngrok from gdrive or try to download (see: https://ngrok.com/download)\n",
        "if [ -f \"/content/bertqa/colab/ngrok-stable-linux-amd64.zip\" ]; then\n",
        "    cp \"/content/bertqa/colab/ngrok-stable-linux-amd64.zip\" .\n",
        "    echo \"Using ngrok-stable-linux-amd64.zip from gdrive\"\n",
        "else\n",
        "    wget -q -c -nc https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
        "fi\n",
        "unzip -qq -n ngrok-stable-linux-amd64.zip\n",
        "rm ngrok-stable-linux-amd64.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YR0N4Iw8yvPq",
        "colab": {}
      },
      "source": [
        "## Get user to enter auth token from ngrok and start tunnel\n",
        "\n",
        "# Get token from ngrok for the tunnel\n",
        "print(\"Get your authtoken from https://dashboard.ngrok.com/auth\")\n",
        "import getpass\n",
        "authtoken = getpass.getpass()\n",
        "\n",
        "#Create tunnel\n",
        "get_ipython().system_raw('./ngrok authtoken $authtoken && ./ngrok tcp 22 &')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "1VjsRvCTyvPt"
      },
      "source": [
        "#### ==============================<br>||====&nbsp;&nbsp;  SSH Login Credentials &nbsp;&nbsp;====||<br>=============================="
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "cellView": "both",
        "id": "WKjt0Wh0yvPv",
        "colab": {}
      },
      "source": [
        "#@title\n",
        "print(\"username: root\")\n",
        "print(\"password: \", sshpass)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7OlwbxpWyvPz"
      },
      "source": [
        "Get the host name and port number at: https://dashboard.ngrok.com/status\n",
        "\n",
        "```bash\n",
        "ssh root@0.tcp.ngrok.io -p [ngrok_port]\n",
        "Login as: root\n",
        "Servrer refused our key\n",
        "root@0.tcp.ngrok.io's password: [see above]\n",
        "\n",
        "(Colab):/content$\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "bjJcssgxyvP0"
      },
      "source": [
        "Install programs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "euzlUBHLyvP1",
        "colab": {}
      },
      "source": [
        "%%bash\n",
        "# vim\n",
        "apt-get install vim > /dev/null\n",
        "echo \"set tabstop     =4\" >> ~/.vimrc\n",
        "echo \"set softtabstop =4\" >> ~/.vimrc\n",
        "echo \"set shiftwidth  =4\" >> ~/.vimrc\n",
        "echo \"set expandtab\"      >> ~/.vimrc\n",
        "\n",
        "# js is a JSON processor\n",
        "apt-get install js > /dev/null\n",
        "\n",
        "apt-get install tree > /dev/null\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Ugtt5PxzyvP3"
      },
      "source": [
        "If you need to kill Ngrok run this cell"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CiP9qYfgyvP4",
        "colab": {}
      },
      "source": [
        "if False:\n",
        "    !kill $(ps aux | grep './ngrok' | awk '{print $2}')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "n3MP_pJ5yvP5"
      },
      "source": [
        "## -- Misc Notes --"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "cm6ErVGkyvP6"
      },
      "source": [
        "### Prevent Disconnects\n",
        "Colab periodically disconnects the browser.<br>\n",
        "You have to save model checkpoints to Google Drive so you don't lose work<br>\n",
        "See: https://mc.ai/google-colab-drive-as-persistent-storage-for-long-training-runs/<br>\n",
        "Something to try...<br>\n",
        "Ctrl+Shift+i in browser and in console run this code...\n",
        "```\n",
        "function KeepAlive(){\n",
        "    console.log(\"Maintaining Connection\");\n",
        "    document.querySelector(\"colab-toolbar-button#connect\").click()\n",
        "}\n",
        "setInterval(KeepAlive,60000);\n",
        "```\n",
        "There have been reports of people having their GPU privileges suspended for letting processes run for over 12 hours. It seems that they may penalize you rather than just cutting you off."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "enrv0jdCyzCY",
        "colab_type": "text"
      },
      "source": [
        "### Monitor GPU\n",
        "```\n",
        "# From cli I think to monitor GPU while fiting\n",
        "$ nvidia-smi dmon\n",
        "$ nvidia-smi pmon\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j5B7tvshjsgs",
        "colab_type": "text"
      },
      "source": [
        "### Code From Elsewhere"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gpTRsZHmzZcA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!nvidia-smi -i 0 -q -d MEMORY,UTILIZATION,POWER,CLOCK,COMPUTE"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}