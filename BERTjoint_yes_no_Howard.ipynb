{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERTjoint yes no - Howard.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "Js7aj8RDhSjz",
        "MNzAuLZDzG_R",
        "javRwDS6zhOC",
        "lRKAhZK8yvO_"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hogo56/BertQA/blob/master/BERTjoint_yes_no_Howard.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Js7aj8RDhSjz",
        "colab_type": "text"
      },
      "source": [
        "# BERTjoint Question Answering Contest"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q5L2dQHvfZCC",
        "colab_type": "text"
      },
      "source": [
        "This code is designed to run on Google Colab. Because we also want to submit the kernel to the Kaggle QA competition it needs to be able to run in either location. This is handled by having python vars when the FLAGS are set:\n",
        "\n",
        "* Kaggle &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; *(cwd = /kaggle/working/)*<br>\n",
        "  {datadir} = /kaggle/input<br>\n",
        "  {outdir} = /kaggle/working<br>\n",
        "* Colab &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; *(cwd = /content/)*<br>\n",
        "  {datadir} = /content/data<br>\n",
        "  {outdir} = /content/output<br>\n",
        "\n",
        "#### - Required Libraries\n",
        "\n",
        "#### - Required Data\n",
        "\n",
        "#### - Inputs\n",
        "   * {datadir}/tensorflow2-question-answering/simplified-nq-test.jsonl\n",
        "   * {datadir}/tensorflow2-question-answering/simplified-nq-test.jsonl\n",
        "\n",
        "#### - Outputs\n",
        "   * {outdir}/predictions.json\n",
        "   * {outdir}/submission.csv<br>\n",
        "   * {outdir}/eval.tf_record<br>\n",
        "   * {outdir}/.ipynb_checkpoints/<br>\n",
        "\n",
        "#### - Notes\n",
        "Look at https://www.kaggle.com/prokaj/bert-joint-baseline-notebook/notebook which is similar to this\n",
        "\n",
        "\n",
        "#### - Credit\n",
        "\n",
        "This notebook is a fork of [mmmarchetti's notebook](https://www.kaggle.com/mmmarchetti/tensorflow-2-0-bert-yes-no-answers) which was a fork of [prokaj's - bert joint baseline notebook](https://www.kaggle.com/prokaj/bert-joint-baseline-notebook/notebook).<br>\n",
        "mmmarchetti made some modifications to slightly improve the code and get the YES / NO answers and leave the unknowns blank."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HaUs7DxSKDVG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Required setup code (will eventually be cut out into a lib file)\n",
        "class ExecutionStop(Exception):             # Custom Error Handler\n",
        "    def __init__(self, value): self.value=value\n",
        "    def __str__(self): return(str(self.value))\n",
        "\n",
        "def list_files(startpath):                  #  Show files\n",
        "    for root, dirs, files in os.walk(startpath):\n",
        "        level = root.replace(startpath, '').count(os.sep)\n",
        "        indent = ' ' * 4 * (level)\n",
        "        print('{}{}/'.format(indent, os.path.basename(root)))\n",
        "        subindent = ' ' * 4 * (level + 1)\n",
        "        for f in files:\n",
        "            print('{}{}'.format(subindent, f))\n",
        "# raise ExecutionStop(\"Message\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F30YID4j_khp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Set file locations    (these variables are not implemented in the FLAGS code yet)\n",
        "import os\n",
        "kernel = ''\n",
        "if os.path.exists('/content'):\n",
        "    print(\"Detected running on Colab\")\n",
        "    kernel = 'Colab'\n",
        "    basedir = '/content'\n",
        "    libdir = '/content/lib'\n",
        "    datadir = '/content/data'\n",
        "    outdir = '/content/output'          # maybe this should be \"/gdrive/My Drive/colab/bertqa/output\"\n",
        "elif os.path.exists('/kaggle'):\n",
        "    print(\"Detected running on Kaggle\")\n",
        "    kernel = 'Kaggle'\n",
        "    basedir = '/kaggle'\n",
        "    libdir = '/kaggle/lib'\n",
        "    datadir = '/kaggle/input'           # this may need to be '../input' for scoring\n",
        "    outdir = '/kaggle/working'          # this may need to be '.' for scoring\n",
        "else:\n",
        "    raise ExecutionStop(\"Cannot continue without determining file locations\")\n",
        "\n",
        "## Config Variables\n",
        "competition = 'tensorflow2-question-answering'\n",
        "train_file = 'simplified-nq-train.jsonl'\n",
        "test_file = 'simplified-nq-test.jsonl'\n",
        "gprojdir = 'bertqa'                 # The project directory on Drive for this competition\n",
        "glibdir = 'BERTjoint_yes_no'        # lib subdir for this notebook on Drive\n",
        "verbose = True\n",
        "EnableAllCode = False               # Prevent codeblocks that should not execute on Run All\n",
        "DownloadBigFiles = True             # Files will not download if already on drive\n",
        "\n",
        "# ToDo -------- search for these vars in /lib and make go away\n",
        "on_kaggle_server = True if kernel == 'Kaggle' else False\n",
        "nq_test_file = f\"{datadir}/{competition}/{train_file}\"\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MNzAuLZDzG_R",
        "colab_type": "text"
      },
      "source": [
        "# ============= Machine Spinup ============="
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vKPv6ALYbXiY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! zdump PST\n",
        "! pwd\n",
        "if verbose:\n",
        "    list_files('/content')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zNni7Dc1NzVe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Reset kernel without removing downloaded data files and libs\n",
        "if False:\n",
        "    %reset\n",
        "    ! rm -i {outdir}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M0k60Je4YEQa",
        "colab_type": "text"
      },
      "source": [
        "## -- Main System Config --\n",
        "<Details><Summary>Global Config</Summary>\n",
        "Put any global system configuration here"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JNKoTizrh9RG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%bash -s {libdir} {datadir} {outdir}\n",
        "zdump PST               # Not sure what is up with the time, PST is running about 8 hrs ahead\n",
        "[ -d $1 ] || mkdir -p $1\n",
        "[ -d $2 ] || mkdir -p $2\n",
        "[ -d $3 ] || mkdir -p $3\n",
        "if [ -d /content/sample_data ]; then\n",
        "    rm -rf /content/sample_data\n",
        "fi"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UdjitRnQynwk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os, sys\n",
        "sys.path.append(libdir)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aheaRzVE6fs1",
        "colab_type": "text"
      },
      "source": [
        "## -- Setup --"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gixU6_gv678n",
        "colab_type": "text"
      },
      "source": [
        "###Google Drive\n",
        "<Details>There are several ways to provide access to your Google Drive from Colab. (What about the Drive FUSE wrapper?)<br>\n",
        "I am not sure if this is the best. This mounts your Drive into the machine.<br>\n",
        "I expect there will be a folder in the Drive that we all share.</Details>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "auXx45x70Qcs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## File link to Google Drive\n",
        "if kernel == 'Colab':\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/gdrive', force_remount=False)   # true to reread drive\n",
        "    # Create a shorter shared directory name than one with a space\n",
        "    ! ln -s '/content/gdrive/My Drive/{gprojdir}' /content/{gprojdir}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ga-sQOkk1YvA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if False:\n",
        "    ## Flush and unmount Google Drive\n",
        "    # You probably won't do this but if you want to at some point click the play button\n",
        "    drive.flush_and_unmount()\n",
        "\n",
        "## Install the large file downloader for Google Drive if needed (Colab already has it installed)\n",
        "#  This works from bash or Python\n",
        "#! pip install gdown   (if you need to install it)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q8BLJAW95ih-",
        "colab_type": "text"
      },
      "source": [
        "### Kaggle API\n",
        "<Details>You will need Kaggle API token to link the Colab instance to your Kaggle account to get data, etc.<br>\n",
        "Go to: https://www.kaggle.com/yourID/account and click on the \"Create New API Token: button to get a file named kaggle.json.<p>You can put your kaggle.json file in your google drive at My Drive/colab/kaggle.json.<br>\n",
        "Alternately, you can store it on your local machine and the script will ask you to upload it.</Details>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lc3IYPUg17c8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Link to Kaggle\n",
        "if kernel == 'Colab':\n",
        "    from google.colab import files\n",
        "\n",
        "    # see if there is a kaggle.json file in gdrive\n",
        "    try:\n",
        "        # see if auth file is in gdrive\n",
        "        f = open(\"/content/gdrive/My Drive/colab/kaggle.json\")\n",
        "        os.environ['KAGGLE_CONFIG_DIR'] = \"/content/gdrive/My Drive/colab/\"\n",
        "        ! ls -l \"/content/gdrive/My Drive/colab/kaggle.json\"\n",
        "    except IOError:\n",
        "        # Have user upload file\n",
        "        ! rm /content/kaggle.json  2> /dev/null\n",
        "        print('Upload kaggle.json.')\n",
        "        # The files.upload() command is failing sporatically with:\n",
        "        #   TypeError: Cannot read property '_uploadFiles' of undefined (just run again)\n",
        "        files.upload()\n",
        "        os.environ['KAGGLE_CONFIG_DIR'] = \"/content/\"\n",
        "        ! ls -l /content/kaggle.json\n",
        "\n",
        "    import kaggle"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "javRwDS6zhOC",
        "colab_type": "text"
      },
      "source": [
        "# =========== Project Specific Stuff ==========="
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "JFQgZRN2PaEE"
      },
      "source": [
        "## -- Project Setup --"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gAqCNh4y8DJz",
        "colab_type": "text"
      },
      "source": [
        "### Download Dataset and Support Files"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cGGM0EVmM24H",
        "colab_type": "text"
      },
      "source": [
        "Kaggle Competition Files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UF-28Eb21E2w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Competition Dataset  (5GB zipped)\n",
        "if DownloadBigFiles and kernel == 'Colab':\n",
        "    if not os.path.exists(f\"{datadir}/compdata.flag\"):\n",
        "        if verbose:\n",
        "            ! kaggle competitions list\n",
        "            print()\n",
        "        print(\"Downloading Competition Data\\n\")\n",
        "        ! kaggle competitions download -c {competition} -p {datadir}\n",
        "        ! mv {datadir}/sample_submission.csv {outdir}/\n",
        "        ! mkdir -p {datadir}/{competition}/\n",
        "        ! unzip {datadir}/{train_file}.zip -d /{datadir}/{competition}\n",
        "        ! rm {datadir}/{train_file}.zip\n",
        "        ! unzip {datadir}/{test_file}.zip -d {datadir}/{competition}\n",
        "        ! rm {datadir}/{test_file}.zip\n",
        "        ! touch {datadir}/compdata.flag\n",
        "    else:\n",
        "        print(\"Competition Data already exists. Not downloading.\\n\")\n",
        "        !ls -l {datadir}/*\n",
        "else:\n",
        "    pass        # if Kaggle, data will already be there\n",
        "\n",
        "public_dataset = os.path.getsize(f\"{datadir}/{competition}/{test_file}\")<20_000_000\n",
        "private_dataset = os.path.getsize(f\"{datadir}/{competition}/{test_file}\")>=20_000_000"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oZhNV3PJHOEL",
        "colab_type": "text"
      },
      "source": [
        "BERTjoint files from: https://www.kaggle.com/prokaj/bert-joint-baseline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s207mPqAHDVw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get BERTjoint model files\n",
        "if DownloadBigFiles and kernel == 'Colab':\n",
        "    if not os.path.exists(f\"{datadir}/bertfiles.flag\"):\n",
        "        print(\"Downloading BERT-joint Model\\n\")\n",
        "        ! mkdir -p {datadir}/bert-joint-baseline/\n",
        "        ! gdown -O {datadir}/prokaj-bert-joint-baseline.zip --id 1vnJ052xpw1fmpWLMyLhrkUn5j_ymoGP7\n",
        "        getfiles = \"bert_config* model_cpkt* nq-test* vocab*\"\n",
        "        ! unzip {datadir}/prokaj-bert-joint-baseline.zip {getfiles} -d {datadir}/bert-joint-baseline/\n",
        "        ! rm {datadir}/prokaj-bert-joint-baseline.zip\n",
        "        if verbose:\n",
        "            ! ls -l {datadir}/bert-joint-baseline/\n",
        "        ! touch {datadir}/bertfiles.flag\n",
        "    else:\n",
        "        print(\"BERT-joint Files already exists. Not downloading.\\n\")\n",
        "        ! ls -l {datadir}/bert-joint-baseline/\n",
        "else:\n",
        "    pass        # if Kaggle, data will already be there"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BOHYKCuLHXAT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get BERTjoint model files\n",
        "if False and DownloadBigFiles and kernel == 'Colab':\n",
        "    if not os.path.exists(f\"{datadir}/bertfiles.flag\"):\n",
        "        print(\"Downloading BERT-joint Model\\n\")\n",
        "        ! mkdir -p {datadir}/bert-joint-baseline/\n",
        "        getfiles = \"bert_config* model_cpkt* nq-test* vocab*\"\n",
        "        ! unzip {basedir}/{gprojdir}/data/prokaj-bert-joint-baseline.zip {getfiles} -d {datadir}/bert-joint-baseline/\n",
        "        if verbose:\n",
        "            ! ls -l {datadir}/bert-joint-baseline/\n",
        "        ! touch {datadir}/bertfiles.flag\n",
        "    else:\n",
        "        print(\"BERT-joint Files already exists. Not downloading.\\n\")\n",
        "        ! ls -l {datadir}/bert-joint-baseline/\n",
        "else:\n",
        "    pass        # if Kaggle, data will already be there"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XJoB8YB3MrvX",
        "colab_type": "text"
      },
      "source": [
        "BERTjoint files from: \n",
        "https://github.com/google-research/language/tree/master/language/question_answering/bert_joint\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1uvuFGJ0MqHo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get BERTjoint model files\n",
        "if False and DownloadBigFiles and kernel == 'Colab':\n",
        "    if not os.path.exists(f\"{datadir}/bertfiles.flag\"):\n",
        "        print(\"Downloading BERT-joint Model\\n\")\n",
        "        ! gsutil cp -R gs://bert-nq/bert-joint-baseline {datadir}\n",
        "        ! touch {datadir}/bertfiles.flag\n",
        "    else:\n",
        "        print(\"BERT-joint Files already exists. Not downloading.\\n\")\n",
        "        ! ls -l {datadir}/bert-joint-baseline/\n",
        "else:\n",
        "    pass        # if Kaggle, data will already be there"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c4NrOOtpOVN0",
        "colab_type": "text"
      },
      "source": [
        "BERT files from: https://github.com/google-research/bert<br>\n",
        "(Not the model we are using at the moment)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pdp6tqXmmLw_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## get BERT (this is unlikely to be the BERT-joint files needed for competition)\n",
        "# this version of BERT seems won't import as is. On line 88 of lib/bert/optimization.py\n",
        "#    change   tr.train.Optimizer to tf.keras.optimizers.Optimizer\n",
        "if False and DownloadBigFiles and kernel == 'Colab':\n",
        "    ! git clone https://github.com/google-research/bert.git\n",
        "    ! mv bert lib\n",
        "\n",
        "    # get some pretrained models  (I really  have no idea what these are or if useful)\n",
        "    ! wget https://storage.googleapis.com/bert_models/2018_10_18/cased_L-12_H-768_A-12.zip\n",
        "    ! unzip cased_L-12_H-768_A-12.zip\n",
        "    ! rm cased_L-12_H-768_A-12.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oU6FPUHjvH1r",
        "colab_type": "text"
      },
      "source": [
        "<Details><Summary>BERT tf.compat.v1 Notes</Summary>\n",
        "baseline_w_bert_translated_to_tf2_0 (next code block) comes from /dimitreoliveira with this warning:<br>\n",
        "This baseline uses code that was migrated from TF1.x. Be aware that it contains use of tf.compat.v1, which is not permitted to be eligible for TF2.0 prizes in this competition. It is intended to be used as a starting point, but we're excited to see how much better you can do using TF2.0!<br>\n",
        "https://www.kaggle.com/dimitreoliveira/using-tf-2-0-w-bert-on-nq-translated-to-tf2-0</Details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "bNPxSmhMJC5c"
      },
      "source": [
        "### Library Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yr69yF8fepTA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Copy lib files over from Google Drive\n",
        "if kernel == 'Colab':\n",
        "    ! cp -a /content/{gprojdir}/lib/{glibdir}/* {libdir}    # ToDo ----- Subdir this by Notebook\n",
        "if kernel == 'Kaggle':\n",
        "    pass                            # ToDo ------ Need to figure out how Kaggle gets updated lib fies\n",
        "if verbose:\n",
        "    ! ls -l {libdir}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qhpBRp11R0jv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Load Libraries\n",
        "import os, sys, importlib\n",
        "\n",
        "#magic to make colab path to Tensorflow V2\n",
        "%tensorflow_version 2.x \n",
        "import tensorflow as tf\n",
        "print(\"TensofFlow\", tf.__version__)\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import collections\n",
        "\n",
        "sys.path.extend([f\"{libdir}/\"])      ## ToDo make sure this works and extra (old) libs are not in datadir\n",
        "import bert_utils\n",
        "import modeling\n",
        "import tokenization\n",
        "\n",
        "import json"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XsBNWHa3p6CL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## uncomment and use this cell to reimport libs you have updated\n",
        "# importlibe.reload(bert_utils)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yJC4o-hkuIfO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! zdump PST\n",
        "! pwd\n",
        "if verbose:\n",
        "    list_files('/content')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s7niWvRW-R5C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# raise ExecutionStop(\"Execution stopped\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NsnKczo07ygW",
        "colab_type": "text"
      },
      "source": [
        "## -- Code Implementation in Tensorflow 2.0 --"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v7JSYsp-72La",
        "colab_type": "text"
      },
      "source": [
        "**A few notes:**\n",
        "- If you want to keep using **flags** and **logging** you will have to use the **absl** lib (this is recommended by the TF team).\n",
        "- Since we won't use it with the kernels, he removed most of the **TPU** related stuff to reduce complexity.\n",
        "- Tensorflow 2 don't let us use global variables **(tf.compat.v1.trainable_variables())**.\n",
        "\n",
        "In this notebook, we'll be using the Bert baseline for Tensorflow to create predictions for the Natural Questions test set. Note that this uses a model that has already been pre-trained - we're only doing inference here. A GPU is required, and this should take between 1-2 hours to run.\n",
        "\n",
        "The original script can be found [here](https://github.com/google-research/language/blob/master/language/question_answering/bert_joint/run_nq.py).<br>\n",
        "The supporting modules were drawn from the [official Tensorflow model repository](https://github.com/tensorflow/models/tree/master/official).<br>\n",
        "The bert-joint-baseline data is described [here](https://github.com/google-research/language/tree/master/language/question_answering/bert_joint).\n",
        "\n",
        "**Note:** This baseline uses code that was migrated from TF1.x. Be aware that it contains use of tf.compat.v1, which is not permitted to be eligible for [TF2.0 prizes in this competition](https://www.kaggle.com/c/tensorflow2-question-answering/overview/prizes)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rhcWhldD17Q4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%bash\n",
        "zdump PST"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zTkY-he7qRbj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## grab bert config\n",
        "\n",
        "with open(f\"{datadir}/bert-joint-baseline/bert_config.json\", 'r') as f:\n",
        "    bertconf = json.load(f)\n",
        "if verbose:\n",
        "        print(json.dumps(bertconf, indent=4))\n",
        "# ToDo ------- search for variable config in ./lib and change to bertconf\n",
        "config = bertconf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "beaF_fYhtUxn",
        "colab_type": "text"
      },
      "source": [
        "### Support Classes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X4hOx_86tTFI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TDense(tf.keras.layers.Layer):\n",
        "    def __init__(self,\n",
        "                 output_size,\n",
        "                 kernel_initializer=None,\n",
        "                 bias_initializer=\"zeros\",\n",
        "                **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.output_size = output_size\n",
        "        self.kernel_initializer = kernel_initializer\n",
        "        self.bias_initializer = bias_initializer\n",
        "\n",
        "    def build(self,input_shape):\n",
        "        dtype = tf.as_dtype(self.dtype or tf.keras.backend.floatx())\n",
        "        if not (dtype.is_floating or dtype.is_complex):\n",
        "          raise TypeError(\"Unable to build `TDense` layer with \"\n",
        "                          \"non-floating point (and non-complex) \"\n",
        "                          \"dtype %s\" % (dtype,))\n",
        "        input_shape = tf.TensorShape(input_shape)\n",
        "        if tf.compat.dimension_value(input_shape[-1]) is None:\n",
        "          raise ValueError(\"The last dimension of the inputs to \"\n",
        "                           \"`TDense` should be defined. \"\n",
        "                           \"Found `None`.\")\n",
        "        last_dim = tf.compat.dimension_value(input_shape[-1])\n",
        "        self.input_spec = tf.keras.layers.InputSpec(min_ndim=3, axes={-1: last_dim})\n",
        "        self.kernel = self.add_weight(\n",
        "            \"kernel\",\n",
        "            shape=[self.output_size,last_dim],\n",
        "            initializer=self.kernel_initializer,\n",
        "            dtype=self.dtype,\n",
        "            trainable=True)\n",
        "        self.bias = self.add_weight(\n",
        "            \"bias\",\n",
        "            shape=[self.output_size],\n",
        "            initializer=self.bias_initializer,\n",
        "            dtype=self.dtype,\n",
        "            trainable=True)\n",
        "        super(TDense, self).build(input_shape)\n",
        "\n",
        "    def call(self,x):\n",
        "        return tf.matmul(x,self.kernel,transpose_b=True)+self.bias\n",
        "\n",
        "\n",
        "def mk_model(config):\n",
        "    seq_len = config['max_position_embeddings']\n",
        "    unique_id  = tf.keras.Input(shape=(1,),dtype=tf.int64,name='unique_id')\n",
        "    input_ids   = tf.keras.Input(shape=(seq_len,),dtype=tf.int32,name='input_ids')\n",
        "    input_mask  = tf.keras.Input(shape=(seq_len,),dtype=tf.int32,name='input_mask')\n",
        "    segment_ids = tf.keras.Input(shape=(seq_len,),dtype=tf.int32,name='segment_ids')\n",
        "    BERT = modeling.BertModel(config=config,name='bert')\n",
        "    pooled_output, sequence_output = BERT(input_word_ids=input_ids,\n",
        "                                          input_mask=input_mask,\n",
        "                                          input_type_ids=segment_ids)\n",
        "    \n",
        "    logits = TDense(2,name='logits')(sequence_output)\n",
        "    start_logits,end_logits = tf.split(logits,axis=-1,num_or_size_splits= 2,name='split')\n",
        "    start_logits = tf.squeeze(start_logits,axis=-1,name='start_squeeze')\n",
        "    end_logits   = tf.squeeze(end_logits,  axis=-1,name='end_squeeze')\n",
        "    \n",
        "    ans_type      = TDense(5,name='ans_type')(pooled_output)\n",
        "    return tf.keras.Model([input_ for input_ in [unique_id,input_ids,input_mask,segment_ids] \n",
        "                           if input_ is not None],\n",
        "                          [unique_id,start_logits,end_logits,ans_type],\n",
        "                          name='bert-baseline')    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qEnip0KLtx--",
        "colab_type": "text"
      },
      "source": [
        "### Create Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xzVE2wQGt2Au",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Some changes to bert_config.json\n",
        "small_config = bertconf.copy()\n",
        "small_config['vocab_size']=16\n",
        "small_config['hidden_size']=64\n",
        "small_config['max_position_embeddings'] = 32\n",
        "small_config['num_hidden_layers'] = 4\n",
        "small_config['num_attention_heads'] = 4\n",
        "small_config['intermediate_size'] = 256\n",
        "small_config\n",
        "\n",
        "model= mk_model(config)\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YymO1FiO-Au9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#                                   ### This was excluded from my copy\n",
        "if False:\n",
        "    model_params = {v.name:v for v in model.trainable_variables}\n",
        "    model_roots = np.unique([v.name.split('/')[0] for v in model.trainable_variables])\n",
        "    print(model_roots)\n",
        "    saved_names = [k for k,v in tf.train.list_variables('../input/bertjointbaseline/bert_joint.ckpt')]\n",
        "    a_map = {v:v+':0' for v in saved_names}\n",
        "    model_roots = np.unique([v.name.split('/')[0] for v in model.trainable_variables])\n",
        "    def transform(x):\n",
        "        x = x.replace('attention/self','attention')\n",
        "        x = x.replace('attention','self_attention')\n",
        "        x = x.replace('attention/output','attention_output')  \n",
        "\n",
        "        x = x.replace('/dense','')\n",
        "        x = x.replace('/LayerNorm','_layer_norm')\n",
        "        x = x.replace('embeddings_layer_norm','embeddings/layer_norm')  \n",
        "\n",
        "        x = x.replace('attention_output_layer_norm','attention_layer_norm')  \n",
        "        x = x.replace('embeddings/word_embeddings','word_embeddings/embeddings')\n",
        "\n",
        "        x = x.replace('/embeddings/','/embedding_postprocessor/')  \n",
        "        x = x.replace('/token_type_embeddings','/type_embeddings')  \n",
        "        x = x.replace('/pooler/','/pooler_transform/')  \n",
        "        x = x.replace('answer_type_output_bias','ans_type/bias')  \n",
        "        x = x.replace('answer_type_output_','ans_type/')\n",
        "        x = x.replace('cls/nq/output_','logits/')\n",
        "        x = x.replace('/weights','/kernel')\n",
        "\n",
        "        return x\n",
        "    a_map = {k:model_params.get(transform(v),None) for k,v in a_map.items() if k!='global_step'}\n",
        "    tf.compat.v1.train.init_from_checkpoint(ckpt_dir_or_file='../input/bertjointbaseline/bert_joint.ckpt',\n",
        "                                            assignment_map=a_map)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JJOVtX_NuEA3",
        "colab_type": "text"
      },
      "source": [
        "### Checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6eI_LdIAuGFV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cpkt = tf.train.Checkpoint(model=model)\n",
        "cpkt.restore(f\"{datadir}/bert-joint-baseline/model_cpkt-1\").assert_consumed()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IDnBbvUCuHI-",
        "colab_type": "text"
      },
      "source": [
        "### Setting the Flags"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0D_JoNumuOPH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#                                   ### This cell was in my copy but not in former\n",
        "class DummyObject:\n",
        "    def __init__(self,**kwargs):\n",
        "        self.__dict__.update(kwargs)\n",
        "\n",
        "FLAGS=DummyObject(skip_nested_contexts=True,\n",
        "                 max_position=50,\n",
        "                 max_contexts=48,\n",
        "                 max_query_length=64,\n",
        "                 max_seq_length=512,\n",
        "                 doc_stride=128,\n",
        "                 include_unknowns=-1.0,\n",
        "                 n_best_size=20,\n",
        "                 max_answer_length=30)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CSm1q0MouPwq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tqdm\n",
        "eval_records = f\"{datadir}/bert-joint-baseline/nq-test.tfrecords\"\n",
        "#nq_test_file = f\"{datadir}/tensorflow2-question-answering/simplified-nq-test.jsonl\"\n",
        "if on_kaggle_server and private_dataset:\n",
        "    eval_records='nq-test.tfrecords'\n",
        "if not os.path.exists(eval_records):\n",
        "    # tf2baseline.FLAGS.max_seq_length = 512\n",
        "    eval_writer = bert_utils.FeatureWriter(\n",
        "        filename=os.path.join(eval_records),\n",
        "        is_training=False)\n",
        "\n",
        "    tokenizer = tokenization.FullTokenizer(vocab_file=f\"{datadir}/bert-joint-baseline/vocab-nq.txt\", \n",
        "                                           do_lower_case=True)\n",
        "\n",
        "    features = []\n",
        "    convert = bert_utils.ConvertExamples2Features(tokenizer=tokenizer,\n",
        "                                                   is_training=False,\n",
        "                                                   output_fn=eval_writer.process_feature,\n",
        "                                                   collect_stat=False)\n",
        "\n",
        "    n_examples = 0\n",
        "    tqdm_notebook= tqdm.tqdm_notebook if not on_kaggle_server else None\n",
        "    for examples in bert_utils.nq_examples_iter(input_file=nq_test_file, \n",
        "                                           is_training=False,\n",
        "                                           tqdm=tqdm_notebook):\n",
        "        for example in examples:\n",
        "            n_examples += convert(example)\n",
        "\n",
        "    eval_writer.close()\n",
        "    print('number of test examples: %d, written to file: %d' % (n_examples,eval_writer.num_features))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dTJRYQjQuTsB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "seq_length = FLAGS.max_seq_length #config['max_position_embeddings']\n",
        "name_to_features = {\n",
        "      \"unique_id\": tf.io.FixedLenFeature([], tf.int64),\n",
        "      \"input_ids\": tf.io.FixedLenFeature([seq_length], tf.int64),\n",
        "      \"input_mask\": tf.io.FixedLenFeature([seq_length], tf.int64),\n",
        "      \"segment_ids\": tf.io.FixedLenFeature([seq_length], tf.int64),\n",
        "  }\n",
        "\n",
        "def _decode_record(record, name_to_features=name_to_features):\n",
        "    \"\"\"Decodes a record to a TensorFlow example.\"\"\"\n",
        "    example = tf.io.parse_single_example(serialized=record, features=name_to_features)\n",
        "\n",
        "    # tf.Example only supports tf.int64, but the TPU only supports tf.int32.\n",
        "    # So cast all int64 to int32.\n",
        "    for name in list(example.keys()):\n",
        "        t = example[name]\n",
        "        if name != 'unique_id': #t.dtype == tf.int64:\n",
        "            t = tf.cast(t, dtype=tf.int32)\n",
        "        example[name] = t\n",
        "\n",
        "    return example\n",
        "\n",
        "def _decode_tokens(record):\n",
        "    return tf.io.parse_single_example(serialized=record, \n",
        "                                      features={\n",
        "                                          \"unique_id\": tf.io.FixedLenFeature([], tf.int64),\n",
        "                                          \"token_map\" :  tf.io.FixedLenFeature([seq_length], tf.int64)\n",
        "                                      })\n",
        "      \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VKzXED-KuXjv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "raw_ds = tf.data.TFRecordDataset(eval_records)\n",
        "token_map_ds = raw_ds.map(_decode_tokens)\n",
        "decoded_ds = raw_ds.map(_decode_record)\n",
        "ds = decoded_ds.batch(batch_size=16,drop_remainder=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "33Npl_YL-1or",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#                                       ### This cell was not in my copy but former\n",
        "# next(iter(decoded_ds))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NT2UMXOZuY4p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "result=model.predict_generator(ds,verbose=1 if not on_kaggle_server else 0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VkIgBtoIua2p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.savez_compressed('bert-joint-baseline-output.npz',\n",
        "                    **dict(zip(['uniqe_id','start_logits','end_logits','answer_type_logits'],\n",
        "                               result)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S9WQFNVPuctX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Span = collections.namedtuple(\"Span\", [\"start_token_idx\", \"end_token_idx\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5vL3t4j5ufId",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ScoreSummary(object):\n",
        "  def __init__(self):\n",
        "    self.predicted_label = None\n",
        "    self.short_span_score = None\n",
        "    self.cls_token_score = None\n",
        "    self.answer_type_logits = None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5kR_ixQEuhmc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class EvalExample(object):\n",
        "  \"\"\"Eval data available for a single example.\"\"\"\n",
        "  def __init__(self, example_id, candidates):\n",
        "    self.example_id = example_id\n",
        "    self.candidates = candidates\n",
        "    self.results = {}\n",
        "    self.features = {}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3nLoKGECulk3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_best_indexes(logits, n_best_size):\n",
        "  \"\"\"Get the n-best logits from a list.\"\"\"\n",
        "  index_and_score = sorted(\n",
        "      enumerate(logits[1:], 1), key=lambda x: x[1], reverse=True)\n",
        "  best_indexes = []\n",
        "  for i in range(len(index_and_score)):\n",
        "    if i >= n_best_size:\n",
        "      break\n",
        "    best_indexes.append(index_and_score[i][0])\n",
        "  return best_indexes\n",
        "\n",
        "def top_k_indices(logits,n_best_size,token_map):\n",
        "    indices = np.argsort(logits[1:])+1\n",
        "    indices = indices[token_map[indices]!=-1]\n",
        "    return indices[-n_best_size:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e11n9VMvupHr",
        "colab_type": "text"
      },
      "source": [
        "## 1- Understanding the code\n",
        "#### For a better understanding, I will briefly explain here.\n",
        "#### In the item \"answer_type\", in the last lines of this block, it is responsible for storing the identified response type, which, according to [github project repository](https://github.com/google-research/language/blob/master/language/question_answering/bert_joint/run_nq.py) can be:\n",
        "1. UNKNOWN = 0\n",
        "2. YES = 1\n",
        "3. NO = 2\n",
        "4. SHORT = 3\n",
        "5. LONG = 4"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4GNEaA1QusjQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compute_predictions(example):\n",
        "  \"\"\"Converts an example into an NQEval object for evaluation.\"\"\"\n",
        "  predictions = []\n",
        "  n_best_size = FLAGS.n_best_size\n",
        "  max_answer_length = FLAGS.max_answer_length\n",
        "  i = 0\n",
        "  for unique_id, result in example.results.items():\n",
        "    if unique_id not in example.features:\n",
        "      raise ValueError(\"No feature found with unique_id:\", unique_id)\n",
        "    token_map = np.array(example.features[unique_id][\"token_map\"]) #.int64_list.value\n",
        "    start_indexes = top_k_indices(result.start_logits,n_best_size,token_map)\n",
        "    if len(start_indexes)==0:\n",
        "        continue\n",
        "    end_indexes   = top_k_indices(result.end_logits,n_best_size,token_map)\n",
        "    if len(end_indexes)==0:\n",
        "        continue\n",
        "    indexes = np.array(list(np.broadcast(start_indexes[None],end_indexes[:,None])))  \n",
        "    indexes = indexes[(indexes[:,0]<indexes[:,1])*(indexes[:,1]-indexes[:,0]<max_answer_length)]\n",
        "    for start_index,end_index in indexes:\n",
        "        summary = ScoreSummary()\n",
        "        summary.short_span_score = (\n",
        "            result.start_logits[start_index] +\n",
        "            result.end_logits[end_index])\n",
        "        summary.cls_token_score = (\n",
        "            result.start_logits[0] + result.end_logits[0])\n",
        "        summary.answer_type_logits = result.answer_type_logits-result.answer_type_logits.mean()\n",
        "        start_span = token_map[start_index]\n",
        "        end_span = token_map[end_index] + 1\n",
        "\n",
        "        # Span logits minus the cls logits seems to be close to the best.\n",
        "        score = summary.short_span_score - summary.cls_token_score\n",
        "        predictions.append((score, i, summary, start_span, end_span))\n",
        "        i += 1 # to break ties\n",
        "\n",
        "  # Default empty prediction.\n",
        "  score = -10000.0\n",
        "  short_span = Span(-1, -1)\n",
        "  long_span  = Span(-1, -1)\n",
        "  summary    = ScoreSummary()\n",
        "\n",
        "  if predictions:\n",
        "    score, _, summary, start_span, end_span = sorted(predictions, reverse=True)[0]\n",
        "    short_span = Span(start_span, end_span)\n",
        "    for c in example.candidates:\n",
        "      start = short_span.start_token_idx\n",
        "      end = short_span.end_token_idx\n",
        "      ## print(c['top_level'],c['start_token'],start,c['end_token'],end)\n",
        "      if c[\"top_level\"] and c[\"start_token\"] <= start and c[\"end_token\"] >= end:\n",
        "        long_span = Span(c[\"start_token\"], c[\"end_token\"])\n",
        "        break\n",
        "\n",
        "  summary.predicted_label = {\n",
        "      \"example_id\": int(example.example_id),\n",
        "      \"long_answer\": {\n",
        "          \"start_token\": int(long_span.start_token_idx),\n",
        "          \"end_token\": int(long_span.end_token_idx),\n",
        "          \"start_byte\": -1,\n",
        "          \"end_byte\": -1\n",
        "      },\n",
        "      \"long_answer_score\": float(score),\n",
        "      \"short_answers\": [{\n",
        "          \"start_token\": int(short_span.start_token_idx),\n",
        "          \"end_token\": int(short_span.end_token_idx),\n",
        "          \"start_byte\": -1,\n",
        "          \"end_byte\": -1\n",
        "      }],\n",
        "      \"short_answer_score\": float(score),\n",
        "      \"yes_no_answer\": \"NONE\",\n",
        "      \"answer_type_logits\": summary.answer_type_logits.tolist(),\n",
        "      # here:\n",
        "      \"answer_type\": int(np.argmax(summary.answer_type_logits))\n",
        "  }\n",
        "\n",
        "  return summary"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ww0cVDwMuv62",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compute_pred_dict(candidates_dict, dev_features, raw_results,tqdm=None):\n",
        "    \"\"\"Computes official answer key from raw logits.\"\"\"\n",
        "    raw_results_by_id = [(int(res.unique_id),1, res) for res in raw_results]\n",
        "\n",
        "    examples_by_id = [(int(k),0,v) for k, v in candidates_dict.items()]\n",
        "  \n",
        "    features_by_id = [(int(d['unique_id']),2,d) for d in dev_features] \n",
        "  \n",
        "    # Join examples with features and raw results.\n",
        "    examples = []\n",
        "    print('merging examples...')\n",
        "    merged = sorted(examples_by_id + raw_results_by_id + features_by_id)\n",
        "    print('done.')\n",
        "    for idx, type_, datum in merged:\n",
        "        if type_==0: #isinstance(datum, list):\n",
        "            examples.append(EvalExample(idx, datum))\n",
        "        elif type_==2: #\"token_map\" in datum:\n",
        "            examples[-1].features[idx] = datum\n",
        "        else:\n",
        "            examples[-1].results[idx] = datum\n",
        "\n",
        "    # Construct prediction objects.\n",
        "    print('Computing predictions...')\n",
        "   \n",
        "    nq_pred_dict = {}\n",
        "    #summary_dict = {}\n",
        "    if tqdm is not None:\n",
        "        examples = tqdm(examples)\n",
        "    for e in examples:\n",
        "        summary = compute_predictions(e)\n",
        "        #summary_dict[e.example_id] = summary\n",
        "        nq_pred_dict[e.example_id] = summary.predicted_label\n",
        "\n",
        "    return nq_pred_dict\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wE-BkBWhuzMt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def read_candidates_from_one_split(input_path):\n",
        "  \"\"\"Read candidates from a single jsonl file.\"\"\"\n",
        "  candidates_dict = {}\n",
        "  print(\"Reading examples from: %s\" % input_path)\n",
        "  if input_path.endswith(\".gz\"):\n",
        "    with gzip.GzipFile(fileobj=tf.io.gfile.GFile(input_path, \"rb\")) as input_file:\n",
        "      for index, line in enumerate(input_file):\n",
        "        e = json.loads(line)\n",
        "        candidates_dict[e[\"example_id\"]] = e[\"long_answer_candidates\"]\n",
        "        \n",
        "  else:\n",
        "    with tf.io.gfile.GFile(input_path, \"r\") as input_file:\n",
        "      for index, line in enumerate(input_file):\n",
        "        e = json.loads(line)\n",
        "        candidates_dict[e[\"example_id\"]] = e[\"long_answer_candidates\"]\n",
        "        # candidates_dict['question'] = e['question_text']\n",
        "  return candidates_dict"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DmBJNth5u04G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def read_candidates(input_pattern):\n",
        "  \"\"\"Read candidates with real multiple processes.\"\"\"\n",
        "  input_paths = tf.io.gfile.glob(input_pattern)\n",
        "  final_dict = {}\n",
        "  for input_path in input_paths:\n",
        "    final_dict.update(read_candidates_from_one_split(input_path))\n",
        "  return final_dict"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pYqeZlaLu2uR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "all_results = [bert_utils.RawResult(*x) for x in zip(*result)]\n",
        "    \n",
        "print (\"Going to candidates file\")\n",
        "\n",
        "candidates_dict = read_candidates('../input/tensorflow2-question-answering/simplified-nq-test.jsonl')\n",
        "\n",
        "print (\"setting up eval features\")\n",
        "\n",
        "eval_features = list(token_map_ds)\n",
        "\n",
        "print (\"compute_pred_dict\")\n",
        "\n",
        "tqdm_notebook= tqdm.tqdm_notebook\n",
        "nq_pred_dict = compute_pred_dict(candidates_dict, \n",
        "                                       eval_features,\n",
        "                                       all_results,\n",
        "                                      tqdm=tqdm_notebook)\n",
        "\n",
        "predictions_json = {\"predictions\": list(nq_pred_dict.values())}\n",
        "\n",
        "print (\"writing json\")\n",
        "\n",
        "with tf.io.gfile.GFile('predictions.json', \"w\") as f:\n",
        "    json.dump(predictions_json, f, indent=4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ore6D3Fku6oY",
        "colab_type": "text"
      },
      "source": [
        "## 2- Main Change\n",
        "#### Here is the small, but main change: we created an if to check the predicted response type and thus filter / identify the responses that are passed to the submission file."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sTdPRVxZvIRK",
        "colab_type": "text"
      },
      "source": [
        "### Filtering the Answers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tefIpu_ou9gm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_short_answer(entry):\n",
        "    answer = []    \n",
        "    if entry['answer_type'] == 0:\n",
        "        return \"\"\n",
        "    \n",
        "    elif entry['answer_type'] == 1:\n",
        "        return 'YES'\n",
        "    \n",
        "    elif entry['answer_type'] == 2:\n",
        "        return 'NO'\n",
        "        \n",
        "    elif entry[\"short_answer_score\"] < 1.5:\n",
        "        return \"\"\n",
        "    \n",
        "    else:\n",
        "        for short_answer in entry[\"short_answers\"]:\n",
        "            if short_answer[\"start_token\"] > -1:\n",
        "                answer.append(str(short_answer[\"start_token\"]) + \":\" + str(short_answer[\"end_token\"]))\n",
        "    \n",
        "        return \" \".join(answer)\n",
        "\n",
        "def create_long_answer(entry):\n",
        "    \n",
        "    answer = []\n",
        "    \n",
        "    if entry['answer_type'] == 0:\n",
        "        return ''\n",
        "    \n",
        "    elif entry[\"long_answer_score\"] < 1.5:\n",
        "        return \"\"\n",
        "\n",
        "    elif entry[\"long_answer\"][\"start_token\"] > -1:\n",
        "        answer.append(str(entry[\"long_answer\"][\"start_token\"]) + \":\" + str(entry[\"long_answer\"][\"end_token\"]))\n",
        "        return \" \".join(answer)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B3s7hVDYvL-I",
        "colab_type": "text"
      },
      "source": [
        "* ### Creating a DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qux93i9pvFUQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_answers_df = pd.read_json(\"../working/predictions.json\")\n",
        "for var_name in ['long_answer_score','short_answer_score','answer_type']:\n",
        "    test_answers_df[var_name] = test_answers_df['predictions'].apply(lambda q: q[var_name])\n",
        "test_answers_df[\"long_answer\"] = test_answers_df[\"predictions\"].apply(create_long_answer)\n",
        "test_answers_df[\"short_answer\"] = test_answers_df[\"predictions\"].apply(create_short_answer)\n",
        "test_answers_df[\"example_id\"] = test_answers_df[\"predictions\"].apply(lambda q: str(q[\"example_id\"]))\n",
        "\n",
        "long_answers = dict(zip(test_answers_df[\"example_id\"], test_answers_df[\"long_answer\"]))\n",
        "short_answers = dict(zip(test_answers_df[\"example_id\"], test_answers_df[\"short_answer\"]))\n",
        "\n",
        "test_answers_df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KLhi7tgDvRVH",
        "colab_type": "text"
      },
      "source": [
        "### Generating the Submission File"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T62_ONnivUfY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sample_submission = pd.read_csv(\"../input/tensorflow2-question-answering/sample_submission.csv\")\n",
        "\n",
        "long_prediction_strings = sample_submission[sample_submission[\"example_id\"].str.contains(\"_long\")].apply(lambda q: long_answers[q[\"example_id\"].replace(\"_long\", \"\")], axis=1)\n",
        "short_prediction_strings = sample_submission[sample_submission[\"example_id\"].str.contains(\"_short\")].apply(lambda q: short_answers[q[\"example_id\"].replace(\"_short\", \"\")], axis=1)\n",
        "\n",
        "sample_submission.loc[sample_submission[\"example_id\"].str.contains(\"_long\"), \"PredictionString\"] = long_prediction_strings\n",
        "sample_submission.loc[sample_submission[\"example_id\"].str.contains(\"_short\"), \"PredictionString\"] = short_prediction_strings\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DEDab0LOvYXg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sample_submission.to_csv('submission.csv', index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aPLARC3bvbyi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sample_submission.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "we9yOLCCvvDo",
        "colab_type": "text"
      },
      "source": [
        "* Yes Answers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fD2yAxelvw-h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "yes_answers = sample_submission[sample_submission['PredictionString'] == 'YES']\n",
        "yes_answers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eT05JPvQvl09",
        "colab_type": "text"
      },
      "source": [
        "* No Answers\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bE7-3uFKv676",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "no_answers = sample_submission[sample_submission['PredictionString'] == 'NO']\n",
        "no_answers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mMYJ3FN-v-fU",
        "colab_type": "text"
      },
      "source": [
        "* Balnk Answers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VFv3tE0YwB1B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "blank_answers = sample_submission[sample_submission['PredictionString'] == '']\n",
        "blank_answers.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9D_W5tinwEET",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "blank_answers.count()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A3Dp8eGQwIWh",
        "colab_type": "text"
      },
      "source": [
        "### I [original author] am only sharing modifications that I believe may help. I left out Tunning and any significant code changes I made."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "akKwcXNnJvi2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! zdump PST\n",
        "! pwd\n",
        "if verbose:\n",
        "    list_files('/content')\n",
        "! ls -l {outdir}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CmkZDNQK98df",
        "colab_type": "text"
      },
      "source": [
        "## -- Submitting Results --"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rKE9TaFxJx1P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "raise ExecutionStop(\"Don't let run all go beyond this\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rYwGNnso97Yr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%bash\n",
        "## View Previous Results\n",
        "#kaggle competitions list\n",
        "kaggle competitions submissions -c tensorflow2-question-answering"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rr1JY__B-PnG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Make Submission\n",
        "# I am not sure if we can submit this competition from this as it has to be a kernel submission\n",
        "#! kaggle competitions submit -c tensorflow2-question-answering -f $RESULT_CSV  -m 'test kaggle cli 3'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sBJjB1J3-fUL",
        "colab_type": "text"
      },
      "source": [
        "Verify submission by viewing previous results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "lRKAhZK8yvO_"
      },
      "source": [
        "End of Project Notebook\n",
        "# ====== Please fold this stuff up and ignore ====="
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "r_TVSYoeyvPa"
      },
      "source": [
        "### SSH Setup\n",
        "This is only neeeded if you want to log into the Colab machine. Otherwise fold it up and ignore.<br>\n",
        "To use it you have to create a login at https://ngrok.com\n",
        "<Details>Thanks to Imad El Hanafi (https://imadelhanafi.com) for showing me how to do this.<p>\n",
        "You will need to create a free account at https://ngrok.com/ for the SSH tunnel to work.</Details>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Udo19R6y9LD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "assert False        # Make sure user does not accedentially drop into this code"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Y0NMPj1fyvPb",
        "colab": {}
      },
      "source": [
        "%%bash\n",
        "## Install sshd; Set to allow login and config\n",
        "apt-get install -o=Dpkg::Use-Pty=0 openssh-server pwgen > /dev/null\n",
        "mkdir -p /var/run/sshd\n",
        "echo \"PermitRootLogin yes\" >> /etc/ssh/sshd_config\n",
        "echo \"PasswordAuthentication yes\" >> /etc/ssh/sshd_config\n",
        "# set host key to known value (need to test if exist)\n",
        "if [ -f \"/content/bertqa/colab/ssh_host_rsa_key.pub\" ]; then\n",
        "    cp \"/content/bertqa/colab/ssh_host_rsa_key.pub\" /etc/ssh/\n",
        "    echo \"Using ssh_host_rsa_key from gdrive\"\n",
        "fi\n",
        "# this script will give fix the login shell so Python will work\n",
        "if [ -f \"/content/bertqa/colab/init_shell.sh\" ]; then\n",
        "    echo \"source /content/bertqa/colab/init_shell.sh\" >> /root/.bashrc\n",
        "fi"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "S1mmJ__lyvPh",
        "colab": {}
      },
      "source": [
        "## setup ssh user / pass and start sshd\n",
        "\n",
        "#Generate a random root password\n",
        "import random, string\n",
        "sshpass = ''.join(random.choice(string.ascii_letters + string.digits) for i in range(30))\n",
        "\n",
        "#Set root password\n",
        "! echo root:$sshpass | chpasswd\n",
        "\n",
        "#Run sshd\n",
        "get_ipython().system_raw('/usr/sbin/sshd -D &')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "edJ3pW6YyvPl",
        "colab": {}
      },
      "source": [
        "%%bash\n",
        "## Get Ngrok from gdrive or try to download (see: https://ngrok.com/download)\n",
        "if [ -f \"/content/bertqa/colab/ngrok-stable-linux-amd64.zip\" ]; then\n",
        "    cp \"/content/bertqa/colab/ngrok-stable-linux-amd64.zip\" .\n",
        "    echo \"Using ngrok-stable-linux-amd64.zip from gdrive\"\n",
        "else\n",
        "    wget -q -c -nc https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
        "fi\n",
        "unzip -qq -n ngrok-stable-linux-amd64.zip\n",
        "rm ngrok-stable-linux-amd64.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YR0N4Iw8yvPq",
        "colab": {}
      },
      "source": [
        "## Get user to enter auth token from ngrok and start tunnel\n",
        "\n",
        "# Get token from ngrok for the tunnel\n",
        "print(\"Get your authtoken from https://dashboard.ngrok.com/auth\")\n",
        "import getpass\n",
        "authtoken = getpass.getpass()\n",
        "\n",
        "#Create tunnel\n",
        "get_ipython().system_raw('./ngrok authtoken $authtoken && ./ngrok tcp 22 &')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "1VjsRvCTyvPt"
      },
      "source": [
        "#### ==============================<br>|====&nbsp;&nbsp;  SSH Login Credentials &nbsp;&nbsp;====||<br>=============================="
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "cellView": "both",
        "id": "WKjt0Wh0yvPv",
        "colab": {}
      },
      "source": [
        "#@title\n",
        "print(\"username: root\")\n",
        "print(\"password: \", sshpass)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7OlwbxpWyvPz"
      },
      "source": [
        "Get the host name and port number at: https://dashboard.ngrok.com/status\n",
        "\n",
        "```bash\n",
        "ssh root@0.tcp.ngrok.io -p [ngrok_port]\n",
        "Login as: root\n",
        "Servrer refused our key\n",
        "root@0.tcp.ngrok.io's password: [see above]\n",
        "\n",
        "(Colab):/content$\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "bjJcssgxyvP0"
      },
      "source": [
        "Install vim"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "euzlUBHLyvP1",
        "colab": {}
      },
      "source": [
        "! apt-get install vim > /dev/null"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Ugtt5PxzyvP3"
      },
      "source": [
        "If you need to kill Ngrok run this cell"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CiP9qYfgyvP4",
        "colab": {}
      },
      "source": [
        "if EnableAllCode and False:\n",
        "    !kill $(ps aux | grep './ngrok' | awk '{print $2}')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "n3MP_pJ5yvP5"
      },
      "source": [
        "## -- Misc Notes --"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "cm6ErVGkyvP6"
      },
      "source": [
        "### Prevent Disconnects\n",
        "Colab periodically disconnects the browser.<br>\n",
        "You have to save model checkpoints to Google Drive so you don't lose work<br>\n",
        "See: https://mc.ai/google-colab-drive-as-persistent-storage-for-long-training-runs/<br>\n",
        "Something to try...<br>\n",
        "Ctrl+Shift+i in browser and in console run this code...\n",
        "```\n",
        "function KeepAlive(){\n",
        "    console.log(\"Maintaining Connection\");\n",
        "    document.querySelector(\"colab-toolbar-button#connect\").click()\n",
        "}\n",
        "setInterval(KeepAlive,60000);\n",
        "```\n",
        "There have been reports of people having their GPU privileges suspended for letting processes run for over 12 hours. It seems that they may penalize you rather than just cutting you off."
      ]
    }
  ]
}